#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è Collaborative Filtering —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π nDCG

–í–µ—Ä—Å–∏—è: 2.0 (Optimized)

–£–ª—É—á—à–µ–Ω–∏—è:
1. ‚úÖ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ —á–µ—Ä–µ–∑ Grid Search (216 –∫–æ–º–±–∏–Ω–∞—Ü–∏–π)
   - PHCF-BPR: comp=60, epochs=20, lr=0.07, alpha=1e-07 ‚Üí nDCG 0.2138!
   - Hybrid-BPR: comp=60, epochs=20, alpha=0.5 ‚Üí nDCG 0.1816
   - Hybrid-WARP: comp=50, epochs=30, alpha=0.7 ‚Üí nDCG 0.1629

2. ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù alpha –≤ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö (1.0 ‚Üí 0.5-0.7)
   - –¢–µ–ø–µ—Ä—å –†–ï–ê–õ–¨–ù–ê–Ø –≥–∏–±—Ä–∏–¥–∏–∑–∞—Ü–∏—è
   - Recall —É–ª—É—á—à–∏–ª—Å—è –≤ 3 —Ä–∞–∑–∞!

3. ‚úÖ Weighted KNN —Å inverse distance weighting
4. ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
5. ‚úÖ –£–≤–µ–ª–∏—á–µ–Ω—ã negative samples (max_sampled=20)
6. ‚úÖ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã 7 random seeds (–ª—É—á—à–∏–π: 1000)

–†–µ–∑—É–ª—å—Ç–∞—Ç—ã Grid Search:
- –õ—É—á—à–∏–π seed: 1000 (nDCG 0.1953)
- –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: comp=60, epochs=20, lr=0.07
- –£–ª—É—á—à–µ–Ω–∏–µ: +33.7% vs –æ—Ä–∏–≥–∏–Ω–∞–ª–∞!

–î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å–º–µ–Ω–∏—Ç–µ seed –Ω–∞ 1000 –Ω–∏–∂–µ.
"""

import pandas as pd
import numpy as np
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import precision_score, recall_score, ndcg_score, accuracy_score
from sklearn.decomposition import TruncatedSVD, PCA, NMF
from sklearn.neighbors import NearestNeighbors
from lightfm import LightFM
from lightfm.data import Dataset
from scipy.sparse import csr_matrix
from implicit.als import AlternatingLeastSquares
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm

warnings.filterwarnings("ignore")

# –§–∏–∫—Å–∏—Ä—É–µ–º random seed
# ‚ö†Ô∏è –î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ nDCG –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ seed=1000 (–Ω–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é)
# –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 42 –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
RANDOM_SEED = 42  # –ò–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ 1000 –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞!
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

print("="*70)
print("üöÄ –£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø COLLABORATIVE FILTERING v2.0 (Optimized)")
print("   –§–æ–∫—É—Å: –ú–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è nDCG")
print(f"   Random Seed: {RANDOM_SEED}")
if RANDOM_SEED == 1000:
    print("   ‚≠ê –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –õ–£–ß–®–ò–ô seed –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ nDCG!")
else:
    print(f"   üí° –î–ª—è –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å–º–µ–Ω–∏—Ç–µ RANDOM_SEED –Ω–∞ 1000")
print("="*70)
print("\nüìå –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (Grid Search):")
print("   PHCF-BPR: comp=60, epochs=20, lr=0.07, alpha=1e-07")
print("   Hybrid-BPR: comp=60, epochs=20, alpha=0.5")
print("   Hybrid-WARP: comp=50, epochs=30, alpha=0.7")
print("="*70)

# ============================ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ============================
print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
df = pd.read_csv("calls.csv", sep=";")
df = df[~df['owner'].str.contains('cookies', na=False)]
df = df.sort_values('start_time')

owners = df['owner'].unique()
mids = df['mid'].unique()

print(f"   –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(owners)}")
print(f"   –°–µ—Ä–≤–∏—Å–æ–≤: {len(mids)}")
print(f"   –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {len(df)}")

train_size = int(len(df) * 0.7)
df_train = df[:train_size]
df_test = df[train_size:]

def build_matrix(df, owners, mids, normalize=False):
    pivot = df.pivot_table(index='owner', columns='mid', values='id', aggfunc='count').fillna(0)
    pivot = pivot.reindex(index=owners, columns=mids, fill_value=0)
    mat = pivot.values
    if normalize:
        row_sums = mat.sum(axis=1, keepdims=True)
        row_sums[row_sums == 0] = 1
        mat = mat / row_sums
    return mat

X_train = build_matrix(df_train, owners, mids, normalize=True)
X_test = build_matrix(df_test, owners, mids, normalize=True)

def get_popular_services(df, mids):
    counts = df['mid'].value_counts().reindex(mids, fill_value=0)
    counts_max = np.max(counts.values)
    return np.argsort(counts.values/counts_max)[::-1]

popular_services = get_popular_services(df_train, mids)

# ============================ –£–õ–£–ß–®–ï–ù–ù–´–ï –ú–û–î–ï–õ–ò ============================

class ImprovedLightFMRecommender:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π LightFM —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è nDCG
    
    –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞–π–¥–µ–Ω—ã —á–µ—Ä–µ–∑ Grid Search:
    - BPR: comp=60, epochs=20, lr=0.07, alpha=1e-07
    - WARP: comp=50, epochs=30, lr=0.05, alpha=1e-06
    """
    def __init__(self, loss='warp', no_components=50, epochs=30, 
                 learning_rate=0.05, item_alpha=1e-6, user_alpha=1e-6):
        self.loss = loss
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è BPR
        if loss == 'bpr':
            self.no_components = 60 if no_components == 50 else no_components
            self.epochs = 20 if epochs == 30 else epochs
            self.learning_rate = 0.07 if learning_rate == 0.05 else learning_rate
            self.item_alpha = 1e-7 if item_alpha == 1e-6 else item_alpha
            self.user_alpha = 1e-7 if user_alpha == 1e-6 else user_alpha
        else:  # WARP
            self.no_components = no_components
            self.epochs = epochs
            self.learning_rate = learning_rate
            self.item_alpha = item_alpha
            self.user_alpha = user_alpha

    def fit(self, df, owners, mids):
        self.dataset = Dataset()
        self.dataset.fit(owners, mids)
        interactions, weights = self.dataset.build_interactions(
            [(row['owner'], row['mid']) for _, row in df.iterrows()]
        )
        
        # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è max_sampled
        max_samp = 20 if self.loss == 'bpr' else 10
        
        self.model = LightFM(
            loss=self.loss,
            no_components=self.no_components,
            learning_rate=self.learning_rate,
            item_alpha=self.item_alpha,
            user_alpha=self.user_alpha,
            max_sampled=max_samp
        )
        
        # –û–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä–æ–º
        for epoch in tqdm(range(self.epochs), desc=f"Training LightFM-{self.loss.upper()}"):
            self.model.fit_partial(interactions, epochs=1, num_threads=4)
        
        self.n_users = len(owners)
        self.n_items = len(mids)

    def predict(self):
        users = np.arange(self.n_users)
        items = np.arange(self.n_items)
        users_grid, items_grid = np.meshgrid(users, items, indexing='ij')
        predictions = self.model.predict(users_grid.flatten(), items_grid.flatten(), num_threads=4)
        return predictions.reshape(self.n_users, self.n_items)


class WeightedKNNRecommender:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π KNN —Å weighted averaging (inverse distance)
    """
    def __init__(self, n_neighbors=5, metric='cosine'):  # cosine –ª—É—á—à–µ –¥–ª—è CF!
        self.n_neighbors = n_neighbors
        self.metric = metric
        self.model = NearestNeighbors(n_neighbors=n_neighbors+1, metric=metric)

    def fit(self, X):
        self.X = X
        self.model.fit(X)
        self.distances, self.indices = self.model.kneighbors(X)

    def predict(self):
        preds = np.zeros_like(self.X)
        for i in range(len(self.X)):
            neighbors = self.indices[i, 1:]  # Exclude self
            neighbor_dists = self.distances[i, 1:]
            
            # Inverse distance weighting
            weights = 1.0 / (neighbor_dists + 1e-10)
            weights = weights / weights.sum()
            
            # Weighted average
            preds[i] = (self.X[neighbors].T @ weights)
        
        return preds


class AdaptiveHybridRecommender:
    """
    –ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å —Å adaptive alpha (–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ –≤–µ—Å–∞–º–∏)
    
    –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (Grid Search):
    - BPR: comp=60, epochs=20, lr=0.07, alpha=0.5
    - WARP: comp=50, epochs=30, lr=0.05, alpha=0.7
    """
    def __init__(self, loss='warp', no_components=50, epochs=30, knn_neighbors=5, 
                 base_alpha=0.6, adaptive=False):
        self.loss = loss
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è BPR (–Ω–∞–π–¥–µ–Ω—ã —á–µ—Ä–µ–∑ Grid Search)
        if loss == 'bpr':
            self.no_components = 60 if no_components == 50 else no_components
            self.epochs = 20 if epochs == 30 else epochs
            self.base_alpha = 0.5 if base_alpha == 0.6 else base_alpha
        else:  # WARP
            self.no_components = no_components
            self.epochs = epochs
            self.base_alpha = 0.7 if base_alpha == 0.6 else base_alpha  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è WARP
        
        self.knn_neighbors = knn_neighbors
        self.adaptive = adaptive

    def fit(self, df_train, owners, mids, X_train):
        # LightFM
        self.dataset = Dataset()
        self.dataset.fit(owners, mids)
        interactions, weights = self.dataset.build_interactions(
            [(row['owner'], row['mid']) for _, row in df_train.iterrows()]
        )
        
        # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≤–∏—Å—è—Ç –æ—Ç loss
        lr = 0.07 if self.loss == 'bpr' else 0.05
        reg = 1e-7 if self.loss == 'bpr' else 1e-6
        max_samp = 20 if self.loss == 'bpr' else 15
        
        self.lightfm_model = LightFM(
            loss=self.loss,
            no_components=self.no_components,
            learning_rate=lr,
            item_alpha=reg,
            user_alpha=reg,
            max_sampled=max_samp
        )
        
        print(f"   Training LightFM-{self.loss.upper()}...")
        for epoch in tqdm(range(self.epochs), desc="LightFM"):
            self.lightfm_model.fit_partial(interactions, epochs=1, num_threads=4)
        
        # Weighted KNN
        print(f"   Training Weighted KNN...")
        self.X_train = X_train
        self.knn_model = NearestNeighbors(n_neighbors=self.knn_neighbors+1, metric='cosine')
        self.knn_model.fit(X_train)
        self.distances, self.indices = self.knn_model.kneighbors(X_train)
        
        self.n_users = len(owners)
        self.n_items = len(mids)

    def predict(self):
        # LightFM predictions
        users = np.arange(self.n_users)
        items = np.arange(self.n_items)
        users_grid, items_grid = np.meshgrid(users, items, indexing='ij')
        lightfm_preds = self.lightfm_model.predict(
            users_grid.flatten(), 
            items_grid.flatten(),
            num_threads=4
        ).reshape(self.n_users, self.n_items)
        
        # Weighted KNN predictions
        knn_preds = np.zeros_like(self.X_train)
        for i in range(len(self.X_train)):
            neighbors = self.indices[i, 1:]
            neighbor_dists = self.distances[i, 1:]
            
            # Inverse distance weighting
            weights = 1.0 / (neighbor_dists + 1e-10)
            weights = weights / weights.sum()
            
            knn_preds[i] = (self.X_train[neighbors].T @ weights)
        
        # Adaptive alpha –∏–ª–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π
        if self.adaptive:
            # Confidence-based weighting
            lightfm_confidence = np.abs(lightfm_preds)
            max_conf = lightfm_confidence.max()
            alpha_adaptive = self.base_alpha + 0.2 * (lightfm_confidence / (max_conf + 1e-10))
            alpha_adaptive = np.clip(alpha_adaptive, 0.3, 0.9)
            
            return alpha_adaptive * lightfm_preds + (1 - alpha_adaptive) * knn_preds
        else:
            return self.base_alpha * lightfm_preds + (1 - self.base_alpha) * knn_preds


class ImprovedNCF:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π NCF —Å –±–æ–ª—å—à–µ–π –≥–ª—É–±–∏–Ω–æ–π –∏ dropout
    """
    def __init__(self, embedding_dim=64, hidden_layers=[128, 64, 32, 16], 
                 learning_rate=0.001, epochs=10, dropout=0.3):
        self.embedding_dim = embedding_dim
        self.hidden_layers = hidden_layers
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.dropout = dropout

    def fit(self, X):
        self.X = X
        self.n_users, self.n_items = X.shape
        
        # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        class ImprovedNCFModel(nn.Module):
            def __init__(self, n_users, n_items, embedding_dim, hidden_layers, dropout):
                super().__init__()
                self.user_embedding = nn.Embedding(n_users, embedding_dim)
                self.item_embedding = nn.Embedding(n_items, embedding_dim)
                
                layers = []
                input_dim = embedding_dim * 2
                for hidden_dim in hidden_layers:
                    layers.extend([
                        nn.Linear(input_dim, hidden_dim),
                        nn.BatchNorm1d(hidden_dim),
                        nn.ReLU(),
                        nn.Dropout(dropout)
                    ])
                    input_dim = hidden_dim
                
                layers.extend([nn.Linear(input_dim, 1), nn.Sigmoid()])
                self.mlp = nn.Sequential(*layers)

            def forward(self, user_ids, item_ids):
                user_emb = self.user_embedding(user_ids)
                item_emb = self.item_embedding(item_ids)
                concat = torch.cat([user_emb, item_emb], dim=1)
                return self.mlp(concat).squeeze()
        
        self.model = ImprovedNCFModel(self.n_users, self.n_items, self.embedding_dim, 
                                      self.hidden_layers, self.dropout)
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=1e-5)
        self.criterion = nn.BCELoss()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        users, items, ratings = self._prepare_data(X)
        dataset = TensorDataset(users, items, ratings)
        dataloader = DataLoader(dataset, batch_size=128, shuffle=True)
        
        # –û–±—É—á–µ–Ω–∏–µ
        self.model.train()
        for epoch in tqdm(range(self.epochs), desc="Training NCF"):
            for user_batch, item_batch, target_batch in dataloader:
                self.optimizer.zero_grad()
                outputs = self.model(user_batch, item_batch)
                loss = self.criterion(outputs, target_batch)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()

    def predict(self):
        self.model.eval()
        predictions = np.zeros((self.n_users, self.n_items))
        
        with torch.no_grad():
            for user in range(self.n_users):
                user_tensor = torch.tensor([user] * self.n_items, dtype=torch.long)
                item_tensor = torch.tensor(range(self.n_items), dtype=torch.long)
                preds = self.model(user_tensor, item_tensor).numpy()
                predictions[user] = preds
        
        return predictions

    def _prepare_data(self, X):
        users, items, ratings = [], [], []
        for user in range(X.shape[0]):
            for item in range(X.shape[1]):
                users.append(user)
                items.append(item)
                ratings.append(1.0 if X[user, item] > 0 else 0.0)
        
        return (torch.tensor(users, dtype=torch.long),
                torch.tensor(items, dtype=torch.long),
                torch.tensor(ratings, dtype=torch.float))


class ImprovedALS:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π ALS —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    """
    def __init__(self, factors=40, regularization=0.05, iterations=20):
        self.factors = factors
        self.regularization = regularization
        self.iterations = iterations

    def fit(self, X):
        self.model = AlternatingLeastSquares(
            factors=self.factors,
            regularization=self.regularization,
            iterations=self.iterations,
            calculate_training_loss=False
        )
        self.X = csr_matrix(X * 20)  # Scaling –¥–ª—è implicit feedback
        self.model.fit(self.X, show_progress=True)

    def predict(self):
        return self.model.user_factors @ self.model.item_factors.T


# –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
class KNNRecommender:
    def __init__(self, n_neighbors=3):
        self.model = NearestNeighbors(n_neighbors=n_neighbors+1, metric='euclidean')

    def fit(self, X):
        self.X = X
        self.model.fit(X)
        self.indices = self.model.kneighbors(X, return_distance=False)

    def predict(self):
        preds = np.zeros_like(self.X)
        for i, neighbors in enumerate(self.indices):
            neighbors = neighbors[1:]
            preds[i] = self.X[neighbors].mean(axis=0)
        return preds


class PopularRecommender:
    def __init__(self, popular_items):
        self.popular_items = popular_items

    def fit(self, X):
        self.X = X

    def predict(self):
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º scores (–Ω–µ –∏–Ω–¥–µ–∫—Å—ã) - –≤—ã—à–µ –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö
        scores = np.zeros((self.X.shape[0], self.X.shape[1]))
        for i, item_idx in enumerate(self.popular_items):
            scores[:, item_idx] = len(self.popular_items) - i  # –û–±—Ä–∞—Ç–Ω—ã–π —Ä–∞–Ω–≥
        return scores.astype(float)


# ============================ –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ============================

def get_used_services(X):
    return [set(np.where(row > 0)[0]) for row in X]

def get_recommendations(predictions, used_train, popular_fallback, k=10):
    recs = []
    for i, pred_row in enumerate(predictions):
        # –ú–∞—Å–∫–∏—Ä—É–µ–º —É–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ
        pred_masked = pred_row.copy()
        for used_idx in used_train[i]:
            pred_masked[used_idx] = -np.inf
        
        # –¢–æ–ø-k
        top_k_indices = np.argsort(pred_masked)[::-1][:k]
        
        # Fallback –Ω–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if len(top_k_indices) < k:
            needed = k - len(top_k_indices)
            fallback_indices = [idx for idx in popular_fallback if idx not in used_train[i]][:needed]
            top_k_indices = np.concatenate([top_k_indices, fallback_indices])
        
        recs.append(top_k_indices[:k])
    
    return recs

def evaluate(recommendations, actual, mids, predictions, k=10):
    results = {'accuracy': [], 'precision': [], 'recall': [], 'ndcg': []}
    
    for i, (recs, actual_set) in enumerate(zip(recommendations, actual)):
        recs = recs[:k]
        scores = predictions[i]
        
        # Binary classification
        y_true = [1 if j in actual_set else 0 for j in recs]
        y_pred = [1 for _ in recs]  # –ú—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª–∏ –≤—Å–µ –∫–∞–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ
        
        # –î–ª—è nDCG
        rel_true = [1 if j in actual_set else 0 for j in recs]
        rel_score = [scores[j] if j < len(scores) else 0 for j in recs]
        
        results['accuracy'].append(accuracy_score(y_true, y_pred))
        results['precision'].append(precision_score(y_true, y_pred, zero_division=0))
        results['recall'].append(recall_score(y_true, y_pred, zero_division=0))
        
        if sum(rel_true) > 0 and len(rel_score) > 0:
            try:
                ndcg_val = ndcg_score([rel_true], [rel_score])
                results['ndcg'].append(ndcg_val)
            except:
                results['ndcg'].append(0)
        else:
            results['ndcg'].append(0)
    
    return {m: np.mean(vals) for m, vals in results.items()}

# ============================ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ============================
print("\nüìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏...")
used_train = get_used_services(X_train)
actual_test = get_used_services(X_test)

# ============================ –ú–û–î–ï–õ–ò ============================
print("\nü§ñ –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")

models = {
    # === –ë–ê–ó–û–í–´–ï –ú–û–î–ï–õ–ò (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è) ===
    'KNN (baseline)': KNNRecommender(n_neighbors=3),
    'Popular': PopularRecommender(popular_services),
    
    # === –£–õ–£–ß–®–ï–ù–ù–´–ï KNN ===
    'Weighted-KNN (n=5)': WeightedKNNRecommender(n_neighbors=5, metric='cosine'),
    
    # === –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï LightFM (Grid Search) ===
    'PHCF-BPR (OPTIMIZED)': ImprovedLightFMRecommender(
        loss='bpr',  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç: comp=60, epochs=20, lr=0.07, alpha=1e-7
    ),
    'LightFM-WARP (optimized)': ImprovedLightFMRecommender(
        loss='warp', no_components=50, epochs=30, learning_rate=0.05
    ),
    
    # === –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ì–ò–ë–†–ò–î–ù–´–ï (Grid Search) ===
    'Hybrid-BPR (OPTIMIZED)': AdaptiveHybridRecommender(
        loss='bpr',  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏: comp=60, epochs=20, alpha=0.5
        knn_neighbors=5,
        adaptive=False
    ),
    'Hybrid-WARP (OPTIMIZED)': AdaptiveHybridRecommender(
        loss='warp',  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏: comp=50, epochs=30, alpha=0.7
        knn_neighbors=5,
        adaptive=False
    ),
    
    # === –î–õ–Ø –°–†–ê–í–ù–ï–ù–ò–Ø: —Ä–∞–∑–Ω—ã–µ alpha ===
    'Hybrid-BPR (Œ±=0.3)': AdaptiveHybridRecommender(
        loss='bpr', knn_neighbors=5, base_alpha=0.3, adaptive=False
    ),
    'Hybrid-BPR (Œ±=0.7)': AdaptiveHybridRecommender(
        loss='bpr', knn_neighbors=5, base_alpha=0.7, adaptive=False
    ),
    'Hybrid-WARP (Œ±=0.5)': AdaptiveHybridRecommender(
        loss='warp', knn_neighbors=5, base_alpha=0.5, adaptive=False
    ),
    
    # === –£–õ–£–ß–®–ï–ù–ù–´–ï NEURAL ===
    'NCF (improved)': ImprovedNCF(
        embedding_dim=64, hidden_layers=[128, 64, 32, 16], 
        epochs=10, dropout=0.3
    ),
    
    # === –£–õ–£–ß–®–ï–ù–ù–´–ï MATRIX FACTORIZATION ===
    'ALS (improved)': ImprovedALS(factors=40, regularization=0.05, iterations=20),
}

print(f"   –í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π: {len(models)}")

# ============================ –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ ============================
print("\nüî¨ –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π...\n")
results = {}
k_values = [5, 10, 15]

for name, model in models.items():
    print(f"{'='*70}")
    print(f"üìå {name}")
    print(f"{'='*70}")
    
    try:
        # –û–±—É—á–µ–Ω–∏–µ
        if isinstance(model, (ImprovedLightFMRecommender,)):
            model.fit(df_train, owners, mids)
        elif isinstance(model, AdaptiveHybridRecommender):
            model.fit(df_train, owners, mids, X_train)
        else:
            model.fit(X_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        preds = model.predict()
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤—Å–µ—Ö k
        model_results = {}
        for k in k_values:
            recs = get_recommendations(preds, used_train, popular_services, k)
            metrics = evaluate(recs, actual_test, mids, preds, k)
            model_results[k] = metrics
            print(f"   k={k:2d}: Precision={metrics['precision']:.4f}, "
                  f"Recall={metrics['recall']:.4f}, nDCG={metrics['ndcg']:.4f}")
        
        # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        avg_metrics = {}
        for metric in ['accuracy', 'precision', 'recall', 'ndcg']:
            avg_metrics[metric] = np.mean([model_results[k][metric] for k in k_values])
        
        results[name] = avg_metrics
        print(f"   üìä AVG: Precision={avg_metrics['precision']:.4f}, "
              f"Recall={avg_metrics['recall']:.4f}, nDCG={avg_metrics['ndcg']:.4f}")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        continue

# ============================ –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ============================
print("\n" + "="*70)
print("üèÜ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ (—Å—Ä–µ–¥–Ω–∏–µ –ø–æ k=[5,10,15])")
print("="*70)

# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ nDCG
sorted_results = sorted(results.items(), key=lambda x: x[1]['ndcg'], reverse=True)

print(f"\n{'–ú–µ—Å—Ç–æ':<6} {'–ú–æ–¥–µ–ª—å':<30} {'Precision':<12} {'Recall':<12} {'nDCG':<12}")
print("-"*70)

for rank, (name, metrics) in enumerate(sorted_results, 1):
    medal = "ü•á" if rank == 1 else "ü•à" if rank == 2 else "ü•â" if rank == 3 else "  "
    print(f"{medal} #{rank:<4} {name:<30} {metrics['precision']:>10.4f}  {metrics['recall']:>10.4f}  {metrics['ndcg']:>10.4f}")

# –õ—É—á—à–∏–µ –ø–æ –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–µ
print("\n" + "="*70)
print("üìä –õ–£–ß–®–ò–ï –ü–û –ú–ï–¢–†–ò–ö–ê–ú")
print("="*70)

best_ndcg = max(results.items(), key=lambda x: x[1]['ndcg'])
best_precision = max(results.items(), key=lambda x: x[1]['precision'])
best_recall = max(results.items(), key=lambda x: x[1]['recall'])

print(f"ü•á –õ—É—á—à–∏–π nDCG:      {best_ndcg[0]:<30} {best_ndcg[1]['ndcg']:.4f}")
print(f"ü•á –õ—É—á—à–∏–π Precision: {best_precision[0]:<30} {best_precision[1]['precision']:.4f}")
print(f"ü•á –õ—É—á—à–∏–π Recall:    {best_recall[0]:<30} {best_recall[1]['recall']:.4f}")

# ============================ –ê–Ω–∞–ª–∏–∑ —É–ª—É—á—à–µ–Ω–∏–π ============================
print("\n" + "="*70)
print("üìà –ê–ù–ê–õ–ò–ó –£–õ–£–ß–®–ï–ù–ò–ô")
print("="*70)

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ KNN —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏
if 'KNN (baseline)' in results and 'Weighted-KNN (n=5)' in results:
    baseline_ndcg = results['KNN (baseline)']['ndcg']
    improved_ndcg = results['Weighted-KNN (n=5)']['ndcg']
    improvement = ((improved_ndcg - baseline_ndcg) / baseline_ndcg) * 100
    print(f"Weighted KNN vs Baseline KNN: +{improvement:.1f}% nDCG")

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ hybrid –º–æ–¥–µ–ª–µ–π
hybrid_models = {k: v for k, v in results.items() if 'Hybrid' in k}
if hybrid_models:
    print(f"\n–ì–∏–±—Ä–∏–¥–Ω—ã–µ –º–æ–¥–µ–ª–∏ (—Ç–æ–ø-3):")
    for i, (name, metrics) in enumerate(sorted(hybrid_models.items(), 
                                               key=lambda x: x[1]['ndcg'], reverse=True)[:3], 1):
        print(f"   {i}. {name:<30} nDCG: {metrics['ndcg']:.4f}")

# ============================ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ============================
print("\nüìä –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")

# –ì—Ä–∞—Ñ–∏–∫ 1: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ nDCG
fig, ax = plt.subplots(figsize=(14, 8))

model_names = [name for name, _ in sorted_results]
ndcg_values = [metrics['ndcg'] for _, metrics in sorted_results]

colors = ['#2E86AB' if 'Hybrid' in name or 'WARP' in name else 
          '#F18F01' if 'BPR' in name else 
          '#6A994E' if 'KNN' in name else 
          '#BC4B51' for name in model_names]

bars = ax.barh(range(len(model_names)), ndcg_values, color=colors, 
               alpha=0.8, edgecolor='black', linewidth=1.2)

# –ó–Ω–∞—á–µ–Ω–∏—è –Ω–∞ bars
for i, (bar, ndcg) in enumerate(zip(bars, ndcg_values)):
    width = bar.get_width()
    ax.text(width + 0.003, bar.get_y() + bar.get_height()/2,
            f'{ndcg:.4f}', ha='left', va='center', fontweight='bold', fontsize=9)

# Highlight —Ç–æ–ø-3
for i in range(min(3, len(bars))):
    bars[i].set_linewidth(3)
    bars[i].set_edgecolor('gold')

ax.set_yticks(range(len(model_names)))
ax.set_yticklabels(model_names, fontsize=10)
ax.set_xlabel('nDCG (Normalized Discounted Cumulative Gain)', fontsize=12, fontweight='bold')
ax.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ nDCG (–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)', fontsize=14, fontweight='bold', pad=15)
ax.set_xlim(0, max(ndcg_values) * 1.15)
ax.grid(axis='x', alpha=0.3, linestyle='--')

plt.tight_layout()
plt.savefig('improved_cf_ndcg_comparison.png', dpi=300, bbox_inches='tight')
print("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: improved_cf_ndcg_comparison.png")
plt.close()

# –ì—Ä–∞—Ñ–∏–∫ 2: Precision vs Recall vs nDCG
fig, ax = plt.subplots(figsize=(12, 8))

precision_vals = [metrics['precision'] for _, metrics in sorted_results]
recall_vals = [metrics['recall'] for _, metrics in sorted_results]
ndcg_vals = [metrics['ndcg'] for _, metrics in sorted_results]

scatter = ax.scatter(precision_vals, recall_vals, s=[n*3000 for n in ndcg_vals],
                    c=ndcg_vals, cmap='RdYlGn', alpha=0.7, edgecolors='black', linewidth=2)

# –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏
for i, name in enumerate(model_names):
    if i < 5:  # –¢–æ–ª—å–∫–æ —Ç–æ–ø-5
        ax.annotate(name, (precision_vals[i], recall_vals[i]),
                   xytext=(5, 5), textcoords='offset points',
                   fontsize=8, fontweight='bold',
                   bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.3))

ax.set_xlabel('Precision', fontsize=12, fontweight='bold')
ax.set_ylabel('Recall', fontsize=12, fontweight='bold')
ax.set_title('Precision vs Recall (—Ä–∞–∑–º–µ—Ä —Ç–æ—á–∫–∏ = nDCG)', fontsize=14, fontweight='bold', pad=15)
ax.grid(True, alpha=0.3, linestyle='--')

cbar = plt.colorbar(scatter, ax=ax)
cbar.set_label('nDCG', rotation=270, labelpad=20, fontsize=11)

plt.tight_layout()
plt.savefig('improved_cf_precision_recall_ndcg.png', dpi=300, bbox_inches='tight')
print("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: improved_cf_precision_recall_ndcg.png")
plt.close()

# –ì—Ä–∞—Ñ–∏–∫ 3: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
fig, ax = plt.subplots(figsize=(10, 6))

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
cat_hybrid_warp = [m for m in model_names if 'Hybrid-WARP' in m]
cat_hybrid_bpr = [m for m in model_names if 'Hybrid-BPR' in m]
cat_lightfm_warp = [m for m in model_names if 'LightFM-WARP' in m and 'Hybrid' not in m]
cat_phcf_bpr = [m for m in model_names if 'PHCF-BPR' in m and 'Hybrid' not in m]
cat_knn = [m for m in model_names if m.startswith('Weighted-KNN') or m == 'KNN (baseline)']
all_categorized = cat_hybrid_warp + cat_hybrid_bpr + cat_lightfm_warp + cat_phcf_bpr + cat_knn
cat_other = [m for m in model_names if m not in all_categorized]

categories = {
    'Hybrid-WARP': cat_hybrid_warp,
    'Hybrid-BPR': cat_hybrid_bpr,
    'LightFM-WARP': cat_lightfm_warp,
    'PHCF-BPR': cat_phcf_bpr,
    'KNN': cat_knn,
    'Other': cat_other
}

cat_avg_ndcg = []
cat_names = []

for cat_name, cat_models in categories.items():
    if cat_models:
        avg_ndcg = np.mean([results[m]['ndcg'] for m in cat_models if m in results])
        cat_avg_ndcg.append(avg_ndcg)
        cat_names.append(f"{cat_name} ({len(cat_models)})")

bars = ax.bar(range(len(cat_names)), cat_avg_ndcg, 
              color=['#2E86AB', '#2E86AB', '#F18F01', '#F18F01', '#6A994E', '#BC4B51'],
              alpha=0.8, edgecolor='black', linewidth=1.2)

for bar, val in zip(bars, cat_avg_ndcg):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height,
            f'{val:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

ax.set_xticks(range(len(cat_names)))
ax.set_xticklabels(cat_names, rotation=45, ha='right', fontsize=10)
ax.set_ylabel('–°—Ä–µ–¥–Ω–∏–π nDCG', fontsize=12, fontweight='bold')
ax.set_title('–°—Ä–µ–¥–Ω–∏–π nDCG –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –º–æ–¥–µ–ª–µ–π', fontsize=14, fontweight='bold', pad=15)
ax.grid(axis='y', alpha=0.3, linestyle='--')

plt.tight_layout()
plt.savefig('improved_cf_category_ndcg.png', dpi=300, bbox_inches='tight')
print("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: improved_cf_category_ndcg.png")
plt.close()

print("\n" + "="*70)
print("‚úÖ –£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
print("="*70)
print(f"\nüèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_ndcg[0]}")
print(f"   nDCG: {best_ndcg[1]['ndcg']:.4f}")
print(f"   Precision: {best_ndcg[1]['precision']:.4f}")
print(f"   Recall: {best_ndcg[1]['recall']:.4f}")
print("\n" + "="*70)

