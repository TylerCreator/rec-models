#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ø—Ä–æ–±–ª–µ–º—ã –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

–ò–∑–º–µ–Ω–µ–Ω–∏—è:
1. –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π split –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
2. –†–∞–∑–Ω—ã–µ random seeds –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
3. –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
4. –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
5. Cross-validation –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
"""

import argparse
import json
import logging
import sys
from collections import Counter
from pathlib import Path
from typing import List, Tuple, Dict

import networkx as nx
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    ndcg_score,
    precision_score,
    recall_score,
)
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import LabelBinarizer, LabelEncoder, MultiLabelBinarizer
from torch_geometric.data import Data
from torch_geometric.nn import APPNP, GATConv, GCNConv, SAGEConv, GATv2Conv, BatchNorm, LayerNorm
from tqdm import tqdm

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ==================== –ú–æ–¥–µ–ª–∏ (–∫–æ–ø–∏—Ä—É–µ–º –ª—É—á—à–∏–µ) ====================

class DAGNN(nn.Module):
    def __init__(self, in_channels: int, K: int, dropout: float = 0.5):
        super().__init__()
        self.propagation = APPNP(K=K, alpha=0.1)
        self.att = nn.Parameter(torch.Tensor(K + 1))
        self.dropout = dropout
        self.reset_parameters()

    def reset_parameters(self):
        self.propagation.reset_parameters()
        nn.init.uniform_(self.att, 0.0, 1.0)

    def forward(self, x, edge_index, training=True):
        xs = [x]
        edge_weight = torch.ones(edge_index.size(1), dtype=torch.float32, device=edge_index.device)
        for _ in range(self.propagation.K):
            x = self.propagation.propagate(edge_index, x=x, edge_weight=edge_weight)
            if training:
                x = F.dropout(x, p=self.dropout, training=training)
            xs.append(x)
        out = torch.stack(xs, dim=-1)
        att_weights = F.softmax(self.att, dim=0)
        out = (out * att_weights.view(1, 1, -1)).sum(dim=-1)
        return out


class DAGNNRecommender(nn.Module):
    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int, 
                 K: int = 10, dropout: float = 0.4):
        super().__init__()
        self.dropout = dropout
        
        self.lin1 = nn.Linear(in_channels, hidden_channels)
        self.bn1 = nn.BatchNorm1d(hidden_channels)
        
        self.lin2 = nn.Linear(hidden_channels, hidden_channels)
        self.bn2 = nn.BatchNorm1d(hidden_channels)
        
        self.dagnn = DAGNN(hidden_channels, K, dropout=dropout)
        
        self.lin3 = nn.Linear(hidden_channels, hidden_channels // 2)
        self.bn3 = nn.BatchNorm1d(hidden_channels // 2)
        
        self.lin_out = nn.Linear(hidden_channels // 2, out_channels)

    def forward(self, x, edge_index, training=True):
        x = self.lin1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=training)
        
        identity = x
        x = self.lin2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = x + identity
        x = F.dropout(x, p=self.dropout, training=training)
        
        x = self.dagnn(x, edge_index, training=training)
        
        x = self.lin3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=training)
        
        x = self.lin_out(x)
        return x


class GCNRecommender(nn.Module):
    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int, dropout: float = 0.5):
        super().__init__()
        self.dropout = dropout
        
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.bn1 = LayerNorm(hidden_channels)
        
        self.conv2 = GCNConv(hidden_channels, hidden_channels * 2)
        self.bn2 = LayerNorm(hidden_channels * 2)
        
        self.conv3 = GCNConv(hidden_channels * 2, hidden_channels)
        self.bn3 = LayerNorm(hidden_channels)
        
        self.lin1 = nn.Linear(hidden_channels, hidden_channels // 2)
        self.bn4 = nn.LayerNorm(hidden_channels // 2)
        
        self.lin2 = nn.Linear(hidden_channels // 2, out_channels)

    def forward(self, data, training=True):
        x, edge_index = data.x, data.edge_index
        
        x = self.conv1(x, edge_index)
        x = self.bn1(x)
        x = F.elu(x)
        x = F.dropout(x, p=self.dropout, training=training)
        
        x = self.conv2(x, edge_index)
        x = self.bn2(x)
        x = F.elu(x)
        x = F.dropout(x, p=self.dropout, training=training)
        
        x = self.conv3(x, edge_index)
        x = self.bn3(x)
        x = F.elu(x)
        x = F.dropout(x, p=self.dropout, training=training)
        
        x = self.lin1(x)
        x = self.bn4(x)
        x = F.elu(x)
        x = F.dropout(x, p=self.dropout, training=training)
        
        x = self.lin2(x)
        return x


class GraphSAGERecommender(nn.Module):
    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int, dropout: float = 0.4):
        super().__init__()
        self.dropout = dropout
        
        self.sage1 = SAGEConv(in_channels, hidden_channels, aggr='mean')
        self.ln1 = LayerNorm(hidden_channels)
        
        self.sage2 = SAGEConv(hidden_channels, hidden_channels * 2, aggr='max')
        self.ln2 = LayerNorm(hidden_channels * 2)
        
        self.sage3 = SAGEConv(hidden_channels * 2, hidden_channels, aggr='mean')
        self.ln3 = LayerNorm(hidden_channels)
        
        self.lin1 = nn.Linear(hidden_channels, hidden_channels // 2)
        self.ln4 = nn.LayerNorm(hidden_channels // 2)
        
        self.lin_out = nn.Linear(hidden_channels // 2, out_channels)

    def forward(self, data, training=True):
        x, edge_index = data.x, data.edge_index
        
        x = self.sage1(x, edge_index)
        x = self.ln1(x)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=training)
        
        x = self.sage2(x, edge_index)
        x = self.ln2(x)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=training)
        
        x = self.sage3(x, edge_index)
        x = self.ln3(x)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=training)
        
        x = self.lin1(x)
        x = self.ln4(x)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout * 0.5, training=training)
        
        x = self.lin_out(x)
        return x


class SASRec(nn.Module):
    """
    SASRec - Self-Attentive Sequential Recommendation
    –û–¥–Ω–∞ –∏–∑ —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è sequential —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    - Embedding + Positional Encoding
    - Multi-head Self-Attention blocks
    - Point-wise Feed-Forward
    - Residual connections + LayerNorm
    """
    
    def __init__(self, num_items: int, hidden_size: int = 64, num_heads: int = 2, 
                 num_blocks: int = 2, dropout: float = 0.3, max_len: int = 50, num_classes: int = None):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_items = num_items
        
        # –ï—Å–ª–∏ num_classes –Ω–µ –∑–∞–¥–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º num_items
        if num_classes is None:
            num_classes = num_items
        
        # Embeddings
        self.item_embedding = nn.Embedding(num_items + 1, hidden_size, padding_idx=0)
        self.pos_embedding = nn.Embedding(max_len, hidden_size)
        self.dropout = nn.Dropout(dropout)
        
        # Self-Attention Blocks
        self.attention_blocks = nn.ModuleList([
            nn.TransformerEncoderLayer(
                d_model=hidden_size,
                nhead=num_heads,
                dim_feedforward=hidden_size * 4,
                dropout=dropout,
                activation='relu',
                batch_first=True,
                norm_first=True  # Pre-LN (–±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ)
            )
            for _ in range(num_blocks)
        ])
        
        self.layer_norm = nn.LayerNorm(hidden_size)
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ü–µ–ª–µ–≤—ã–µ –∫–ª–∞—Å—Å—ã (—Å–µ—Ä–≤–∏—Å—ã)
        self.fc = nn.Linear(hidden_size, num_classes)
    
    def forward(self, sequences):
        """
        Args:
            sequences: (batch_size, seq_len)
        Returns:
            (batch_size, num_items)
        """
        batch_size, seq_len = sequences.shape
        
        # Create position indices
        positions = torch.arange(seq_len, device=sequences.device).unsqueeze(0).expand(batch_size, -1)
        
        # Embeddings
        x = self.item_embedding(sequences) + self.pos_embedding(positions)
        x = self.dropout(x)
        
        # Attention mask (–¥–ª—è padding)
        mask = (sequences == 0).to(sequences.device)
        
        # Self-Attention Blocks
        for block in self.attention_blocks:
            x = block(x, src_key_padding_mask=mask)
        
        # Take last item representation
        x = self.layer_norm(x[:, -1, :])
        
        # Prediction
        x = self.fc(x)
        return x


class Caser(nn.Module):
    """
    Caser - Convolutional Sequence Embedding Recommendation
    CNN-based –º–æ–¥–µ–ª—å –¥–ª—è sequential —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    - Embedding Matrix
    - Horizontal Convolutions (skip-gram –ø–∞—Ç—Ç–µ—Ä–Ω—ã)
    - Vertical Convolutions (union-level –ø–∞—Ç—Ç–µ—Ä–Ω—ã)
    - Concatenate + FC layers
    """
    
    def __init__(self, num_items: int, embedding_dim: int = 64, 
                 num_h_filters: int = 16, num_v_filters: int = 4,
                 dropout: float = 0.3, L: int = 5, num_classes: int = None):
        super().__init__()
        self.L = L  # –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è convolution
        self.num_items = num_items
        
        # –ï—Å–ª–∏ num_classes –Ω–µ –∑–∞–¥–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º num_items
        if num_classes is None:
            num_classes = num_items
        
        # Embedding
        self.item_embedding = nn.Embedding(num_items + 1, embedding_dim, padding_idx=0)
        
        # Horizontal convolutional filters (—Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –æ–∫–æ–Ω)
        self.conv_h = nn.ModuleList([
            nn.Conv2d(1, num_h_filters, (i, embedding_dim))
            for i in [2, 3, 4]  # –û–∫–Ω–∞ —Ä–∞–∑–º–µ—Ä–æ–º 2, 3, 4
        ])
        
        # Vertical convolutional filter
        self.conv_v = nn.Conv2d(1, num_v_filters, (L, 1))
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # Fully connected layers
        fc_input_dim = num_h_filters * len(self.conv_h) + num_v_filters * embedding_dim
        
        self.fc1 = nn.Linear(fc_input_dim, 128)
        self.bn1 = nn.BatchNorm1d(128)
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ü–µ–ª–µ–≤—ã–µ –∫–ª–∞—Å—Å—ã (—Å–µ—Ä–≤–∏—Å—ã)
        self.fc2 = nn.Linear(128, num_classes)
    
    def forward(self, sequences):
        """
        Args:
            sequences: (batch_size, seq_len)
        Returns:
            (batch_size, num_items)
        """
        batch_size = sequences.size(0)
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ L —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        seq_len = sequences.size(1)
        if seq_len >= self.L:
            sequences = sequences[:, -self.L:]
        else:
            # Padding —Å–ª–µ–≤–∞
            pad = torch.zeros(batch_size, self.L - seq_len, dtype=torch.long, device=sequences.device)
            sequences = torch.cat([pad, sequences], dim=1)
        
        # Embedding: (batch_size, L, embedding_dim)
        embedded = self.item_embedding(sequences)
        embedded = self.dropout(embedded)
        
        # –î–æ–±–∞–≤–ª—è–µ–º channel dimension –¥–ª—è Conv2d
        embedded = embedded.unsqueeze(1)  # (batch_size, 1, L, embedding_dim)
        
        # Horizontal convolutions
        h_out = []
        for conv in self.conv_h:
            conv_out = F.relu(conv(embedded).squeeze(3))  # (batch_size, num_h_filters, L')
            pool_out = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)  # (batch_size, num_h_filters)
            h_out.append(pool_out)
        h_out = torch.cat(h_out, dim=1)  # Concatenate all horizontal filters
        
        # Vertical convolution
        v_out = F.relu(self.conv_v(embedded).squeeze(2))  # (batch_size, num_v_filters, embedding_dim)
        v_out = v_out.view(batch_size, -1)  # Flatten
        
        # Concatenate horizontal and vertical
        out = torch.cat([h_out, v_out], dim=1)
        out = self.dropout(out)
        
        # FC layers
        out = self.fc1(out)
        out = self.bn1(out)
        out = F.relu(out)
        out = self.dropout(out)
        
        out = self.fc2(out)
        return out


class DAGNNSequential(nn.Module):
    """
    DAGNN + Sequential - –ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≥—Ä–∞—Ñ–æ–≤—ã–µ —Å–≤—è–∑–∏ (DAGNN) —Å –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (GRU)
    
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    1. DAGNN –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–µ—Å—å –≥—Ä–∞—Ñ ‚Üí node embeddings
    2. –ò–∑–≤–ª–µ–∫–∞–µ–º embeddings –¥–ª—è –≤—Å–µ—Ö —É–∑–ª–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    3. GRU –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å embeddings
    4. –§–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ hidden state
    """
    
    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int,
                 K: int = 10, num_gru_layers: int = 2, dropout: float = 0.4):
        super().__init__()
        self.dropout = dropout
        self.hidden_channels = hidden_channels
        self.num_gru_layers = num_gru_layers
        
        # DAGNN –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥—Ä–∞—Ñ–∞
        self.lin1 = nn.Linear(in_channels, hidden_channels)
        self.bn1 = nn.BatchNorm1d(hidden_channels)
        
        self.dagnn = DAGNN(hidden_channels, K, dropout=dropout)
        
        # GRU –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.gru = nn.GRU(
            input_size=hidden_channels,
            hidden_size=hidden_channels,
            num_layers=num_gru_layers,
            batch_first=True,
            dropout=dropout if num_gru_layers > 1 else 0
        )
        
        # Output layers
        self.bn2 = nn.BatchNorm1d(hidden_channels)
        self.lin_out = nn.Linear(hidden_channels, out_channels)
    
    def forward(self, x, edge_index, sequences, lengths, training=True):
        """
        Args:
            x: node features [num_nodes, in_channels]
            edge_index: graph edges [2, num_edges]
            sequences: node indices in sequences [batch_size, seq_len]
            lengths: real sequence lengths [batch_size]
            training: bool
        Returns:
            predictions [batch_size, out_channels]
        """
        # 1. DAGNN: –ø–æ–ª—É—á–∞–µ–º embeddings –¥–ª—è –≤—Å–µ—Ö —É–∑–ª–æ–≤
        x = self.lin1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=training)
        
        node_embeddings = self.dagnn(x, edge_index, training=training)  # [num_nodes, hidden_channels]
        
        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º embeddings –¥–ª—è —É–∑–ª–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö
        batch_size, seq_len = sequences.shape
        
        # –°–æ–∑–¥–∞–µ–º padding embedding (–Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä)
        padding_emb = torch.zeros(1, self.hidden_channels, device=node_embeddings.device)
        node_embeddings_with_padding = torch.cat([padding_emb, node_embeddings], dim=0)
        # –¢–µ–ø–µ—Ä—å –∏–Ω–¥–µ–∫—Å 0 = padding, –∏–Ω–¥–µ–∫—Å—ã 1..N = —Ä–µ–∞–ª—å–Ω—ã–µ —É–∑–ª—ã
        
        # Flatten sequences –¥–ª—è –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏—è
        flat_sequences = sequences.view(-1)  # [batch_size * seq_len]
        
        # –ü–æ–ª—É—á–∞–µ–º embeddings
        seq_embeddings = node_embeddings_with_padding[flat_sequences]  # [batch_size * seq_len, hidden_channels]
        seq_embeddings = seq_embeddings.view(batch_size, seq_len, self.hidden_channels)
        
        # 3. GRU –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        # Pack padded sequence –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        packed = nn.utils.rnn.pack_padded_sequence(
            seq_embeddings, lengths.cpu(), batch_first=True, enforce_sorted=False
        )
        gru_out, hidden = self.gru(packed)
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π hidden state
        last_hidden = hidden[-1]  # [batch_size, hidden_channels]
        
        # 4. –§–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        out = self.bn2(last_hidden)
        out = F.relu(out)
        out = F.dropout(out, p=self.dropout, training=training)
        out = self.lin_out(out)
        
        return out


class DAGNNAttention(nn.Module):
    """
    DAGNN + Attention - –ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç attention mechanism –¥–ª—è –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è —É–∑–ª–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    1. DAGNN –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≥—Ä–∞—Ñ ‚Üí node embeddings
    2. Multi-head attention –ø–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —É–∑–ª–æ–≤
    3. –§–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    """
    
    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int,
                 K: int = 10, num_heads: int = 4, dropout: float = 0.4):
        super().__init__()
        self.dropout = dropout
        self.hidden_channels = hidden_channels
        
        # DAGNN –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥—Ä–∞—Ñ–∞
        self.lin1 = nn.Linear(in_channels, hidden_channels)
        self.bn1 = nn.BatchNorm1d(hidden_channels)
        
        self.dagnn = DAGNN(hidden_channels, K, dropout=dropout)
        
        # Multi-head attention
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_channels,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        self.layer_norm = nn.LayerNorm(hidden_channels)
        
        # Output layers
        self.lin2 = nn.Linear(hidden_channels, hidden_channels // 2)
        self.bn2 = nn.BatchNorm1d(hidden_channels // 2)
        self.lin_out = nn.Linear(hidden_channels // 2, out_channels)
    
    def forward(self, x, edge_index, sequences, mask=None, training=True):
        """
        Args:
            x: node features [num_nodes, in_channels]
            edge_index: graph edges [2, num_edges]
            sequences: node indices in sequences [batch_size, seq_len]
            mask: attention mask [batch_size, seq_len] (optional)
            training: bool
        Returns:
            predictions [batch_size, out_channels]
        """
        # 1. DAGNN: –ø–æ–ª—É—á–∞–µ–º embeddings –¥–ª—è –≤—Å–µ—Ö —É–∑–ª–æ–≤
        x = self.lin1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=training)
        
        node_embeddings = self.dagnn(x, edge_index, training=training)
        
        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º embeddings –¥–ª—è —É–∑–ª–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö
        batch_size, seq_len = sequences.shape
        
        # –°–æ–∑–¥–∞–µ–º padding embedding (–Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä)
        padding_emb = torch.zeros(1, self.hidden_channels, device=node_embeddings.device)
        node_embeddings_with_padding = torch.cat([padding_emb, node_embeddings], dim=0)
        
        flat_sequences = sequences.view(-1)
        seq_embeddings = node_embeddings_with_padding[flat_sequences]
        seq_embeddings = seq_embeddings.view(batch_size, seq_len, self.hidden_channels)
        
        # 3. Self-attention –ø–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        attn_out, attn_weights = self.attention(
            seq_embeddings, seq_embeddings, seq_embeddings,
            key_padding_mask=mask
        )
        
        # Residual connection + LayerNorm
        seq_embeddings = self.layer_norm(seq_embeddings + attn_out)
        
        # 4. Aggregation: –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        # –ò–ª–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å mean/max pooling
        last_output = seq_embeddings[:, -1, :]  # [batch_size, hidden_channels]
        
        # 5. –§–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        out = self.lin2(last_output)
        out = self.bn2(out)
        out = F.relu(out)
        out = F.dropout(out, p=self.dropout, training=training)
        out = self.lin_out(out)
        
        return out


class GRU4Rec(nn.Module):
    """
    GRU4Rec - –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è session-based —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    - Embedding —Å–ª–æ–π –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    - –ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–π GRU –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
    - Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
    - –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    """
    
    def __init__(self, num_items: int, embedding_dim: int = 64, hidden_size: int = 128, 
                 num_layers: int = 2, dropout: float = 0.3, num_classes: int = None):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout
        
        # –ï—Å–ª–∏ num_classes –Ω–µ –∑–∞–¥–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º num_items (backward compatibility)
        if num_classes is None:
            num_classes = num_items
        
        # Embedding –¥–ª—è —É–∑–ª–æ–≤ –≥—Ä–∞—Ñ–∞ (–≤—Å–µ —É–∑–ª—ã, –≤–∫–ª—é—á–∞—è padding)
        self.embedding = nn.Embedding(num_items, embedding_dim, padding_idx=0)
        
        # Multi-layer GRU
        self.gru = nn.GRU(
            input_size=embedding_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
        )
        
        # Batch normalization
        self.bn1 = nn.BatchNorm1d(hidden_size)
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)
        self.bn2 = nn.BatchNorm1d(hidden_size // 2)
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ü–µ–ª–µ–≤—ã–µ –∫–ª–∞—Å—Å—ã (—Å–µ—Ä–≤–∏—Å—ã)
        self.fc2 = nn.Linear(hidden_size // 2, num_classes)
        
        self.dropout_layer = nn.Dropout(dropout)
        
    def forward(self, sequences, lengths=None):
        """
        Args:
            sequences: (batch_size, seq_len) - –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —É–∑–ª–æ–≤
            lengths: (batch_size,) - —Ä–µ–∞–ª—å–Ω—ã–µ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        
        Returns:
            (batch_size, num_items) - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        """
        batch_size = sequences.size(0)
        
        # Embedding
        embedded = self.embedding(sequences)  # (batch_size, seq_len, embedding_dim)
        embedded = self.dropout_layer(embedded)
        
        # GRU
        if lengths is not None:
            # Pack padded sequences –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            packed = nn.utils.rnn.pack_padded_sequence(
                embedded, lengths.cpu(), batch_first=True, enforce_sorted=False
            )
            gru_out, hidden = self.gru(packed)
            gru_out, _ = nn.utils.rnn.pad_packed_sequence(gru_out, batch_first=True)
        else:
            gru_out, hidden = self.gru(embedded)
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π hidden state (last layer, all batch)
        last_hidden = hidden[-1]  # (batch_size, hidden_size)
        
        # Batch normalization
        x = self.bn1(last_hidden)
        x = F.relu(x)
        x = self.dropout_layer(x)
        
        # FC layers
        x = self.fc1(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.dropout_layer(x)
        
        # Output
        x = self.fc2(x)
        
        return x


# ==================== –£—Ç–∏–ª–∏—Ç—ã ====================

def load_dag_from_json(json_path: Path) -> Tuple[nx.DiGraph, List]:
    logger.info(f"Loading DAG from {json_path}")
    
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    dag = nx.DiGraph()
    id_to_mid = {}

    for composition in data:
        for node in composition["nodes"]:
            if "mid" in node:
                id_to_mid[str(node["id"])] = f"service_{node['mid']}"
            else:
                id_to_mid[str(node["id"])] = f"table_{node['id']}"

        for link in composition["links"]:
            source = str(link["source"])
            target = str(link["target"])
            src_node = id_to_mid[source]
            tgt_node = id_to_mid[target]
            dag.add_node(src_node, type='service' if src_node.startswith("service") else 'table')
            dag.add_node(tgt_node, type='service' if tgt_node.startswith("service") else 'table')
            dag.add_edge(src_node, tgt_node)

    logger.info(f"Loaded DAG with {dag.number_of_nodes()} nodes and {dag.number_of_edges()} edges")
    return dag, data


def extract_paths_from_compositions(data: List) -> List[List[str]]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –†–ï–ê–õ–¨–ù–´–ï –ø—É—Ç–∏ –∏–∑ –∫–æ–º–ø–æ–∑–∏—Ü–∏–π (–Ω–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ DFS)
    
    Args:
        data: –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ compositionsDAG.json
    
    Returns:
        List of unique real paths from compositions
    """
    logger.info("Extracting REAL paths from compositions (not synthetic DFS paths)")
    
    all_paths = []
    
    for comp_idx, composition in enumerate(data):
        # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ –¥–ª—è —ç—Ç–æ–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏
        comp_graph = nx.DiGraph()
        id_to_mid = {}
        
        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è —ç—Ç–æ–π –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏
        for node in composition["nodes"]:
            node_id = str(node["id"])
            if "mid" in node:
                node_name = f"service_{node['mid']}"
            else:
                node_name = f"table_{node['id']}"
            id_to_mid[node_id] = node_name
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–±—Ä–∞ –∏–∑ —ç—Ç–æ–π –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏
        for link in composition["links"]:
            source = str(link["source"])
            target = str(link["target"])
            if source in id_to_mid and target in id_to_mid:
                comp_graph.add_edge(id_to_mid[source], id_to_mid[target])
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –ø—Ä–æ—Å—Ç—ã–µ –ø—É—Ç–∏ –≤ —ç—Ç–æ–π –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏
        start_nodes = [n for n in comp_graph.nodes() if comp_graph.in_degree(n) == 0]
        end_nodes = [n for n in comp_graph.nodes() if comp_graph.out_degree(n) == 0]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –ø—É—Ç–∏ –æ—Ç –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∫ –∫–æ–Ω–µ—á–Ω—ã–º —É–∑–ª–∞–º
        for start in start_nodes:
            for end in end_nodes:
                try:
                    for path in nx.all_simple_paths(comp_graph, start, end):
                        if len(path) > 1:
                            all_paths.append(path)
                except nx.NetworkXNoPath:
                    continue
    
    logger.info(f"Extracted {len(all_paths)} REAL paths from {len(data)} compositions")
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    unique_paths = []
    seen = set()
    for path in all_paths:
        path_tuple = tuple(path)
        if path_tuple not in seen:
            seen.add(path_tuple)
            unique_paths.append(path)
    
    logger.info(f"Unique paths: {len(unique_paths)}")
    
    return unique_paths


def build_graph_from_real_paths(paths: List[List[str]]) -> nx.DiGraph:
    """
    –°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¢–û–õ–¨–ö–û —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π –∏–∑ –∫–æ–º–ø–æ–∑–∏—Ü–∏–π
    """
    logger.info("Building graph from real paths...")
    
    path_graph = nx.DiGraph()
    
    for path in paths:
        for i in range(len(path) - 1):
            source = path[i]
            target = path[i + 1]
            
            source_type = 'service' if source.startswith('service') else 'table'
            target_type = 'service' if target.startswith('service') else 'table'
            
            path_graph.add_node(source, type=source_type)
            path_graph.add_node(target, type=target_type)
            path_graph.add_edge(source, target)
    
    logger.info(f"Built graph from paths: {path_graph.number_of_nodes()} nodes, "
                f"{path_graph.number_of_edges()} edges")
    
    return path_graph


def extract_honest_graph_features(graph: nx.DiGraph, X_raw: List) -> np.ndarray:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ß–ï–°–¢–ù–´–ï –≥—Ä–∞—Ñ–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ë–ï–ó –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è target
    
    –ü—Ä–∏–∑–Ω–∞–∫–∏ –æ—Å–Ω–æ–≤–∞–Ω—ã –¢–û–õ–¨–ö–û –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ:
    - –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —É–∑–ª–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
    - –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å–ª–µ–¥—É—é—â–∏—Ö —É–∑–ª–æ–≤ (—Å–æ—Å–µ–¥–µ–π)
    - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –≤—Å–µ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
    
    –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º: target node, —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ target, —Ä–µ–±—Ä–∞ –∫ target
    """
    logger.info("Extracting HONEST graph features (without target leakage)...")
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≥—Ä–∞—Ñ–∞ –æ–¥–∏–Ω —Ä–∞–∑
    pagerank = nx.pagerank(graph)
    in_degree = dict(graph.in_degree())
    out_degree = dict(graph.out_degree())
    
    try:
        betweenness = nx.betweenness_centrality(graph)
    except:
        betweenness = {node: 0.0 for node in graph.nodes()}
    
    try:
        clustering = nx.clustering(graph.to_undirected())
    except:
        clustering = {node: 0.0 for node in graph.nodes()}
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    max_in = max(in_degree.values()) if in_degree else 1
    max_out = max(out_degree.values()) if out_degree else 1
    
    graph_features = []
    
    for context in X_raw:
        features = []
        last_node = context[-1]
        
        # === –ü—Ä–∏–∑–Ω–∞–∫–∏ –ü–û–°–õ–ï–î–ù–ï–ì–û —É–∑–ª–∞ (–æ—Ç–∫—É–¥–∞ –ø–µ—Ä–µ—Ö–æ–¥–∏–º) ===
        features.append(in_degree.get(last_node, 0) / max_in)
        features.append(out_degree.get(last_node, 0) / max_out)
        features.append(pagerank.get(last_node, 0))
        features.append(betweenness.get(last_node, 0))
        features.append(clustering.get(last_node, 0))
        features.append(1 if last_node.startswith('service') else 0)
        
        # === –ü—Ä–∏–∑–Ω–∞–∫–∏ –ö–û–ù–¢–ï–ö–°–¢–ê ===
        features.append(len(context))  # –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        
        # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –≤—Å–µ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
        context_pagerank = [pagerank.get(node, 0) for node in context]
        context_out_deg = [out_degree.get(node, 0) / max_out for node in context]
        
        features.append(np.mean(context_pagerank) if context_pagerank else 0)
        features.append(np.max(context_pagerank) if context_pagerank else 0)
        features.append(np.mean(context_out_deg) if context_out_deg else 0)
        
        # === –ü—Ä–∏–∑–Ω–∞–∫–∏ –í–û–ó–ú–û–ñ–ù–´–• —Å–ª–µ–¥—É—é—â–∏—Ö —É–∑–ª–æ–≤ (—Å–æ—Å–µ–¥–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —É–∑–ª–∞) ===
        successors = list(graph.successors(last_node))
        features.append(len(successors))  # –°–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞
        
        if successors:
            # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Å–æ—Å–µ–¥–µ–π (–∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
            succ_pagerank = [pagerank.get(s, 0) for s in successors]
            succ_in_deg = [in_degree.get(s, 0) / max_in for s in successors]
            
            features.append(np.mean(succ_pagerank))
            features.append(np.max(succ_pagerank))
            features.append(np.mean(succ_in_deg))
            features.append(np.max(succ_in_deg))
            
            # –°–∫–æ–ª—å–∫–æ —Å–æ—Å–µ–¥–µ–π —è–≤–ª—è—é—Ç—Å—è services
            service_count = sum(1 for s in successors if s.startswith('service'))
            features.append(service_count / len(successors) if successors else 0)
        else:
            # –ù–µ—Ç —Å–æ—Å–µ–¥–µ–π - –Ω—É–ª–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            features.extend([0, 0, 0, 0, 0])
        
        graph_features.append(features)
    
    graph_features_array = np.array(graph_features, dtype=np.float32)
    logger.info(f"Extracted honest graph features: shape {graph_features_array.shape}")
    
    return graph_features_array


def create_training_data(paths: List[List[str]]) -> Tuple[List, List]:
    """
    –°–æ–∑–¥–∞–µ—Ç –æ–±—É—á–∞—é—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –ø—É—Ç–µ–π
    
    –õ–æ–≥–∏–∫–∞:
    - –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—É—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –í–°–ï –ø–µ—Ä–µ—Ö–æ–¥—ã
    - –ò—Å–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ—Ö–æ–¥—ã –Ω–∞ —Ç–∞–±–ª–∏—Ü—ã (–æ–Ω–∏ —Å—Ç–∞—Ä—Ç–æ–≤—ã–µ, –Ω–∞ –Ω–∏—Ö –Ω–µ–ª—å–∑—è –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å)
    - –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç–∏ –ª—é–±–æ–π –¥–ª–∏–Ω—ã (–≤–∫–ª—é—á–∞—è –¥–ª–∏–Ω—É 2)
    """
    X_raw = []
    y_raw = []

    for path in paths:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –ø–µ—Ä–µ—Ö–æ–¥—ã –æ—Ç –ø–æ–∑–∏—Ü–∏–∏ 1 –¥–æ –∫–æ–Ω—Ü–∞ (–≤–∫–ª—é—á–∞—è –ø–æ—Å–ª–µ–¥–Ω–∏–π)
        for i in range(1, len(path)):
            context = tuple(path[:i])
            next_step = path[i]
            
            # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ—Ö–æ–¥—ã –Ω–∞ —Ç–∞–±–ª–∏—Ü—ã (–æ–Ω–∏ —Å—Ç–∞—Ä—Ç–æ–≤—ã–µ)
            if next_step.startswith("service"):
                X_raw.append(context)
                y_raw.append(next_step)

    logger.info(f"Created {len(X_raw)} training samples from {len(paths)} paths")
    return X_raw, y_raw


def prepare_pytorch_geometric_data(dag: nx.DiGraph, X_raw: List, y_raw: List, paths: List[List[str]]) -> Tuple[Data, torch.Tensor, torch.Tensor, Dict, Dict]:
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è PyTorch Geometric
    –ì—Ä–∞—Ñ —Å—Ç—Ä–æ–∏—Ç—Å—è –∏–∑ –†–ï–ê–õ–¨–ù–´–• –ø—É—Ç–µ–π
    
    –û–ü–¢–ò–ú–ê–õ–¨–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –¥–ª—è –º–∞–ª–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞:
    - –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç DAG —Å–µ–º–∞–Ω—Ç–∏–∫—É)
    - –ë–ï–ó self-loops (–º–µ–Ω—å—à–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ)
    - –ë–ï–ó —Å–∏–º–º–µ—Ç—Ä–∏–∑–∞—Ü–∏–∏ (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏)
    
    Returns:
        data_pyg: Graph data (–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ, 97 —Ä–µ–±–µ—Ä)
        contexts: Context node indices (in node_map)
        targets: Target service indices (in service_map)
        node_map: Mapping all nodes -> indices
        service_map: Mapping only services -> indices (for predictions)
    """
    logger.info("Preparing PyTorch Geometric data from real paths...")
    
    # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π
    path_graph = build_graph_from_real_paths(paths)
    
    node_list = list(path_graph.nodes)
    node_encoder = LabelEncoder()
    node_ids = node_encoder.fit_transform(node_list)
    node_map = {node: idx for node, idx in zip(node_list, node_ids)}

    # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ –ë–ï–ó –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–π (–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –º–∞–ª–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞)
    edge_index = torch.tensor([[node_map[u], node_map[v]] for u, v in path_graph.edges], dtype=torch.long).t()
    features = [[1, 0] if path_graph.nodes[n]['type'] == 'service' else [0, 1] for n in node_list]
    x = torch.tensor(features, dtype=torch.float)
    data_pyg = Data(x=x, edge_index=edge_index)
    
    logger.info(f"Graph: {edge_index.size(1)} directed edges (no self-loops, no symmetrization)")

    contexts = torch.tensor([node_map[context[-1]] for context in X_raw], dtype=torch.long)
    
    # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ –¢–û–õ–¨–ö–û –¥–ª—è —Å–µ—Ä–≤–∏—Å–æ–≤ (—Ü–µ–ª–µ–≤—ã—Ö –∫–ª–∞—Å—Å–æ–≤)
    unique_services = sorted(set(y_raw))
    service_map = {service: idx for idx, service in enumerate(unique_services)}
    targets = torch.tensor([service_map[y] for y in y_raw], dtype=torch.long)
    
    logger.info(f"Created service_map: {len(service_map)} unique target services")
    logger.info(f"Services: {unique_services}")

    return data_pyg, contexts, targets, node_map, service_map


def prepare_gru4rec_data(X_raw: List, y_raw: List, node_map: Dict, service_map: Dict, max_seq_len: int = 10) -> Tuple:
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è GRU4Rec
    
    Args:
        X_raw: –°–ø–∏—Å–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ (—Ç—É–ø–ª—ã —É–∑–ª–æ–≤)
        y_raw: –°–ø–∏—Å–æ–∫ —Ü–µ–ª–µ–≤—ã—Ö —É–∑–ª–æ–≤ (—Å–µ—Ä–≤–∏—Å–æ–≤)
        node_map: –ú–∞–ø–ø–∏–Ω–≥ –≤—Å–µ—Ö —É–∑–ª–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å—ã (–¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π)
        service_map: –ú–∞–ø–ø–∏–Ω–≥ —Å–µ—Ä–≤–∏—Å–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å—ã (–¥–ª—è —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)
        max_seq_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    
    Returns:
        sequences: (num_samples, max_seq_len) - padded –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–∏–Ω–¥–µ–∫—Å—ã –≤ node_map)
        lengths: (num_samples,) - —Ä–µ–∞–ª—å–Ω—ã–µ –¥–ª–∏–Ω—ã
        targets: (num_samples,) - —Ü–µ–ª–µ–≤—ã–µ —Å–µ—Ä–≤–∏—Å—ã (–∏–Ω–¥–µ–∫—Å—ã –≤ service_map)
    """
    logger.info("Preparing GRU4Rec data")
    
    sequences = []
    lengths = []
    targets_list = []
    
    for context, target in zip(X_raw, y_raw):
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –∏–Ω–¥–µ–∫—Å—ã (–∏—Å–ø–æ–ª—å–∑—É–µ–º node_map –¥–ª—è –≤—Å–µ—Ö —É–∑–ª–æ–≤)
        seq = [node_map[node] + 1 for node in context]  # +1 –¥–ª—è padding_idx=0
        seq_len = len(seq)
        
        # Padding –∏–ª–∏ truncation
        if seq_len < max_seq_len:
            seq = [0] * (max_seq_len - seq_len) + seq  # Padding —Å–ª–µ–≤–∞
        else:
            seq = seq[-max_seq_len:]  # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ max_seq_len —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            seq_len = max_seq_len
        
        sequences.append(seq)
        lengths.append(min(seq_len, max_seq_len))
        # –¶–µ–ª–µ–≤–æ–π —Å–µ—Ä–≤–∏—Å –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑ service_map (—Ç–æ–ª—å–∫–æ —Å–µ—Ä–≤–∏—Å—ã)
        targets_list.append(service_map[target])
    
    sequences = torch.tensor(sequences, dtype=torch.long)
    lengths = torch.tensor(lengths, dtype=torch.long)
    targets_tensor = torch.tensor(targets_list, dtype=torch.long)
    
    logger.info(f"GRU4Rec data: {sequences.shape}, lengths: {lengths.shape}")
    logger.info(f"Targets use service_map: {len(service_map)} classes")
    
    return sequences, lengths, targets_tensor


class FocalLoss(nn.Module):
    """
    Focal Loss –¥–ª—è –±–æ—Ä—å–±—ã —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏
    –£–º–µ–Ω—å—à–∞–µ—Ç –≤–µ—Å "–ª–µ–≥–∫–∏—Ö" –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ "—Å–ª–æ–∂–Ω—ã—Ö"
    """
    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha  # –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
        self.gamma = gamma  # –§–æ–∫—É—Å–∏—Ä—É—é—â–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)
        pt = torch.exp(-ce_loss)
        focal_loss = ((1 - pt) ** self.gamma) * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


def label_smoothing_loss(pred, target, smoothing=0.1, num_classes=None):
    """
    Label smoothing –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    –í–º–µ—Å—Ç–æ one-hot [0, 1, 0] –∏—Å–ø–æ–ª—å–∑—É–µ—Ç [0.05, 0.9, 0.05]
    """
    if num_classes is None:
        num_classes = pred.size(-1)
    
    confidence = 1.0 - smoothing
    smooth_value = smoothing / (num_classes - 1)
    
    one_hot = torch.zeros_like(pred).scatter_(1, target.unsqueeze(1), 1)
    smooth_one_hot = one_hot * confidence + (1 - one_hot) * smooth_value
    
    log_probs = F.log_softmax(pred, dim=-1)
    loss = -(smooth_one_hot * log_probs).sum(dim=-1).mean()
    
    return loss


def evaluate_model_with_ndcg(preds: np.ndarray, true_labels: np.ndarray,
                             proba_preds: np.ndarray = None, name: str = "Model") -> Dict[str, float]:
    metrics = {}
    metrics['accuracy'] = accuracy_score(true_labels, preds)
    metrics['f1'] = f1_score(true_labels, preds, average='macro', zero_division=0)
    metrics['precision'] = precision_score(true_labels, preds, average='macro', zero_division=0)
    metrics['recall'] = recall_score(true_labels, preds, average='macro', zero_division=0)

    logger.info(f"\n{'='*50}")
    logger.info(f"üìä {name} Metrics")
    logger.info(f"{'='*50}")
    logger.info(f"Accuracy:  {metrics['accuracy']:.4f}")
    logger.info(f"F1-score:  {metrics['f1']:.4f}")
    logger.info(f"Precision: {metrics['precision']:.4f}")
    logger.info(f"Recall:    {metrics['recall']:.4f}")
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö
    unique_preds = np.unique(preds)
    logger.info(f"Unique predictions: {len(unique_preds)} (classes: {unique_preds})")
    pred_dist = Counter(preds)
    logger.info(f"Prediction distribution: {dict(sorted(pred_dist.items()))}")
    
    # WARNING –¥–ª—è mode collapse
    if len(unique_preds) < len(np.unique(true_labels)) / 2:
        logger.warning(f"‚ö†Ô∏è  MODE COLLAPSE DETECTED! Predicting only {len(unique_preds)}/{len(np.unique(true_labels))} classes")

    if proba_preds is not None:
        try:
            n_classes = proba_preds.shape[1]
            lb = LabelBinarizer()
            lb.fit(range(n_classes))
            true_bin = lb.transform(true_labels)

            if true_bin.ndim == 1:
                true_bin = np.eye(n_classes)[true_labels]

            metrics['ndcg'] = ndcg_score(true_bin, proba_preds)
            logger.info(f"nDCG:      {metrics['ndcg']:.4f}")
        except Exception as e:
            logger.warning(f"nDCG:      ‚ùå Error: {e}")
            metrics['ndcg'] = None
    else:
        logger.info("nDCG:      Not available")
        metrics['ndcg'] = None

    return metrics


# ==================== –û–±—É—á–µ–Ω–∏–µ —Å —Ä–∞–∑–Ω—ã–º–∏ seeds ====================

def train_model_generic(model, data_pyg, contexts_train, targets_train, contexts_test, targets_test,
                       optimizer, scheduler, epochs, model_name, model_seed):
    """–û–±—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π seed –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    torch.manual_seed(model_seed)
    np.random.seed(model_seed)
    
    best_loss = float('inf')
    patience_counter = 0
    patience = 50

    for epoch in tqdm(range(epochs), desc=f"{model_name} Training"):
        model.train()
        optimizer.zero_grad()
        
        # –†–∞–∑–Ω–∞—è –ª–æ–≥–∏–∫–∞ forward –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        if hasattr(model, 'dagnn'):  # DAGNN
            out = model(data_pyg.x, data_pyg.edge_index, training=True)[contexts_train]
        else:  # GCN, GraphSAGE
            out = model(data_pyg, training=True)[contexts_train]
        
        loss = F.cross_entropy(out, targets_train)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping
        optimizer.step()
        scheduler.step(loss)
        
        if loss.item() < best_loss:
            best_loss = loss.item()
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                logger.info(f"   Early stopping at epoch {epoch}")
                break

    model.eval()
    with torch.no_grad():
        if hasattr(model, 'dagnn'):
            test_output = model(data_pyg.x, data_pyg.edge_index, training=False)[contexts_test]
        else:
            test_output = model(data_pyg, training=False)[contexts_test]
        
        preds = test_output.argmax(dim=1).numpy()
        proba = F.softmax(test_output, dim=1).numpy()

    return preds, proba


def train_model_with_focal_loss(model, data_pyg, contexts_train, targets_train, contexts_test, targets_test,
                                optimizer, scheduler, epochs, model_name, model_seed, 
                                class_weights=None, use_label_smoothing=True):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Å Focal Loss –∏ Label Smoothing
    –ë–æ—Ä–µ—Ç—Å—è —Å mode collapse –∏ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏
    """
    
    torch.manual_seed(model_seed)
    np.random.seed(model_seed)
    
    # –°–æ–∑–¥–∞–µ–º Focal Loss —Å –≤–µ—Å–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤
    if class_weights is not None:
        class_weights_tensor = torch.FloatTensor(class_weights)
        focal_criterion = FocalLoss(alpha=class_weights_tensor, gamma=2.0)
    else:
        focal_criterion = FocalLoss(gamma=2.0)
    
    best_loss = float('inf')
    patience_counter = 0
    patience = 50
    num_classes = None

    for epoch in tqdm(range(epochs), desc=f"{model_name} Training (Focal)"):
        model.train()
        optimizer.zero_grad()
        
        # Forward pass
        if hasattr(model, 'dagnn'):  # DAGNN
            out = model(data_pyg.x, data_pyg.edge_index, training=True)[contexts_train]
        else:  # GCN, GraphSAGE
            out = model(data_pyg, training=True)[contexts_train]
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –∏–∑ –≤—ã—Ö–æ–¥–∞ –º–æ–¥–µ–ª–∏ (–≤ –ø–µ—Ä–≤–æ–π —ç–ø–æ—Ö–µ)
        if num_classes is None:
            num_classes = out.size(-1)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è loss: Focal Loss + Label Smoothing
        if use_label_smoothing:
            focal_loss = focal_criterion(out, targets_train)
            smooth_loss = label_smoothing_loss(out, targets_train, smoothing=0.1, num_classes=num_classes)
            loss = 0.7 * focal_loss + 0.3 * smooth_loss  # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è
        else:
            loss = focal_criterion(out, targets_train)
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step(loss)
        
        if loss.item() < best_loss:
            best_loss = loss.item()
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                logger.info(f"   Early stopping at epoch {epoch}")
                break

    model.eval()
    with torch.no_grad():
        if hasattr(model, 'dagnn'):
            test_output = model(data_pyg.x, data_pyg.edge_index, training=False)[contexts_test]
        else:
            test_output = model(data_pyg, training=False)[contexts_test]
        
        # Temperature scaling –¥–ª—è –ª—É—á—à–µ–π –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏
        temperature = 1.5
        test_output = test_output / temperature
        
        preds = test_output.argmax(dim=1).numpy()
        proba = F.softmax(test_output, dim=1).numpy()

    return preds, proba


def train_sasrec(
    sequences_train, targets_train, sequences_test, targets_test,
    num_items, epochs, hidden_size=64, num_heads=2, num_blocks=2,
    dropout=0.3, lr=0.001, model_seed=42, num_classes=None
):
    """–û–±—É—á–µ–Ω–∏–µ SASRec –º–æ–¥–µ–ª–∏"""
    
    logger.info(f"Training SASRec (hidden={hidden_size}, heads={num_heads}, blocks={num_blocks}, dropout={dropout})...")
    
    torch.manual_seed(model_seed)
    np.random.seed(model_seed)
    
    # –ï—Å–ª–∏ num_classes –Ω–µ –∑–∞–¥–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º num_items
    if num_classes is None:
        num_classes = num_items
    
    model = SASRec(
        num_items=num_items + 1,
        num_classes=num_classes,
        hidden_size=hidden_size,
        num_heads=num_heads,
        num_blocks=num_blocks,
        dropout=dropout,
        max_len=50
    )
    
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=20
    )
    
    best_loss = float('inf')
    patience_counter = 0
    patience = 50
    
    for epoch in tqdm(range(epochs), desc="SASRec Training"):
        model.train()
        optimizer.zero_grad()
        
        out = model(sequences_train)
        loss = F.cross_entropy(out, targets_train)
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step(loss)
        
        if loss.item() < best_loss:
            best_loss = loss.item()
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                logger.info(f"   Early stopping at epoch {epoch}")
                break
    
    model.eval()
    with torch.no_grad():
        test_output = model(sequences_test)
        preds = test_output.argmax(dim=1).numpy()
        proba = F.softmax(test_output, dim=1).numpy()
    
    return preds, proba


def train_dagnn_sequential(
    data_pyg, sequences_train, lengths_train, targets_train,
    sequences_test, lengths_test, targets_test,
    hidden_channels, epochs, K=10, num_gru_layers=2,
    dropout=0.4, lr=0.001, model_seed=42, num_service_classes=None
):
    """–û–±—É—á–µ–Ω–∏–µ DAGNNSequential –º–æ–¥–µ–ª–∏ (DAGNN + GRU)"""
    
    logger.info(f"Training DAGNNSequential (hidden={hidden_channels}, K={K}, GRU_layers={num_gru_layers}, dropout={dropout})...")
    
    torch.manual_seed(model_seed)
    np.random.seed(model_seed)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    if num_service_classes is None:
        num_service_classes = data_pyg.x.size(0)  # Fallback: –≤—Å–µ —É–∑–ª—ã
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = DAGNNSequential(
        in_channels=2,
        hidden_channels=hidden_channels,
        out_channels=num_service_classes,  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–µ—Ä–≤–∏—Å—ã
        K=K,
        num_gru_layers=num_gru_layers,
        dropout=dropout
    )
    
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=20
    )
    
    best_loss = float('inf')
    patience_counter = 0
    patience = 50
    
    for epoch in tqdm(range(epochs), desc="DAGNNSequential Training"):
        model.train()
        optimizer.zero_grad()
        
        # Forward pass
        out = model(
            x=data_pyg.x,
            edge_index=data_pyg.edge_index,
            sequences=sequences_train,
            lengths=lengths_train,
            training=True
        )
        
        loss = F.cross_entropy(out, targets_train)
        
        # Backward pass
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step(loss)
        
        # Early stopping
        if loss.item() < best_loss:
            best_loss = loss.item()
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                logger.info(f"   Early stopping at epoch {epoch}")
                break
    
    # Evaluation
    model.eval()
    with torch.no_grad():
        test_output = model(
            x=data_pyg.x,
            edge_index=data_pyg.edge_index,
            sequences=sequences_test,
            lengths=lengths_test,
            training=False
        )
        preds = test_output.argmax(dim=1).numpy()
        proba = F.softmax(test_output, dim=1).numpy()
    
    return preds, proba


def train_dagnn_attention(
    data_pyg, sequences_train, targets_train,
    sequences_test, targets_test,
    hidden_channels, epochs, K=10, num_heads=4,
    dropout=0.4, lr=0.001, model_seed=42, num_service_classes=None
):
    """–û–±—É—á–µ–Ω–∏–µ DAGNNAttention –º–æ–¥–µ–ª–∏ (DAGNN + Attention)"""
    
    logger.info(f"Training DAGNNAttention (hidden={hidden_channels}, K={K}, heads={num_heads}, dropout={dropout})...")
    
    torch.manual_seed(model_seed)
    np.random.seed(model_seed)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    if num_service_classes is None:
        num_service_classes = data_pyg.x.size(0)  # Fallback: –≤—Å–µ —É–∑–ª—ã
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = DAGNNAttention(
        in_channels=2,
        hidden_channels=hidden_channels,
        out_channels=num_service_classes,  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–µ—Ä–≤–∏—Å—ã
        K=K,
        num_heads=num_heads,
        dropout=dropout
    )
    
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=20
    )
    
    best_loss = float('inf')
    patience_counter = 0
    patience = 50
    
    for epoch in tqdm(range(epochs), desc="DAGNNAttention Training"):
        model.train()
        optimizer.zero_grad()
        
        # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è padding
        mask = (sequences_train == 0)
        
        # Forward pass
        out = model(
            x=data_pyg.x,
            edge_index=data_pyg.edge_index,
            sequences=sequences_train,
            mask=mask,
            training=True
        )
        
        loss = F.cross_entropy(out, targets_train)
        
        # Backward pass
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step(loss)
        
        # Early stopping
        if loss.item() < best_loss:
            best_loss = loss.item()
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                logger.info(f"   Early stopping at epoch {epoch}")
                break
    
    # Evaluation
    model.eval()
    with torch.no_grad():
        mask_test = (sequences_test == 0)
        test_output = model(
            x=data_pyg.x,
            edge_index=data_pyg.edge_index,
            sequences=sequences_test,
            mask=mask_test,
            training=False
        )
        preds = test_output.argmax(dim=1).numpy()
        proba = F.softmax(test_output, dim=1).numpy()
    
    return preds, proba


def train_caser(
    sequences_train, targets_train, sequences_test, targets_test,
    num_items, epochs, embedding_dim=64, num_h_filters=16,
    num_v_filters=4, dropout=0.3, lr=0.001, model_seed=42, num_classes=None
):
    """–û–±—É—á–µ–Ω–∏–µ Caser –º–æ–¥–µ–ª–∏"""
    
    logger.info(f"Training Caser (embedding={embedding_dim}, h_filters={num_h_filters}, v_filters={num_v_filters}, dropout={dropout})...")
    
    torch.manual_seed(model_seed)
    np.random.seed(model_seed)
    
    # –ï—Å–ª–∏ num_classes –Ω–µ –∑–∞–¥–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º num_items
    if num_classes is None:
        num_classes = num_items
    
    model = Caser(
        num_items=num_items + 1,
        num_classes=num_classes,
        embedding_dim=embedding_dim,
        num_h_filters=num_h_filters,
        num_v_filters=num_v_filters,
        dropout=dropout,
        L=5
    )
    
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=20
    )
    
    best_loss = float('inf')
    patience_counter = 0
    patience = 50
    
    for epoch in tqdm(range(epochs), desc="Caser Training"):
        model.train()
        optimizer.zero_grad()
        
        out = model(sequences_train)
        loss = F.cross_entropy(out, targets_train)
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step(loss)
        
        if loss.item() < best_loss:
            best_loss = loss.item()
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                logger.info(f"   Early stopping at epoch {epoch}")
                break
    
    model.eval()
    with torch.no_grad():
        test_output = model(sequences_test)
        preds = test_output.argmax(dim=1).numpy()
        proba = F.softmax(test_output, dim=1).numpy()
    
    return preds, proba


def train_gru4rec(
    sequences_train, lengths_train, targets_train,
    sequences_test, lengths_test, targets_test,
    num_items, epochs, embedding_dim=64, hidden_size=128,
    num_layers=2, dropout=0.4, lr=0.001, model_seed=42, num_classes=None
):
    """–û–±—É—á–µ–Ω–∏–µ GRU4Rec –º–æ–¥–µ–ª–∏"""
    
    logger.info(f"Training GRU4Rec (embedding={embedding_dim}, hidden={hidden_size}, layers={num_layers}, dropout={dropout})...")
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed
    torch.manual_seed(model_seed)
    np.random.seed(model_seed)
    
    # –ï—Å–ª–∏ num_classes –Ω–µ –∑–∞–¥–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º num_items (backward compatibility)
    if num_classes is None:
        num_classes = num_items
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = GRU4Rec(
        num_items=num_items,  # –î–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–≤—Å–µ —É–∑–ª—ã + padding)
        num_classes=num_classes,  # –î–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (—Ç–æ–ª—å–∫–æ —Å–µ—Ä–≤–∏—Å—ã)
        embedding_dim=embedding_dim,
        hidden_size=hidden_size,
        num_layers=num_layers,
        dropout=dropout
    )
    
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=20
    )
    
    best_loss = float('inf')
    patience_counter = 0
    patience = 50
    
    for epoch in tqdm(range(epochs), desc="GRU4Rec Training"):
        model.train()
        optimizer.zero_grad()
        
        # Forward pass
        out = model(sequences_train, lengths_train)
        loss = F.cross_entropy(out, targets_train)
        
        # Backward pass
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step(loss)
        
        # Early stopping
        if loss.item() < best_loss:
            best_loss = loss.item()
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                logger.info(f"   Early stopping at epoch {epoch}")
                break
    
    # Evaluation
    model.eval()
    with torch.no_grad():
        test_output = model(sequences_test, lengths_test)
        preds = test_output.argmax(dim=1).numpy()
        proba = F.softmax(test_output, dim=1).numpy()
    
    return preds, proba


def main():
    parser = argparse.ArgumentParser(description="FINAL DAG-based Recommender with Fixed Results")
    parser.add_argument("--data", type=str, default="compositionsDAG.json")
    parser.add_argument("--test-size", type=float, default=0.3)
    parser.add_argument("--epochs", type=int, default=200)
    parser.add_argument("--hidden-channels", type=int, default=64)
    parser.add_argument("--learning-rate", type=float, default=0.001)
    parser.add_argument("--dropout", type=float, default=0.4)
    parser.add_argument("--random-seed", type=int, default=42)
    
    args = parser.parse_args()

    # Load data
    data_path = Path(args.data)
    if not data_path.exists():
        logger.error(f"Data file not found: {data_path}")
        sys.exit(1)

    dag, compositions_data = load_dag_from_json(data_path)
    paths = extract_paths_from_compositions(compositions_data)
    X_raw, y_raw = create_training_data(paths)
    
    # –ü–æ—Å—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —á–µ—Å—Ç–Ω—ã—Ö —Ñ–∏—á–µ–π
    path_graph = build_graph_from_real_paths(paths)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ß–ï–°–¢–ù–´–ï –≥—Ä–∞—Ñ–æ–≤—ã–µ —Ñ–∏—á–∏ (–±–µ–∑ —É—Ç–µ—á–∫–∏ target)
    honest_graph_features = extract_honest_graph_features(path_graph, X_raw)

    # Vectorize
    logger.info("Vectorizing data...")
    mlb = MultiLabelBinarizer()
    X_base = mlb.fit_transform(X_raw)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å —á–µ—Å—Ç–Ω—ã–º–∏ –≥—Ä–∞—Ñ–æ–≤—ã–º–∏
    logger.info(f"Combining base features {X_base.shape} with honest graph features {honest_graph_features.shape}")
    X_combined = np.hstack([X_base, honest_graph_features])
    logger.info(f"Combined feature matrix shape: {X_combined.shape}")
    
    le = LabelEncoder()
    y = le.fit_transform(y_raw)

    # –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π split (—Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏)
    y_counts = Counter(y)
    min_samples = min(y_counts.values())
    
    # Split –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    if min_samples >= 2:
        logger.info("Using STRATIFIED split to ensure balanced classes...")
        X_base_train, X_base_test, y_train, y_test = train_test_split(
            X_base, y, test_size=args.test_size, random_state=args.random_seed, stratify=y
        )
    else:
        logger.warning(f"Too few samples for stratification (min={min_samples}). Using random split.")
        X_base_train, X_base_test, y_train, y_test = train_test_split(
            X_base, y, test_size=args.test_size, random_state=args.random_seed
        )
    
    logger.info(f"Train samples: {len(X_base_train)}, Test samples: {len(X_base_test)}")
    logger.info(f"Train class distribution: {Counter(y_train)}")
    logger.info(f"Test class distribution: {Counter(y_test)}")

    # Prepare PyG data from real paths
    data_pyg, contexts, targets, node_map, service_map = prepare_pytorch_geometric_data(
        dag, X_raw, y_raw, paths
    )
    
    logger.info(f"\n{'='*60}")
    logger.info(f"üìä DATA SUMMARY:")
    logger.info(f"  Total nodes in graph: {len(node_map)}")
    logger.info(f"  Target service classes: {len(service_map)}")
    logger.info(f"  Models will predict {len(service_map)} classes (services only)")
    logger.info(f"  Graph edges: {data_pyg.edge_index.size(1)} (directed, no self-loops)")
    logger.info(f"{'='*60}\n")
    
    # –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π split –¥–ª—è –≥—Ä–∞—Ñ–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (—Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π)
    target_counts_graph = Counter(targets.numpy())
    min_samples_graph = min(target_counts_graph.values())
    
    if min_samples_graph >= 2:
        contexts_train, contexts_test, targets_train, targets_test = train_test_split(
            contexts, targets, test_size=args.test_size, random_state=args.random_seed,
            stratify=targets.numpy()
        )
    else:
        logger.warning(f"Too few samples for stratification in graph data (min={min_samples_graph}). Using random split.")
        contexts_train, contexts_test, targets_train, targets_test = train_test_split(
            contexts, targets, test_size=args.test_size, random_state=args.random_seed
        )
    logger.info(f"Graph train samples: {len(contexts_train)}, test samples: {len(contexts_test)}")

    results = {}

    # Popularity baseline
    logger.info("Training Popularity baseline...")
    counter = Counter(y_raw)
    top_label = counter.most_common(1)[0][0]
    pop_preds = np.array([le.transform([top_label])[0]] * len(y_test))
    pop_proba = np.zeros((len(y_test), len(le.classes_)))
    top_label_index = le.transform([top_label])[0]
    pop_proba[:, top_label_index] = 1
    results['Popularity'] = evaluate_model_with_ndcg(
        pop_preds, y_test, proba_preds=pop_proba, name="Popularity"
    )

    # GCN - —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º seed (—É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
    logger.info(f"Training GCN with seed={args.random_seed + 1}...")
    torch.manual_seed(args.random_seed + 1)
    gcn = GCNRecommender(
        in_channels=2, 
        hidden_channels=args.hidden_channels * 2,  # –ë–æ–ª—å—à–µ capacity
        out_channels=len(service_map),  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–µ—Ä–≤–∏—Å—ã
        dropout=0.3  # –ú–µ–Ω—å—à–µ dropout –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    )
    opt_gcn = torch.optim.Adam(gcn.parameters(), lr=args.learning_rate * 1.5, weight_decay=5e-5)
    sched_gcn = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_gcn, mode='min', factor=0.5, patience=20)
    
    gcn_preds, gcn_proba = train_model_generic(
        gcn, data_pyg, contexts_train, targets_train, contexts_test, targets_test,
        opt_gcn, sched_gcn, args.epochs, "GCN", args.random_seed + 1
    )
    results['GCN'] = evaluate_model_with_ndcg(
        gcn_preds, targets_test.numpy(), proba_preds=gcn_proba, name="GCN"
    )

    # DAGNN - —Å –¥—Ä—É–≥–∏–º seed
    logger.info(f"Training DAGNN with seed={args.random_seed + 2}...")
    torch.manual_seed(args.random_seed + 2)
    dagnn = DAGNNRecommender(
        in_channels=2, hidden_channels=args.hidden_channels,
        out_channels=len(service_map), dropout=0.4  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–µ—Ä–≤–∏—Å—ã
    )
    opt_dagnn = torch.optim.Adam(dagnn.parameters(), lr=args.learning_rate * 0.8, weight_decay=1e-4)
    sched_dagnn = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_dagnn, mode='min', factor=0.5, patience=20)
    
    dagnn_preds, dagnn_proba = train_model_generic(
        dagnn, data_pyg, contexts_train, targets_train, contexts_test, targets_test,
        opt_dagnn, sched_dagnn, args.epochs, "DAGNN", args.random_seed + 2
    )
    results['DAGNN'] = evaluate_model_with_ndcg(
        dagnn_preds, targets_test.numpy(), proba_preds=dagnn_proba, name="DAGNN"
    )

    # DAGNN IMPROVED - —Å Focal Loss –∏ Label Smoothing
    logger.info("\n" + "="*70)
    logger.info(f"Training DAGNN IMPROVED (Focal Loss + Label Smoothing) seed={args.random_seed + 20}...")
    
    # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–æ—Ä—å–±—ã —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å—é
    train_class_counts = Counter(targets_train.numpy())
    total_samples = sum(train_class_counts.values())
    num_all_classes = len(service_map)  # –í—Å–µ–≥–æ —Å–µ—Ä–≤–∏—Å–æ–≤ (—Ü–µ–ª–µ–≤—ã—Ö –∫–ª–∞—Å—Å–æ–≤)
    
    # Inverse frequency weights –¥–ª—è –í–°–ï–• –∫–ª–∞—Å—Å–æ–≤
    class_weights = []
    for i in range(num_all_classes):
        count = train_class_counts.get(i, 1)  # –ï—Å–ª–∏ –∫–ª–∞—Å—Å–∞ –Ω–µ—Ç, –≤–µ—Å = —Å—Ä–µ–¥–Ω–µ–º—É
        if count > 0:
            weight = total_samples / (len(train_class_counts) * count)
        else:
            weight = 1.0  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –≤–µ—Å –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ train
        class_weights.append(weight)
    
    logger.info(f"Class weights (first 5): {[f'{w:.2f}' for w in class_weights[:5]]}")
    logger.info(f"Total service classes: {num_all_classes}, Present in train: {len(train_class_counts)}")
    logger.info(f"Using Focal Loss (gamma=2.0) + Label Smoothing (0.1)")
    
    torch.manual_seed(args.random_seed + 20)
    dagnn_improved = DAGNNRecommender(
        in_channels=2, 
        hidden_channels=args.hidden_channels * 2,  # –ë–æ–ª—å—à–µ capacity
        out_channels=len(service_map),  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–µ—Ä–≤–∏—Å—ã
        K=15,  # –ë–æ–ª—å—à–µ propagation steps
        dropout=0.5  # –ë–æ–ª—å—à–µ dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
    )
    opt_dagnn_improved = torch.optim.Adam(
        dagnn_improved.parameters(), 
        lr=args.learning_rate * 0.5,  # –ú–µ–Ω—å—à–µ learning rate
        weight_decay=5e-4  # –ë–æ–ª—å—à–µ weight decay
    )
    sched_dagnn_improved = torch.optim.lr_scheduler.ReduceLROnPlateau(
        opt_dagnn_improved, mode='min', factor=0.5, patience=25
    )
    
    dagnn_improved_preds, dagnn_improved_proba = train_model_with_focal_loss(
        dagnn_improved, data_pyg, 
        contexts_train, targets_train, 
        contexts_test, targets_test,
        opt_dagnn_improved, sched_dagnn_improved, args.epochs, 
        "DAGNN-Improved", args.random_seed + 20,
        class_weights=class_weights,
        use_label_smoothing=True
    )
    results['DAGNN-Improved (Focal)'] = evaluate_model_with_ndcg(
        dagnn_improved_preds, targets_test.numpy(), 
        proba_preds=dagnn_improved_proba, 
        name="DAGNN-Improved (Focal Loss + Label Smoothing)"
    )

    # GraphSAGE - —Å –µ—â–µ –¥—Ä—É–≥–∏–º seed
    logger.info(f"Training GraphSAGE with seed={args.random_seed + 3}...")
    torch.manual_seed(args.random_seed + 3)
    sage = GraphSAGERecommender(
        in_channels=2, hidden_channels=args.hidden_channels,
        out_channels=len(service_map), dropout=0.4  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–µ—Ä–≤–∏—Å—ã
    )
    opt_sage = torch.optim.Adam(sage.parameters(), lr=args.learning_rate, weight_decay=1e-4)
    sched_sage = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_sage, mode='min', factor=0.5, patience=20)
    
    sage_preds, sage_proba = train_model_generic(
        sage, data_pyg, contexts_train, targets_train, contexts_test, targets_test,
        opt_sage, sched_sage, args.epochs, "GraphSAGE", args.random_seed + 3
    )
    results['GraphSAGE'] = evaluate_model_with_ndcg(
        sage_preds, targets_test.numpy(), proba_preds=sage_proba, name="GraphSAGE"
    )

    # GRU4Rec - —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
    logger.info(f"Training GRU4Rec with seed={args.random_seed + 4}...")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è GRU4Rec
    sequences_all, lengths_all, targets_gru = prepare_gru4rec_data(X_raw, y_raw, node_map, service_map, max_seq_len=10)
    
    # Split –¥–ª—è Sequential –º–æ–¥–µ–ª–µ–π
    targets_gru_counts = Counter(targets_gru.numpy())
    min_samples_gru = min(targets_gru_counts.values())
    
    if min_samples_gru >= 2:
        sequences_train, sequences_test, lengths_train, lengths_test, targets_gru_train, targets_gru_test = train_test_split(
            sequences_all, lengths_all, targets_gru,
            test_size=args.test_size, 
            random_state=args.random_seed,
            stratify=targets_gru.numpy()
        )
    else:
        logger.warning(f"Too few samples for stratification in Sequential data (min={min_samples_gru}). Using random split.")
        sequences_train, sequences_test, lengths_train, lengths_test, targets_gru_train, targets_gru_test = train_test_split(
            sequences_all, lengths_all, targets_gru,
            test_size=args.test_size, 
            random_state=args.random_seed
        )
    
    logger.info(f"Sequential models train: {len(sequences_train)}, test: {len(sequences_test)}")
    
    # –û–±—É—á–µ–Ω–∏–µ GRU4Rec
    gru4rec_preds, gru4rec_proba = train_gru4rec(
        sequences_train, lengths_train, targets_gru_train,
        sequences_test, lengths_test, targets_gru_test,
        num_items=len(node_map) + 1,  # +1 –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–≤–∫–ª—é—á–∞—è padding_idx=0)
        num_classes=len(service_map),  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–µ—Ä–≤–∏—Å—ã
        epochs=args.epochs,
        embedding_dim=64,
        hidden_size=args.hidden_channels * 2,  # –ë–æ–ª—å—à–µ –¥–ª—è RNN
        num_layers=2,
        dropout=args.dropout,
        lr=args.learning_rate,
        model_seed=args.random_seed + 4
    )
    results['GRU4Rec'] = evaluate_model_with_ndcg(
        gru4rec_preds, targets_gru_test.numpy(), proba_preds=gru4rec_proba, name="GRU4Rec"
    )

    # SASRec - Self-Attention –º–æ–¥–µ–ª—å
    logger.info(f"Training SASRec with seed={args.random_seed + 5}...")
    sasrec_preds, sasrec_proba = train_sasrec(
        sequences_train, targets_gru_train, sequences_test, targets_gru_test,
        num_items=len(node_map) + 1,  # +1 –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–≤–∫–ª—é—á–∞—è padding_idx=0)
        num_classes=len(service_map),  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–µ—Ä–≤–∏—Å—ã
        epochs=args.epochs,
        hidden_size=args.hidden_channels,
        num_heads=2,
        num_blocks=2,
        dropout=args.dropout,
        lr=args.learning_rate,
        model_seed=args.random_seed + 5
    )
    results['SASRec'] = evaluate_model_with_ndcg(
        sasrec_preds, targets_gru_test.numpy(), proba_preds=sasrec_proba, name="SASRec"
    )

    # Caser - CNN –º–æ–¥–µ–ª—å
    logger.info(f"Training Caser with seed={args.random_seed + 6}...")
    caser_preds, caser_proba = train_caser(
        sequences_train, targets_gru_train, sequences_test, targets_gru_test,
        num_items=len(node_map) + 1,  # +1 –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–≤–∫–ª—é—á–∞—è padding_idx=0)
        num_classes=len(service_map),  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–µ—Ä–≤–∏—Å—ã
        epochs=args.epochs,
        embedding_dim=64,
        num_h_filters=16,
        num_v_filters=4,
        dropout=args.dropout,
        lr=args.learning_rate,
        model_seed=args.random_seed + 6
    )
    results['Caser'] = evaluate_model_with_ndcg(
        caser_preds, targets_gru_test.numpy(), proba_preds=caser_proba, name="Caser"
    )

    # DAGNNSequential - –ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å (DAGNN + GRU)
    logger.info(f"\nTraining DAGNNSequential (HYBRID: Graph + Sequence) with seed={args.random_seed + 7}...")
    dagnn_seq_preds, dagnn_seq_proba = train_dagnn_sequential(
        data_pyg,
        sequences_train, lengths_train, targets_gru_train,
        sequences_test, lengths_test, targets_gru_test,
        hidden_channels=args.hidden_channels,
        epochs=args.epochs,
        K=10,
        num_gru_layers=2,
        dropout=args.dropout,
        lr=args.learning_rate,
        model_seed=args.random_seed + 7,
        num_service_classes=len(service_map)  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–µ—Ä–≤–∏—Å—ã
    )
    results['DAGNNSequential (DAGNN+GRU)'] = evaluate_model_with_ndcg(
        dagnn_seq_preds, targets_gru_test.numpy(), proba_preds=dagnn_seq_proba, 
        name="DAGNNSequential (DAGNN+GRU)"
    )

    # DAGNNAttention - –ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å (DAGNN + Attention)
    logger.info(f"\nTraining DAGNNAttention (HYBRID: Graph + Attention) with seed={args.random_seed + 8}...")
    dagnn_att_preds, dagnn_att_proba = train_dagnn_attention(
        data_pyg,
        sequences_train, targets_gru_train,
        sequences_test, targets_gru_test,
        hidden_channels=args.hidden_channels,
        epochs=args.epochs,
        K=10,
        num_heads=4,
        dropout=args.dropout,
        lr=args.learning_rate,
        model_seed=args.random_seed + 8,
        num_service_classes=len(service_map)  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–µ—Ä–≤–∏—Å—ã
    )
    results['DAGNNAttention (DAGNN+Attn)'] = evaluate_model_with_ndcg(
        dagnn_att_preds, targets_gru_test.numpy(), proba_preds=dagnn_att_proba, 
        name="DAGNNAttention (DAGNN+Attention)"
    )

    # Summary
    logger.info("\n" + "="*70)
    logger.info("üèÜ –ò–¢–û–ì–û–í–û–ï –°–†–ê–í–ù–ï–ù–ò–ï")
    logger.info("="*70)
    
    sorted_results = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)
    
    for rank, (model_name, metrics) in enumerate(sorted_results, 1):
        logger.info(f"\n#{rank} {model_name}:")
        for metric_name, value in metrics.items():
            if value is not None:
                logger.info(f"     {metric_name}: {value:.4f}")

    # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–∏–π
    logger.info("\n" + "="*70)
    logger.info("üîç –ê–ù–ê–õ–ò–ó –†–ê–ó–õ–ò–ß–ò–ô")
    logger.info("="*70)
    accuracies = [m['accuracy'] for m in results.values()]
    logger.info(f"Min accuracy: {min(accuracies):.4f}")
    logger.info(f"Max accuracy: {max(accuracies):.4f}")
    logger.info(f"Range: {max(accuracies) - min(accuracies):.4f}")
    logger.info(f"Std dev: {np.std(accuracies):.4f}")


if __name__ == "__main__":
    main()

