# ğŸ”„ ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° GRU4Rec: Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ

**ĞŸĞ¾Ğ»Ğ½Ğ¾Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ GRU4Rec (Session-based Recommendations with Recurrent Neural Networks)**

Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ GRU4Rec, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ DAG sequence recommendation - Ğ¾Ñ‚ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ (Hidasi et al., ICLR 2016) Ğ´Ğ¾ Ğ½Ğ°ÑˆĞµĞ¹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ DAG-aware Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸.

---

## ğŸ“‹ Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ

### ĞÑĞ½Ğ¾Ğ²Ñ‹
1. [Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ](#1-Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ-Ğ¸-Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ)
2. [ĞÑ‚ ÑĞµÑÑĞ¸Ğ¹ Ğº DAG Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼](#2-Ğ¾Ñ‚-ÑĞµÑÑĞ¸Ğ¹-Ğº-dag-Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼)
3. [ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° GRU4Rec](#3-Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°-gru4rec)
4. [Embedding Layer](#4-embedding-layer)
5. [GRU Layer](#5-gru-layer)
6. [Output Layer](#6-output-layer)
7. [DAG Structure Masking](#7-dag-structure-masking)

### ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸
8. [Loss Functions](#8-loss-functions)
9. [Negative Sampling](#9-negative-sampling)
10. [Dropout Strategies](#10-dropout-strategies)
11. [Training Loop](#11-training-loop)
12. [ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Forward Pass](#12-Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹-forward-pass)

### ĞĞ½Ğ°Ğ»Ğ¸Ğ·
13. [Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ GRU4Rec](#13-ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ-Ñ-Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼-gru4rec)
14. [Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ DAGNN Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸](#14-ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ-Ñ-dagnn-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸)
15. [Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹](#15-ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ-Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹)
16. [Best Practices](#16-best-practices)

---

## 1. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ

### 1.1 ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° GRU4Rec

**Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ:** [Session-based Recommendations with Recurrent Neural Networks](https://arxiv.org/abs/1511.06939) (Hidasi et al., ICLR 2016)

**Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ°:** ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ item, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ²Ñ‹Ğ±ĞµÑ€ĞµÑ‚ Ğ² ÑĞµÑÑĞ¸Ğ¸.

```
Session Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ:
[Item1] â†’ [Item2] â†’ [Item3] â†’ ?
 phone     case      charger    headphones?
```

**ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸:**
- ĞŸĞ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (temporal order Ğ²Ğ°Ğ¶ĞµĞ½)
- ĞŸĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ° ÑĞµÑÑĞ¸Ğ¹
- ĞĞµÑ‚ user ID (anonymous sessions)
- Implicit feedback (ĞºĞ»Ğ¸ĞºĞ¸, Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ñ‹)

---

### 1.2 ĞĞ°ÑˆĞ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ DAG

**Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ°:** ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞµÑ€Ğ²Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ².

```
ĞŸÑƒÑ‚ÑŒ Ğ² DAG ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸:
[Table] â†’ [Service1] â†’ [Service2] â†’ ?
                                      Service3?
```

**ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ñ:**
- Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (DAG topology)
- Ğ¡Ñ‚Ñ€Ğ¾Ğ³Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² (DAG edges)
- ĞšĞ¾Ğ½Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° (ÑÑƒĞ¶ĞµĞ½Ğ¸Ğµ, Ğ½Ğµ Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ğµ)
- ĞœĞ°Ğ»Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ (711 training examples)

---

### 1.3 ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ GRU4Rec Ğ´Ğ»Ñ DAG?

**ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸:**

1. **Ğ•ÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°:**
   ```
   Table â†’ Service1 â†’ Service2 â†’ Service3
   (Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹/Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº)
   ```

2. **ĞŸĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°:**
   ```
   ĞšĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹: [Table] â†’ Service1?
   Ğ”Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹: [Table, S1, S2, S3, S4] â†’ S5?
   ```

3. **Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…:**
   - GRU: ~40K Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
   - Graph models: ~50-400K Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
   - ĞœĞµĞ½ÑŒÑˆĞµ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ

4. **Ğ£Ñ‡ĞµÑ‚ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ°:**
   ```
   [Table, S1, S2] â‰  [S2, S1, Table]
   ĞŸĞ¾Ñ€ÑĞ´Ğ¾Ğº Ğ²Ğ°Ğ¶ĞµĞ½!
   ```

---

## 2. ĞÑ‚ ÑĞµÑÑĞ¸Ğ¹ Ğº DAG Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼

### 2.1 ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…

**Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ:**
```json
{
  "composition": {
    "nodes": [
      {"id": "n1", "mid": "1002132"},  // Table
      {"id": "n2", "mid": "308"},      // Service
      {"id": "n3", "mid": "309"}       // Service
    ],
    "links": [
      {"source": "n1", "target": "n2"},
      {"source": "n2", "target": "n3"}
    ]
  }
}
```

**Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿ÑƒÑ‚ĞµĞ¹:**
```python
paths = extract_paths_from_compositions(compositions)
# Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: 943 Ğ¿ÑƒÑ‚Ğ¸ Ğ¸Ğ· 943 ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹

# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¿ÑƒÑ‚Ğ¸:
path = ['table_1002132', 'service_308', 'service_309', 'service_312']
```

---

### 2.2 Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ training pairs

```python
def create_training_pairs(paths):
    X, y = [], []
    for path in paths:
        for i in range(1, len(path)):
            context = tuple(path[:i])    # ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚
            target = path[i]             # Ğ¡Ğ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚
            
            if target.startswith("service"):  # ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞµÑ€Ğ²Ğ¸ÑÑ‹
                X.append(context)
                y.append(target)
    return X, y
```

**ĞŸÑ€Ğ¸Ğ¼ĞµÑ€:**
```
ĞŸÑƒÑ‚ÑŒ: ['table_1002132', 'service_308', 'service_309']

ĞŸĞ°Ñ€Ñ‹:
  Context: ('table_1002132',)              â†’ Target: 'service_308'
  Context: ('table_1002132', 'service_308') â†’ Target: 'service_309'
```

**Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚:**
- **1,016 training pairs** Ğ¸Ğ· 943 Ğ¿ÑƒÑ‚ĞµĞ¹
- **Split:** 711 train / 305 test (70/30)
- **15 ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²** (ĞºĞ»Ğ°ÑÑĞ¾Ğ²)

---

### 2.3 ĞšĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¸Ğ½Ğ´ĞµĞºÑÑ‹

```python
# Node mapping (Ğ²ÑĞµ 50 ÑƒĞ·Ğ»Ğ¾Ğ²)
node_map = {
    'table_1002132': 0,
    'table_1002133': 1,
    ...
    'service_308': 35,
    'service_309': 36,
    ...
}

# Service mapping (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 15 Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²)
service_map = {
    'service_306': 0,
    'service_307': 1,
    'service_308': 2,
    'service_309': 3,
    ...
    'service_397': 14
}
```

**ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ:**
```python
# Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€:
context = ('table_1002132', 'service_308')
target = 'service_309'

# ĞŸĞ¾ÑĞ»Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ:
context_indices = [0+1, 35+1] = [1, 36]  # +1 Ğ´Ğ»Ñ padding_idx=0
target_index = 3  # Ğ² service_map

# Ğ¡ padding (max_len=10):
sequence = [0, 0, 0, 0, 0, 0, 0, 0, 1, 36]  # [10]
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€paddingâ”€â”€â”€â”€â”€â”€â”˜  â””ctxâ”€â”˜
length = 2
```

---

## 3. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° GRU4Rec

### 3.1 ĞĞ±Ñ‰Ğ°Ñ ÑÑ…ĞµĞ¼Ğ°

```python
class GRU4Rec(nn.Module):
    def __init__(self, num_nodes, num_services, embedding_dim=64, 
                 hidden=128, num_layers=2, dropout_embed=0.25, 
                 dropout_hidden=0.4, dag_successors=None):
        super().__init__()
        
        # 1. Embedding layer
        self.embedding = nn.Embedding(num_nodes + 1, embedding_dim, 
                                     padding_idx=0)
        
        # 2. GRU layers
        self.gru = nn.GRU(embedding_dim, hidden, num_layers=num_layers,
                         batch_first=True, dropout=dropout_hidden)
        
        # 3. Output layer
        self.fc = nn.Linear(hidden, num_services)
        
        # 4. DAG structure
        self.dag_successors = dag_successors or {}
```

---

### 3.2 ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT SEQUENCE                                          â”‚
â”‚  [0, 0, 0, 0, 0, 0, 0, 0, 1, 36]                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€paddingâ”€â”€â”€â”€â”€â”˜  â””contextâ”€â”˜                       â”‚
â”‚  Length: 2                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EMBEDDING LAYER                                         â”‚
â”‚  nn.Embedding(51, 64, padding_idx=0)                    â”‚
â”‚                                                          â”‚
â”‚  [10] â†’ [10, 64]                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ Padding (0):  [0, 0, 0, ..., 0]   â”‚ (64 dims)      â”‚
â”‚  â”‚ Node 1:       [0.3, 0.5, ..., 0.2]â”‚                 â”‚
â”‚  â”‚ Node 36:      [0.6, 0.4, ..., 0.7]â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                          â”‚
â”‚  + Dropout(p=0.25) Ğ½Ğ° embeddings                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GRU LAYERS (2 layers, hidden=128)                      â”‚
â”‚                                                          â”‚
â”‚  Layer 1:                                                â”‚
â”‚    h_0 = [0, 0, ..., 0]  (init hidden state)           â”‚
â”‚    For t=0..9:                                           â”‚
â”‚      h_t = GRU_cell(emb[t], h_{t-1})                    â”‚
â”‚                                                          â”‚
â”‚  Layer 2:                                                â”‚
â”‚    h_0 = [0, 0, ..., 0]                                 â”‚
â”‚    For t=0..9:                                           â”‚
â”‚      h_t = GRU_cell(h1_t, h_{t-1})                      â”‚
â”‚                                                          â”‚
â”‚  + Dropout(p=0.4) Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸                          â”‚
â”‚  + Dropout(p=0.4) Ğ½Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğµ                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAST HIDDEN STATE EXTRACTION                            â”‚
â”‚  gru_out[batch_idx, length-1]                           â”‚
â”‚                                                          â”‚
â”‚  Ğ”Ğ»Ñ sequence Ğ´Ğ»Ğ¸Ğ½Ñ‹ 2:                                   â”‚
â”‚    last_hidden = gru_out[0, 1]  # Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ 1 (0-indexed)â”‚
â”‚    [128] - Ğ²ĞµĞºÑ‚Ğ¾Ñ€ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FULLY CONNECTED LAYER                                   â”‚
â”‚  nn.Linear(128, 15)                                     â”‚
â”‚                                                          â”‚
â”‚  [128] â†’ [15]                                           â”‚
â”‚  Ğ›Ğ¾Ğ³Ğ¸Ñ‚Ñ‹ Ğ´Ğ»Ñ 15 ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DAG STRUCTURE MASKING (ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ!)              â”‚
â”‚                                                          â”‚
â”‚  Last node in context: service_308 (node_id=35)         â”‚
â”‚  Valid successors from DAG: [36, 40, 42]                â”‚
â”‚    â†’ service_309, service_315, service_318              â”‚
â”‚                                                          â”‚
â”‚  Mask:                                                   â”‚
â”‚    [-âˆ, -âˆ, -âˆ, 0, -âˆ, ..., 0, -âˆ, 0, -âˆ]              â”‚
â”‚     â””â”€â”€invalidâ”€â”€â”˜  â†‘valid    â†‘valid  â†‘valid            â”‚
â”‚                                                          â”‚
â”‚  Masked logits:                                          â”‚
â”‚    [-âˆ, -âˆ, -âˆ, 2.1, -âˆ, ..., -0.5, -âˆ, 1.2, -âˆ]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OUTPUT                                                  â”‚
â”‚  Softmax â†’ Probabilities [15]                           â”‚
â”‚  [0, 0, 0, 0.72, 0, ..., 0.15, 0, 0.13, 0]             â”‚
â”‚   â””invalidâ”˜  â†‘highest    â†‘      â†‘                      â”‚
â”‚                                                          â”‚
â”‚  Prediction: argmax = 3 (service_309)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Embedding Layer

### 4.1 ĞĞ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ

ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑÑ‹ ÑƒĞ·Ğ»Ğ¾Ğ² Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.

### 4.2 ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°

```python
self.embedding = nn.Embedding(
    num_embeddings=num_nodes + 1,  # 51 (50 nodes + 1 padding)
    embedding_dim=64,               # Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²
    padding_idx=0                   # Ğ˜Ğ½Ğ´ĞµĞºÑ 0 = padding (Ğ²ÑĞµĞ³Ğ´Ğ° Ğ½ÑƒĞ»Ğ¸)
)
```

**ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹:**
- `num_embeddings=51`: Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° lookup Ğ´Ğ»Ñ 51 Ğ¸Ğ½Ğ´ĞµĞºÑĞ° (0-50)
- `embedding_dim=64`: ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑƒĞ·ĞµĞ» â†’ 64-Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€
- `padding_idx=0`: Ğ˜Ğ½Ğ´ĞµĞºÑ 0 Ğ²ÑĞµĞ³Ğ´Ğ° Ğ´Ğ°ĞµÑ‚ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€

---

### 4.3 Forward Pass

```python
# Ğ’Ñ…Ğ¾Ğ´Ğ½Ğ°Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ
sequences = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 36]])  # [1, 10]
#                    â””â”€â”€â”€â”€paddingâ”€â”€â”€â”€â”˜  â””â”€contextâ”€â”˜

# Embedding lookup
emb = self.embedding(sequences)  # [1, 10, 64]

# Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾:
emb[0, 0] = [0, 0, 0, ..., 0]       # padding (Ğ¸Ğ½Ğ´ĞµĞºÑ 0)
emb[0, 8] = [0.3, 0.5, 0.2, ...]    # table_1002132 (Ğ¸Ğ½Ğ´ĞµĞºÑ 1)
emb[0, 9] = [0.6, 0.4, 0.7, ...]    # service_308 (Ğ¸Ğ½Ğ´ĞµĞºÑ 36)
```

**Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹:**
```
Input:  [batch=1, seq_len=10]              integers
Output: [batch=1, seq_len=10, emb_dim=64]  floats
```

---

### 4.4 Dropout Ğ½Ğ° Embeddings

**Ğ¢ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ¸Ğ· Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ GRU4Rec:**

```python
emb = F.dropout(emb, p=self.dropout_embed, training=self.training)
# dropout_embed = 0.25 (25% ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ±Ğ½ÑƒĞ»ÑÑÑ‚ÑÑ)
```

**Ğ—Ğ°Ñ‡ĞµĞ¼ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ dropout Ğ´Ğ»Ñ embeddings:**
- Ğ ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ°
- ĞŸÑ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹
- Ğ’ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 2-3%

**ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ ÑÑ„Ñ„ĞµĞºÑ‚Ğ°:**
```python
# Ğ”Ğ¾ dropout:
emb[0, 9] = [0.6, 0.4, 0.7, 0.2, 0.8, ...]

# ĞŸĞ¾ÑĞ»Ğµ dropout (25%):
emb[0, 9] = [0.8, 0.0, 0.93, 0.0, 1.07, ...]
#            rescale â†“    â†“zero   â†“rescale
```

---

## 5. GRU Layer

### 5.1 Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ GRU?

**GRU (Gated Recurrent Unit)** - ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ LSTM Ğ±ĞµĞ· cell state.

**ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹:**
1. **Reset gate** (r): Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ñ‚ÑŒ Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ³Ğ¾
2. **Update gate** (z): ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ
3. **Candidate state** (hÌƒ): Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ

---

### 5.2 ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° GRU

**Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ñ‹ GRU Ğ´Ğ»Ñ timestep t:**

```
r_t = Ïƒ(W_r Â· [h_{t-1}, x_t] + b_r)        # Reset gate
z_t = Ïƒ(W_z Â· [h_{t-1}, x_t] + b_z)        # Update gate
hÌƒ_t = tanh(W_h Â· [r_t âŠ™ h_{t-1}, x_t] + b_h)  # Candidate
h_t = (1 - z_t) âŠ™ h_{t-1} + z_t âŠ™ hÌƒ_t     # New hidden state
```

**Ğ“Ğ´Ğµ:**
- `h_{t-1}`: Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ [hidden_dim]
- `x_t`: Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ Ğ²Ñ…Ğ¾Ğ´ (embedding) [embedding_dim]
- `Ïƒ`: ÑĞ¸Ğ³Ğ¼Ğ¾Ğ¸Ğ´Ğ° [0, 1]
- `âŠ™`: Ğ¿Ğ¾ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ
- `W_r, W_z, W_h`: Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ²ĞµÑĞ°

---

### 5.3 Ğ˜Ğ½Ñ‚ÑƒĞ¸Ñ†Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ GRU

**Reset gate (r_t):**
```python
r_t = sigmoid(...)  # [0, 1]

# r_t â‰ˆ 0: "Ğ—Ğ°Ğ±ÑƒĞ´ÑŒ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğµ, Ğ½Ğ°Ñ‡Ğ½Ğ¸ Ğ·Ğ°Ğ½Ğ¾Ğ²Ğ¾"
# r_t â‰ˆ 1: "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ²ÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ³Ğ¾"

# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€:
# Ğ•ÑĞ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ item Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ…
# â†’ r_t Ğ±ÑƒĞ´ĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ â†’ "ÑĞ±Ñ€Ğ¾Ñ" ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°
```

**Update gate (z_t):**
```python
z_t = sigmoid(...)  # [0, 1]

# z_t â‰ˆ 0: "Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸ ÑÑ‚Ğ°Ñ€Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ"  
# z_t â‰ˆ 1: "ĞŸĞ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ"

# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€:
# Ğ•ÑĞ»Ğ¸ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ item Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ²Ğ°Ğ¶ĞµĞ½
# â†’ z_t Ğ±ÑƒĞ´ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ â†’ h_t â‰ˆ hÌƒ_t (Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ)
```

**Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ:**
```python
h_t = (1 - z_t) âŠ™ h_{t-1} + z_t âŠ™ hÌƒ_t

# Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ°Ñ€Ñ‹Ğ¼ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¼:
# z_t=0.3: h_t = 0.7*h_old + 0.3*h_new  (Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑÑ‚Ğ°Ñ€Ğ¾Ğ³Ğ¾)
# z_t=0.7: h_t = 0.3*h_old + 0.7*h_new  (Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾)
```

---

### 5.4 GRU Ğ² PyTorch

```python
self.gru = nn.GRU(
    input_size=64,      # embedding_dim
    hidden_size=128,    # Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ
    num_layers=2,       # ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ GRU ÑĞ»Ğ¾ĞµĞ²
    batch_first=True,   # [batch, seq, features]
    dropout=0.4         # dropout Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ layers > 1)
)
```

**ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ GRU:**
```python
# Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ:
# Ğ’ĞµÑĞ° Ğ´Ğ»Ñ input:
W_ir, W_iz, W_ih: [embedding_dim, hidden_size] = [64, 128]

# Ğ’ĞµÑĞ° Ğ´Ğ»Ñ hidden:
W_hr, W_hz, W_hh: [hidden_size, hidden_size] = [128, 128]

# Bias:
b_ir, b_iz, b_ih, b_hr, b_hz, b_hh: [hidden_size] = [128]

# Ğ’ÑĞµĞ³Ğ¾ Ğ½Ğ° 1 ÑĞ»Ğ¾Ğ¹:
(64Ã—128 + 128Ã—128 + 128) Ã— 3 gates = ~57K Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²

# Ğ”Ğ»Ñ 2 ÑĞ»Ğ¾ĞµĞ²:
Layer 1: 64 â†’ 128    (~57K)
Layer 2: 128 â†’ 128   (~82K)
Ğ˜Ğ¢ĞĞ“Ğ: ~139K Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² GRU!
```

---

### 5.5 Forward Pass Ñ‡ĞµÑ€ĞµĞ· GRU

**ĞŸĞ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°:**

```python
# Ğ’Ñ…Ğ¾Ğ´:
seq_embeddings = [1, 10, 64]  # batch=1, seq_len=10, emb=64
lengths = [2]  # Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°

# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ hidden state
h_0 = torch.zeros(num_layers=2, batch=1, hidden=128)
# [2, 1, 128]

# GRU processing:
gru_out, h_final = self.gru(seq_embeddings, h_0)

# gru_out: [1, 10, 128] - hidden state Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ timestep
# h_final: [2, 1, 128] - Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ hidden states Ğ¾Ğ±Ğ¾Ğ¸Ñ… ÑĞ»Ğ¾ĞµĞ²
```

**Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ (ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾):**

```
Timestep 0: emb[0] = [0, 0, ..., 0]  (padding)
  Layer 1: h1_0 = GRU_cell(emb[0], h1_{-1}) = [0.1, 0.05, ...]
  Layer 2: h2_0 = GRU_cell(h1_0, h2_{-1})    = [0.08, 0.03, ...]

Timestep 1-7: padding (Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² last_hidden extraction)

Timestep 8: emb[8] = [0.3, 0.5, ...]  (table_1002132)
  Layer 1: h1_8 = GRU_cell(emb[8], h1_7) = [0.45, 0.32, ...]
  Layer 2: h2_8 = GRU_cell(h1_8, h2_7)   = [0.38, 0.28, ...]

Timestep 9: emb[9] = [0.6, 0.4, ...]  (service_308)
  Layer 1: h1_9 = GRU_cell(emb[9], h1_8) = [0.52, 0.41, ...]
  Layer 2: h2_9 = GRU_cell(h1_9, h2_8)   = [0.48, 0.37, ...]
```

**Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ last hidden:**

```python
# Ğ‘ĞµÑ€ĞµĞ¼ hidden state Ğ½Ğ° Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ length-1
last_hidden = gru_out[0, 1]  # Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ 1 (Ñ‚Ğ°Ğº ĞºĞ°Ğº length=2, 0-indexed)
# [128] - Ğ²ĞµĞºÑ‚Ğ¾Ñ€ Ğ¸Ğ· layer 2
```

**Dropout Ğ½Ğ° GRU output:**

```python
gru_out = F.dropout(gru_out, p=self.dropout_hidden, training=True)
# dropout_hidden = 0.4 (40% ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ±Ğ½ÑƒĞ»ÑÑÑ‚ÑÑ)
```

---

## 6. Output Layer

### 6.1 Fully Connected Layer

```python
self.fc = nn.Linear(hidden, num_services)
# Linear(128, 15)
```

**Forward:**

```python
last_hidden = [0.48, 0.37, 0.55, ...]  # [128]

logits = self.fc(last_hidden)
# logits = last_hidden @ W + b
# W: [128, 15], b: [15]

logits = [-2.1, -1.8, 3.5, 2.1, -0.5, ...]  # [15]
          â†‘     â†‘     â†‘    â†‘
        s_306 s_307 s_308 s_309
```

**Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ¾Ğ²:**
- Ğ’Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ Ğ»Ğ¾Ğ³Ğ¸Ñ‚ (3.5) â†’ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ° Ğ² service_308
- Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ»Ğ¾Ğ³Ğ¸Ñ‚ (2.1) â†’ service_309 Ñ‚Ğ¾Ğ¶Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶ĞµĞ½
- ĞĞ¸Ğ·ĞºĞ¸Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ñ‹ (-2.1) â†’ Ğ¼Ğ°Ğ»Ğ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ ÑĞµÑ€Ğ²Ğ¸ÑÑ‹

---

## 7. DAG Structure Masking

### 7.1 ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ GRU4Rec

**ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»:**
```python
# ĞœĞ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ›Ğ®Ğ‘ĞĞ™ item Ğ¸Ğ· ĞºĞ°Ñ‚Ğ°Ğ»Ğ¾Ğ³Ğ°
logits = [score_1, score_2, ..., score_N]  # Ğ²ÑĞµ items
```

**ĞĞ°ÑˆĞ° Ğ²ĞµÑ€ÑĞ¸Ñ Ñ DAG:**
```python
# ĞœĞ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ’ĞĞ›Ğ˜Ğ”ĞĞ«Ğ• Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ DAG
logits = [score_1, score_2, ..., score_N]  # Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ valid successors
```

---

### 7.2 Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ masking

```python
def forward(self, sequences, lengths, last_nodes=None):
    # ... GRU processing ...
    logits = self.fc(last_hidden)  # [batch, 15]
    
    # DAG structure masking
    if last_nodes is not None:
        mask = torch.zeros_like(logits) - 1e9  # [-âˆ, -âˆ, ...]
        
        for idx, node in enumerate(last_nodes.tolist()):
            succ = self.dag_successors.get(node, [])  # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ‹
            if succ:
                mask[idx, succ] = 0.0  # Ğ Ğ°Ğ·Ñ€ĞµÑˆĞ°ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ successors
        
        logits = logits + mask  # ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ¼Ğ°ÑĞºÑƒ
    
    return logits
```

---

### 7.3 ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ masking

**DAG structure:**
```
service_308 (node_id=35) Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ²:
  â†’ service_309 (class_id=3)
  â†’ service_315 (class_id=9)
  â†’ service_318 (class_id=12)
```

**Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°ÑĞºĞ¸:**
```python
# Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ñ‹ (Ğ±ĞµĞ· Ğ¼Ğ°ÑĞºĞ¸):
logits = [-2.1, -1.8, 3.5, 2.1, -0.5, 1.2, -1.0, 0.8, -0.3, 1.5, ...]
          0     1     2    3     4     5     6     7     8     9

# Dag_successors[35] = [3, 9, 12]

# ĞœĞ°ÑĞºĞ°:
mask = [-âˆ, -âˆ, -âˆ, 0, -âˆ, -âˆ, -âˆ, -âˆ, -âˆ, 0, -âˆ, -âˆ, 0, -âˆ, -âˆ]
        â””invalidâ”˜  â†‘valid                â†‘valid      â†‘valid

# ĞŸĞ¾ÑĞ»Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ:
logits = [-âˆ, -âˆ, -âˆ, 2.1, -âˆ, -âˆ, -âˆ, -âˆ, -âˆ, 1.5, -âˆ, -âˆ, -0.3, -âˆ, -âˆ]

# Softmax:
probs = [0, 0, 0, 0.72, 0, 0, 0, 0, 0, 0.21, 0, 0, 0.07, 0, 0]
```

**Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚:**
- ĞœĞ¾Ğ´ĞµĞ»ÑŒ **Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚** Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ½ĞµĞ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´
- Ğ’ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ñ… successors
- Ğ¡ÑƒĞ¼Ğ¼Ğ° Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ = 1.0 (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğ¼)

---

### 7.4 Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ dag_successors

```python
# Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ñ‚ÑÑ Ğ¸Ğ· Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
successors = defaultdict(list)

for u, v in graph.edges():
    if v.startswith("service"):  # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞµÑ€Ğ²Ğ¸ÑÑ‹ - Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ ĞºĞ»Ğ°ÑÑÑ‹
        successors[node_map[u]].append(service_map[v])

# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€:
successors = {
    0: [2, 5],        # table_1002132 â†’ service_308, service_311
    35: [3, 9, 12],   # service_308 â†’ service_309, service_315, service_318
    ...
}
```

---

## 8. Loss Functions

### 8.1 Cross-Entropy Loss (Default)

**Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ°:**
```
L_CE = -âˆ‘ y_true[i] Â· log(softmax(logits)[i])
     = -log(softmax(logits)[true_class])
```

**Ğ’ ĞºĞ¾Ğ´Ğµ:**
```python
criterion = nn.CrossEntropyLoss()

logits = model(sequences, lengths, last_nodes)  # [batch, 15]
targets = [3, 5, 2, ...]  # [batch] Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ»Ğ°ÑÑÑ‹

loss = criterion(logits, targets)
```

**ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ñ€Ğ°ÑÑ‡ĞµÑ‚Ğ°:**
```python
# Ğ”Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°:
logits = [-âˆ, -âˆ, -âˆ, 2.1, -âˆ, ..., 1.5, -âˆ, ..., -0.3, -âˆ, -âˆ]
          0    1    2    3              9              12

# Softmax (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğ¼):
probs = [0, 0, 0, 0.72, 0, ..., 0.21, 0, ..., 0.07, 0, 0]

# True class = 3 (service_309)
p_true = probs[3] = 0.72

# Loss:
loss = -log(0.72) = 0.329
```

---

### 8.2 BPR-max Loss (Ranking)

**Ğ˜Ğ· Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ GRU4Rec** - Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ´Ğ»Ñ ranking Ğ·Ğ°Ğ´Ğ°Ñ‡.

**Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ°:**
```
L_BPR = -âˆ‘ log(Ïƒ(score_pos - score_neg))
```

**Ğ˜Ğ´ĞµÑ:**
- ĞœĞ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ñƒ Ğ¼ĞµĞ¶Ğ´Ñƒ positive Ğ¸ negative items
- Ranking-based loss Ğ²Ğ¼ĞµÑÑ‚Ğ¾ classification

**Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ:**
```python
class BPRLoss(nn.Module):
    def forward(self, pos_scores, neg_scores):
        # pos_scores: [batch] - Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… items
        # neg_scores: [batch, n_neg] - Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ negative samples
        
        diff = pos_scores.unsqueeze(1) - neg_scores  # [batch, n_neg]
        loss = -torch.log(torch.sigmoid(diff) + 1e-24).mean()
        return loss
```

**ĞŸÑ€Ğ¸Ğ¼ĞµÑ€:**
```python
# ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ item: service_309 (score = 2.1)
# Negative samples: service_306 (-2.1), service_315 (1.5)

diff = [2.1 - (-2.1), 2.1 - 1.5] = [4.2, 0.6]
sigmoid(diff) = [0.985, 0.646]
loss = -mean([log(0.985), log(0.646)]) = 0.23
```

---

### 8.3 Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Loss Functions

**ĞĞ° Ğ½Ğ°ÑˆĞµĞ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ (711 train):**

| Loss | Accuracy | nDCG | F1 | Ğ’Ñ€ĞµĞ¼Ñ/ÑĞ¿Ğ¾Ñ…Ñƒ |
|------|----------|------|-----|-------------|
| **Cross-Entropy** | **54.10%** | **0.7781** | **0.2349** | 0.07s |
| BPR (512 samples) | 53.44% | 0.7748 | 0.2047 | 0.08s |
| BPR (2048 samples) | 54.10% | 0.7729 | 0.2349 | 0.08s |

**Ğ’Ñ‹Ğ²Ğ¾Ğ´Ñ‹:**
- **CE Ğ»ÑƒÑ‡ÑˆĞµ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…** (711 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²)
- BPR Ñ…Ğ¾Ñ€Ğ¾Ñˆ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² (>10K)
- CE Ğ¿Ñ€Ğ¾Ñ‰Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ´Ğ»Ñ Ğ½Ğ°ÑˆĞµĞ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸

---

## 9. Negative Sampling

### 9.1 Ğ—Ğ°Ñ‡ĞµĞ¼ Ğ½ÑƒĞ¶ĞµĞ½ Negative Sampling

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ Cross-Entropy:**
```python
# Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ gradients Ğ´Ğ»Ñ Ğ’Ğ¡Ğ•Ğ¥ 15 ĞºĞ»Ğ°ÑÑĞ¾Ğ²
# ĞŸÑ€Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ ĞºĞ»Ğ°ÑÑĞ¾Ğ² (>100K) - Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾
```

**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ - Negative Sampling:**
```python
# Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ gradients Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ:
# - 1 positive class
# - N negative samples (512-2048)
# Ğ’Ğ¼ĞµÑÑ‚Ğ¾ 100K ĞºĞ»Ğ°ÑÑĞ¾Ğ² â†’ 513 ĞºĞ»Ğ°ÑÑĞ¾Ğ²
```

---

### 9.2 Popularity-based Sampling

**Ğ˜Ğ· Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ GRU4Rec:**

```python
def sample_negatives(targets, num_classes, n_sample, 
                     sample_alpha=0.75, item_popularity=None):
    # Ğ’ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° item i:
    p(i) = popularity(i)^sample_alpha / âˆ‘popularity(j)^sample_alpha
```

**ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹:**

**sample_alpha = 0** (uniform):
```python
p(item) = 1/N  # Ğ’ÑĞµ items Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹
```

**sample_alpha = 1** (popularity):
```python
p(item) = count(item) / total_count
# ĞŸĞ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ items ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ‡Ğ°Ñ‰Ğµ
```

**sample_alpha = 0.75** (recommended):
```python
p(item) = count(item)^0.75 / âˆ‘count^0.75
# Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ: Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ñ‡Ğ°Ñ‰Ğµ, Ğ½Ğ¾ Ğ½Ğµ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼
```

---

### 9.3 ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Negative Sampling

```python
# Popularity counts:
service_counts = {
    'service_306': 5,
    'service_308': 150,  # ĞÑ‡ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¹!
    'service_309': 80,
    ...
    'service_397': 2
}

# Ğ¡ alpha=0.75:
probs = [5^0.75, 150^0.75, 80^0.75, ..., 2^0.75]
      = [3.34, 54.82, 30.15, ..., 1.68]
      / sum = [0.01, 0.22, 0.12, ..., 0.01]

# Ğ¡ÑĞ¼Ğ¿Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ 512 negatives:
neg_samples = multinomial(probs, n=512)
# Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: service_308 Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ÑÑ ~110 Ñ€Ğ°Ğ· (22%)
#            service_397 Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ÑÑ ~5 Ñ€Ğ°Ğ· (1%)
```

---

## 10. Dropout Strategies

### 10.1 Separate Dropout (Ğ¾Ñ‚ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ğ°)

**ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° GRU4Rec:**

```python
dropout_embed = 0.25   # ĞĞ° embeddings
dropout_hidden = 0.4   # ĞĞ° hidden layers
```

**ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ:**

1. **dropout_embed = 0.25** (Ğ½Ğ¸Ğ¶Ğµ):
   - Embeddings - Ğ²Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
   - Ğ¡Ğ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ dropout â†’ Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸
   - 25% Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸

2. **dropout_hidden = 0.4** (Ğ²Ñ‹ÑˆĞµ):
   - Hidden layers - Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ
   - Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ capacity â†’ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ€Ğ¸ÑĞº overfitting
   - 40% Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ

---

### 10.2 ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Dropout

```python
def forward(self, sequences, lengths, last_nodes=None):
    # 1. Dropout Ğ½Ğ° embeddings
    emb = self.embedding(sequences)
    emb = F.dropout(emb, p=self.dropout_embed, training=self.training)
    # 25% ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² â†’ 0
    
    # 2. GRU Ñ dropout Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸
    gru_out, _ = self.gru(emb)
    # Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¹ dropout Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Layer 1 Ğ¸ Layer 2
    
    # 3. Dropout Ğ½Ğ° GRU output
    gru_out = F.dropout(gru_out, p=self.dropout_hidden, training=self.training)
    # 40% ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² â†’ 0
    
    # 4. FC layer (Ğ‘Ğ•Ğ— dropout Ğ½Ğ° Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğµ!)
    logits = self.fc(last_hidden)
```

**Ğ“Ğ´Ğµ ĞĞ• Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ dropout:**
```python
# âŒ ĞĞ•ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ¬ĞĞ:
logits = self.fc(last_hidden)
logits = F.dropout(logits, ...)  # ĞĞ• Ğ´ĞµĞ»Ğ°Ğ¹Ñ‚Ğµ ÑÑ‚Ğ¾!

# âœ… ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ¬ĞĞ:
logits = self.fc(last_hidden)  # Ğ‘ĞµĞ· dropout
```

---

## 11. Training Loop

### 11.1 Standard Training (Cross-Entropy)

```python
def train_epoch(model, seq_train, len_train, targets_train, 
                last_nodes_train, optimizer):
    model.train()
    optimizer.zero_grad()
    
    # Forward pass
    logits = model(seq_train, len_train, last_nodes_train)
    # [711, 15]
    
    # Loss
    loss = F.cross_entropy(logits, targets_train)
    
    # Backward
    loss.backward()
    
    # Gradient clipping (Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ RNN!)
    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    
    # Update
    optimizer.step()
    
    return loss.item()
```

---

### 11.2 Training Ñ BPR Loss

```python
def train_epoch_bpr(model, seq_train, len_train, targets_train,
                    last_nodes_train, optimizer, n_sample=512):
    model.train()
    optimizer.zero_grad()
    
    # Forward (without masking for BPR)
    logits = model(seq_train, len_train, compute_scores=True)
    # [711, 15]
    
    # Get positive scores
    pos_scores = logits[range(711), targets_train]  # [711]
    
    # Sample negatives
    neg_indices = sample_negatives(targets_train, 15, n_sample=512)
    # [711, 512]
    
    neg_scores = logits.gather(1, neg_indices)  # [711, 512]
    
    # BPR loss
    diff = pos_scores.unsqueeze(1) - neg_scores
    loss = -torch.log(torch.sigmoid(diff) + 1e-24).mean()
    
    # Backward
    loss.backward()
    nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    optimizer.step()
    
    return loss.item()
```

---

## 12. ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Forward Pass

### ĞšĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸

**Ğ’Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ:**
```python
# ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚:
context = ('table_1002132', 'service_308')
target = 'service_309'

# ĞŸĞ¾ÑĞ»Ğµ Ğ¿Ñ€ĞµĞ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¸Ğ½Ğ³Ğ°:
sequence = [0, 0, 0, 0, 0, 0, 0, 0, 1, 36]  # [10]
length = 2
last_node = 35  # service_308 Ğ² node_map
target_class = 3  # service_309 Ğ² service_map
```

---

### Ğ¨Ğ°Ğ³ Ğ·Ğ° ÑˆĞ°Ğ³Ğ¾Ğ¼:

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
STEP 1: EMBEDDING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Input: [0, 0, 0, 0, 0, 0, 0, 0, 1, 36]

Embedding Lookup:
  position 0-7: [0, 0, ..., 0] (padding_idx=0)
  position 8:   [0.3, 0.5, 0.2, ..., 0.4]  (node 1)
  position 9:   [0.6, 0.4, 0.7, ..., 0.5]  (node 36)

emb: [1, 10, 64]

Dropout (25%):
  emb[0, 8]: [0.4, 0.0, 0.27, 0.0, 0.8, ...]  (rescaled)
  emb[0, 9]: [0.8, 0.53, 0.0, 0.67, 0.0, ...]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
STEP 2: GRU LAYER 1 (64 â†’ 128)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Initial h_0 = [0, 0, ..., 0]  (128 dims)

Timestep 0: (padding)
  x_t = emb[0, 0] = [0, 0, ...]
  h_0 = GRU(x_t, h_{-1}) = [0.01, 0.02, ..., 0.01]

...timesteps 1-7: padding...

Timestep 8: (table_1002132)
  x_t = emb[0, 8] = [0.4, 0.0, 0.27, ...]
  h_8 = GRU(x_t, h_7) = [0.35, 0.42, 0.28, ..., 0.31]
  
  # Ğ’Ğ½ÑƒÑ‚Ñ€Ğ¸ GRU:
  r_8 = Ïƒ([h_7, x_8] @ W_r) = [0.6, 0.7, ..., 0.5]  (reset)
  z_8 = Ïƒ([h_7, x_8] @ W_z) = [0.4, 0.3, ..., 0.6]  (update)
  hÌƒ_8 = tanh(...) = [0.5, 0.6, ..., 0.4]           (candidate)
  h_8 = (1-z_8)âŠ™h_7 + z_8âŠ™hÌƒ_8                      (combine)

Timestep 9: (service_308) â† ĞšĞ›Ğ®Ğ§Ğ•Ğ’ĞĞ™ ĞœĞĞœĞ•ĞĞ¢
  x_t = emb[0, 9] = [0.8, 0.53, 0.0, ...]
  h_9 = GRU(x_t, h_8) = [0.42, 0.38, 0.51, ..., 0.29]
  
Output Layer 1: gru1_out = [1, 10, 128]
  gru1_out[0, 9] = [0.42, 0.38, 0.51, ..., 0.29]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
STEP 3: GRU LAYER 2 (128 â†’ 128)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Input: gru1_out [1, 10, 128]

Timestep 9:
  x_t = gru1_out[0, 9] = [0.42, 0.38, ...]
  h_9 = GRU(x_t, h_8) = [0.48, 0.37, 0.55, ..., 0.33]

Output Layer 2: gru2_out = [1, 10, 128]
  gru2_out[0, 9] = [0.48, 0.37, 0.55, ..., 0.33]

Dropout (40%):
  final_hidden = [0.64, 0.0, 0.73, 0.0, 0.55, ...]  (rescaled + zeroed)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
STEP 4: EXTRACT LAST HIDDEN
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

length = 2, so last position = 1 (0-indexed)
last_hidden = gru2_out[0, 1]
# BUT WAIT! Padding positions 0-7 Ğ±Ñ‹Ğ»Ğ¸ padding
# Ğ ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑÑ… 8-9
# length=2 Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚: Ğ±ĞµÑ€ĞµĞ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ 1 Ğ² Ğ Ğ•ĞĞ›Ğ¬ĞĞĞœ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ

# ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾:
last_hidden = gru2_out[0, length-1] = gru2_out[0, 1]

# Ğ Ğ°Ğ·Ğ¼ĞµÑ€: [128]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
STEP 5: FULLY CONNECTED
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

last_hidden = [0.64, 0.0, 0.73, ..., 0.55]  [128]

logits = last_hidden @ W + b
# W: [128, 15], b: [15]

logits = [-2.1, -1.8, 3.5, 2.1, -0.5, 1.2, -1.0, 0.8, -0.3, 1.5, -0.8, -1.5, 0.3, -0.9, -2.0]
          0     1     2    3     4     5     6     7     8     9     10    11    12    13    14
          â†‘     â†‘     â†‘    â†‘
        s_306 s_307 s_308 s_309

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
STEP 6: DAG MASKING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Last node = 35 (service_308)
Valid successors = [3, 9, 12]  # service_309, service_315, service_318

Mask:
  [-âˆ, -âˆ, -âˆ, 0, -âˆ, -âˆ, -âˆ, -âˆ, -âˆ, 0, -âˆ, -âˆ, 0, -âˆ, -âˆ]

Masked logits:
  [-âˆ, -âˆ, -âˆ, 2.1, -âˆ, -âˆ, -âˆ, -âˆ, -âˆ, 1.5, -âˆ, -âˆ, 0.3, -âˆ, -âˆ]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
STEP 7: SOFTMAX & PREDICTION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Softmax (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğ¼):
  probs = [0, 0, 0, 0.72, 0, 0, 0, 0, 0, 0.21, 0, 0, 0.07, 0, 0]

Prediction:
  pred = argmax(probs) = 3  (service_309)

True target: 3 (service_309)

Result: âœ… CORRECT!
```

---

## 13. Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ GRU4Rec

### 13.1 Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğ¹

| ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ | ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ» (Hidasi 2016) | ĞĞ°ÑˆĞ° Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ |
|-----------|------------------------|-----------------|
| **Framework** | Theano | PyTorch |
| **Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ°** | Session-based rec | DAG sequence pred |
| **Input** | Item IDs | Node IDs Ğ² DAG |
| **Embedding** | Item embeddings | Node embeddings |
| **GRU** | Standard GRU | Standard GRU âœ“ |
| **Loss** | BPR-max, TOP1, CE | CE, BPR âœ“ |
| **Negative sampling** | Ğ”Ğ¾ 2048 samples | Ğ”Ğ¾ 2048 samples âœ“ |
| **Sample strategy** | Popularity^alpha | Popularity^alpha âœ“ |
| **Dropout** | Separate embed/hidden | Separate embed/hidden âœ“ |
| **Output masking** | âŒ ĞĞµÑ‚ | âœ… DAG masking |
| **GPU optimization** | Custom Theano ops | Standard PyTorch |
| **Batch processing** | Mini-batches | Full batch (711) |

---

### 13.2 Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¸Ğ· Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ğ°

#### âœ… Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¾:

1. **Separate dropout Ğ´Ğ»Ñ embeddings Ğ¸ hidden**
   ```python
   dropout_embed = 0.25
   dropout_hidden = 0.4
   ```

2. **BPR-max loss**
   ```python
   loss = -log(sigmoid(pos_score - neg_score))
   ```

3. **Popularity-based negative sampling**
   ```python
   p(item) ~ popularity^sample_alpha
   ```

4. **Gradient clipping**
   ```python
   nn.utils.clip_grad_norm_(parameters, max_norm=1.0)
   ```

#### âŒ ĞĞµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¾:

1. **TOP1-max loss** (ÑƒĞ±Ñ€Ğ°Ğ»Ğ¸ - BPR Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑĞµĞ±Ñ Ğ»ÑƒÑ‡ÑˆĞµ)
2. **LogQ normalization** (Ğ½Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)
3. **Sample store pre-computation** (Ğ½Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾ - Ğ¼Ğ°Ğ»Ñ‹Ğ¹ batch)
4. **Custom GPU operators** (PyTorch Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ±Ñ‹ÑÑ‚Ñ€)

---

## 14. Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ DAGNN Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸

### 14.1 GRU4Rec vs DAG-GNN

| ĞÑĞ¿ĞµĞºÑ‚ | GRU4Rec | DAG-GNN |
|--------|---------|---------|
| **Paradigm** | Sequential (RNN) | Structural (GNN) |
| **Information flow** | Temporal order | Graph topology |
| **Context** | Full sequence history | K-hop neighborhoods |
| **Parameters** | ~139K (GRU) + embeddings | ~43K total |
| **Accuracy** | **54.10%** ğŸ† | 52.13% |
| **nDCG** | **0.7781** ğŸ† | 0.7571 |
| **F1** | **0.2349** ğŸ† | 0.1805 |
| **Speed** | 0.07s/epoch | 0.003s/epoch (23x faster!) |

---

### 14.2 ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ GRU4Rec Ğ»ÑƒÑ‡ÑˆĞµ Ğ´Ğ»Ñ sequence prediction?

**1. ĞŸĞ¾Ñ€ÑĞ´Ğ¾Ğº Ğ¸Ğ¼ĞµĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ:**
```python
# GRU Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚:
[Table, S1, S2] â†’ S3  âœ“
[S2, S1, Table] â†’ S3  âœ— (Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚)

# DAG-GNN Ğ²Ğ¸Ğ´Ğ¸Ñ‚:
{Table, S1, S2} â†’ S3  (Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾, Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ñ‚ĞµÑ€ÑĞµÑ‚ÑÑ)
```

**2. Long-term dependencies:**
```python
# GRU Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ĞµÑ‚:
[T1, S1, S2, S3, S4, S5, S6] â†’ S7
 â†‘_______memory_________|

# DAG-GNN Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½:
K=10 hops, Ğ½Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ
```

**3. Ğ ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ:**
```python
# GRU hidden state = "Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ" Ğ¾ Ğ²ÑĞµĞ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
h_t = f(h_{t-1}, x_t)  # h_{t-1} ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ²ÑÑ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ

# GNN = aggregation Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸
h_v = aggregate(neighbors)  # Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚ĞµĞºÑƒÑ‰ĞµĞµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ
```

---

## 15. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹

### 15.1 Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°

```
=== SUMMARY ===
Model           | Accuracy | nDCG   | F1     | Time/epoch
----------------|----------|--------|--------|------------
Popularity      | 0.4787   | 0.6597 | 0.0540 | -
DirectedDAGNN   | 0.5115   | 0.7533 | 0.1138 | 0.015s
DeepDAG2022     | 0.5213   | 0.7571 | 0.1805 | 0.010s
DAG-GNN         | 0.5213   | 0.7569 | 0.1805 | 0.003s
DAGNN2021       | 0.5213   | 0.7571 | 0.1805 | 0.030s
GRU4Rec (CE)    | 0.5410   | 0.7781 | 0.2349 | 0.070s  ğŸ†
```

**ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ GRU4Rec Ğ² Ğ´ĞµÑ‚Ğ°Ğ»ÑÑ…:**
- **Accuracy:** 54.10% (+3.8% vs best GNN)
- **Precision:** 36.38% (+117% vs DAG-GNN!)
- **Recall:** 25.70% (+14% vs DAG-GNN)
- **nDCG:** 0.7781 (+2.8% vs DAG-GNN)
- **F1:** 0.2349 (+30% vs DAG-GNN)

---

### 15.2 Ablation Study

**Ğ’Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²:**

| Configuration | Accuracy | nDCG | F1 | ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ğµ |
|---------------|----------|------|-----|------------|
| Baseline GRU | 0.5344 | 0.7710 | 0.2180 | Ğ‘ĞµĞ· DAG masking |
| + DAG masking | **0.5410** | **0.7781** | **0.2349** | +1.2% accuracy! |
| + Separate dropout | 0.5410 | 0.7781 | 0.2349 | Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ĞµĞµ |
| + BPR loss (512) | 0.5344 | 0.7748 | 0.2047 | Ğ¥ÑƒĞ¶Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… |
| + BPR loss (2048) | 0.5410 | 0.7729 | 0.2349 | = CE |

**Ğ’Ñ‹Ğ²Ğ¾Ğ´Ñ‹:**
1. **DAG masking ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµĞ½** - Ğ´Ğ°ĞµÑ‚ +1.2% accuracy
2. **Separate dropout** ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ
3. **CE loss Ğ»ÑƒÑ‡ÑˆĞµ BPR** Ğ½Ğ° Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (711 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²)
4. **Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ negative samples Ğ½Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚** (Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼Ğ°Ğ»)

---

## 16. Best Practices

### 16.1 ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ

**Ğ”Ğ»Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² (<1000 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²):**

```python
model = GRU4Rec(
    num_nodes=50,
    num_services=15,
    embedding_dim=64,        # ĞĞµ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹
    hidden=128,              # 2x embedding_dim
    num_layers=2,            # 2-3 ÑĞ»Ğ¾Ñ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾
    dropout_embed=0.25,      # Ğ£Ğ¼ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¹ dropout Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´Ğµ
    dropout_hidden=0.4,      # Ğ¡Ğ¸Ğ»ÑŒĞ½ĞµĞµ Ğ½Ğ° hidden
    dag_successors=successors
)

optimizer = Adam(model.parameters(), lr=0.001)

train(
    epochs=150,
    loss_type='ce',          # Cross-entropy Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    n_sample=0,              # Ğ‘ĞµĞ· negative sampling
    gradient_clip=1.0        # ĞĞ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ RNN!
)
```

**Ğ”Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² (>10K Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²):**

```python
model = GRU4Rec(
    embedding_dim=128,       # Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ capacity
    hidden=256,
    num_layers=3,
    dropout_embed=0.3,
    dropout_hidden=0.5       # Ğ¡Ğ¸Ğ»ÑŒĞ½ĞµĞµ regularization
)

train(
    loss_type='bpr',         # BPR Ğ»ÑƒÑ‡ÑˆĞµ Ğ´Ğ»Ñ ranking
    n_sample=2048,           # ĞœĞ½Ğ¾Ğ³Ğ¾ negative samples
    sample_alpha=0.75        # Popularity-based
)
```

---

### 16.2 Ğ“Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹

**ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹:**

| ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ | Ğ”Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ | Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ñ | Ğ’Ğ»Ğ¸ÑĞ½Ğ¸Ğµ |
|----------|----------|--------------|---------|
| **embedding_dim** | 32-256 | 64-128 | Capacity Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ |
| **hidden** | 64-512 | 128-256 | ĞŸĞ°Ğ¼ÑÑ‚ÑŒ GRU |
| **num_layers** | 1-3 | 2 | Ğ“Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° |
| **dropout_embed** | 0.1-0.5 | 0.25 | Regularization Ğ²Ñ…Ğ¾Ğ´Ğ° |
| **dropout_hidden** | 0.2-0.7 | 0.4 | Regularization hidden |
| **learning_rate** | 1e-4 - 1e-2 | 1e-3 | Ğ¡ĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ |
| **gradient_clip** | 0.5-5.0 | 1.0 | Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ RNN |

---

### 16.3 ĞšĞ¾Ğ³Ğ´Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ GRU4Rec

**âœ… Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ GRU4Rec ĞºĞ¾Ğ³Ğ´Ğ°:**
- ĞŸĞ¾Ñ€ÑĞ´Ğ¾Ğº ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶ĞµĞ½
- ĞŸĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹
- ĞœĞ°Ğ»Ñ‹Ğ¹-ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ (<10K)
- Temporal dependencies
- ĞÑƒĞ¶Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ precision

**âŒ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ GNN ĞºĞ¾Ğ³Ğ´Ğ°:**
- Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ³Ñ€Ğ°Ñ„Ğ° Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ°
- ĞÑƒĞ¶Ğ½Ğ° ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ (GNN Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ)
- Bidirectional dependencies
- ĞÑ‡ĞµĞ½ÑŒ Ğ¼Ğ°Ğ»Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ (<100 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²)
- Graph-level tasks

---

## Ğ—Ğ°ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ

### ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ takeaways

**GRU4Rec Ğ´Ğ»Ñ DAG - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ñ‰Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ:**

1. **ĞŸĞ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°** (RNN)
   - ĞœĞ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ temporal order
   - Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ long-term dependencies
   - ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· gating

2. **Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ** (DAG masking)
   - Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ½ĞµĞ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´
   - Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ domain knowledge
   - Ğ£Ğ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ precision Ğ½Ğ° 117%!

3. **Ğ¢ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¸Ğ· Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ğ°** (ICLR 2016)
   - Separate dropout
   - BPR-max loss (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
   - Negative sampling
   - Gradient clipping

**Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: 54.1% accuracy - Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ· Ğ²ÑĞµÑ…!** ğŸ†

---

### ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑÑƒÑ‚ÑŒ

**GRU core:**
```
h_t = GRU(h_{t-1}, x_t)
    = (1-z_t)âŠ™h_{t-1} + z_tâŠ™tanh(WÂ·[r_tâŠ™h_{t-1}, x_t])
```

**Ğ¡ DAG masking:**
```
logits_masked[i] = logits[i] if i âˆˆ successors(last_node) else -âˆ
```

**ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ:**
```
Î¸* = argmin E[L(softmax(logits_masked), y_true)]
```

---

*Ğ¤Ğ°Ğ¹Ğ»: GRU4REC_ARCHITECTURE_EXPLAINED.md*  
*Ğ’ĞµÑ€ÑĞ¸Ñ: v1.0*  
*Ğ”Ğ°Ñ‚Ğ°: 2025-11-16*  
*ĞÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ½Ğ°: Hidasi et al., ICLR 2016 + Thost & Chen, ICLR 2021*

