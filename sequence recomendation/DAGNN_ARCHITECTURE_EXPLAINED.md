# üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ DAGNN: –î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ

**–ü–æ–ª–Ω–æ–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ DAGNN (Directed Acyclic Graph Neural Network)**

–≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ DAGNN –º–æ–¥–µ–ª–∏ - –æ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∫–æ–¥–∞, –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ñ–æ—Ä–º—É–ª–∞–º–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–º–∏.

---

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

### –û—Å–Ω–æ–≤—ã
1. [–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö](#1-–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞-–¥–∞–Ω–Ω—ã—Ö)
2. [–í—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏](#2-–≤—Ö–æ–¥–Ω—ã–µ-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã-–º–æ–¥–µ–ª–∏)
3. [Encoder Block](#3-encoder-block)
4. [Residual Connection](#4-residual-connection)
5. [DAGNN Propagation](#5-dagnn-propagation)
6. [Classifier](#6-classifier)
7. [–ü–æ–ª–Ω—ã–π Forward Pass](#7-–ø–æ–ª–Ω—ã–π-forward-pass)

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –¥–µ—Ç–∞–ª–∏
8. [–†–∞–∑–º–µ—Ä—ã —Ç–µ–Ω–∑–æ—Ä–æ–≤](#8-—Ä–∞–∑–º–µ—Ä—ã-—Ç–µ–Ω–∑–æ—Ä–æ–≤-–Ω–∞-–∫–∞–∂–¥–æ–º-—ç—Ç–∞–ø–µ)
9. [–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã](#9-–≥—Ä–∞–¥–∏–µ–Ω—Ç—ã-–∏-backpropagation)
10. [Loss —Ñ—É–Ω–∫—Ü–∏–∏](#10-loss-—Ñ—É–Ω–∫—Ü–∏–∏)
11. [Optimizer](#11-optimizer-–∏-scheduler)
12. [Training Loop](#12-training-loop)
13. [–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤](#13-–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)

### –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ–º—ã
14. [–î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä DAGNN propagation](#14-–¥–µ—Ç–∞–ª—å–Ω—ã–π-—Ä–∞–∑–±–æ—Ä-dagnn-propagation)
15. [–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏ best practices](#15-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ-–ø–∞—Ç—Ç–µ—Ä–Ω—ã-–∏-best-practices)
16. [–ó–∞–∫–ª—é—á–µ–Ω–∏–µ](#16-–∑–∞–∫–ª—é—á–µ–Ω–∏–µ)

---

## 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö

### –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

**–§–∞–π–ª:** `compositionsDAG.json`
```json
{
  "id": "comp_001",
  "composition": {
    "nodes": [
      {"id": "n1", "service": "table_1002132"},
      {"id": "n2", "service": "service_308"},
      {"id": "n3", "service": "service_309"}
    ],
    "links": [
      {"source": "n1", "target": "n2"},
      {"source": "n2", "target": "n3"}
    ]
  }
}
```

**–ß—Ç–æ –∏–∑–≤–ª–µ–∫–∞–µ–º:**
- 943 –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ ‚Üí 106 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π
- –ü—Ä–∏–º–µ—Ä –ø—É—Ç–∏: `['table_1002132', 'service_308', 'service_309']`

---

### –®–∞–≥ 1.1: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π

**–§—É–Ω–∫—Ü–∏—è:** `build_graph_from_real_paths(paths)`

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:**
```python
path_graph = nx.DiGraph()  # –°–æ–∑–¥–∞–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ

for path in paths:  # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏
    for i in range(len(path) - 1):
        source = path[i]      # –ù–∞–ø—Ä–∏–º–µ—Ä: 'table_1002132'
        target = path[i + 1]  # –ù–∞–ø—Ä–∏–º–µ—Ä: 'service_308'
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —É–∑–ª–∞
        source_type = 'service' if source.startswith('service') else 'table'
        target_type = 'service' if target.startswith('service') else 'table'
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –≥—Ä–∞—Ñ
        path_graph.add_node(source, type=source_type)
        path_graph.add_node(target, type=target_type)
        path_graph.add_edge(source, target)
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
```
–ì—Ä–∞—Ñ:
  - 50 —É–∑–ª–æ–≤ (15 —Å–µ—Ä–≤–∏—Å–æ–≤ + 35 —Ç–∞–±–ª–∏—Ü)
  - 97 –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ä–µ–±–µ—Ä
  - –°–≤—è–∑–∏: table ‚Üí service, service ‚Üí service
```

**–ó–∞—á–µ–º –Ω—É–∂–µ–Ω –≥—Ä–∞—Ñ:**
- –ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –º–µ–∂–¥—É —É–∑–ª–∞–º–∏
- DAGNN –±—É–¥–µ—Ç —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ —ç—Ç–∏–º —Å–≤—è–∑—è–º
- –ì—Ä–∞—Ñ –∫–æ–¥–∏—Ä—É–µ—Ç, –∫–∞–∫–∏–µ —É–∑–ª—ã –≤–ª–∏—è—é—Ç –¥—Ä—É–≥ –Ω–∞ –¥—Ä—É–≥–∞

---

### –®–∞–≥ 1.2: –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤

**–§—É–Ω–∫—Ü–∏—è:** `create_training_data(paths)`

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:**
```python
X_raw = []  # –ö–æ–Ω—Ç–µ–∫—Å—Ç—ã
y_raw = []  # –¶–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

for path in paths:
    # path = ['table_1002132', 'service_308', 'service_309']
    
    for i in range(1, len(path)):
        context = tuple(path[:i])  # ('table_1002132',) –∏–ª–∏ ('table_1002132', 'service_308')
        next_step = path[i]         # 'service_308' –∏–ª–∏ 'service_309'
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ—Ö–æ–¥—ã –Ω–∞ —Å–µ—Ä–≤–∏—Å—ã (–æ–Ω–∏ - —Ü–µ–ª–µ–≤—ã–µ –∫–ª–∞—Å—Å—ã)
        if next_step.startswith("service"):
            X_raw.append(context)
            y_raw.append(next_step)
```

**–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:**

| ‚Ññ | –ö–æ–Ω—Ç–µ–∫—Å—Ç (X_raw) | –¶–µ–ª–µ–≤–æ–π —Å–µ—Ä–≤–∏—Å (y_raw) | –ü–æ—è—Å–Ω–µ–Ω–∏–µ |
|---|------------------|------------------------|-----------|
| 0 | `('table_1002132',)` | `'service_308'` | –ü–æ—Å–ª–µ —Ç–∞–±–ª–∏—Ü—ã ‚Üí –ø–µ—Ä–≤—ã–π —Å–µ—Ä–≤–∏—Å |
| 1 | `('table_1002132', 'service_308')` | `'service_309'` | –ü–æ—Å–ª–µ table+service ‚Üí —Å–ª–µ–¥—É—é—â–∏–π —Å–µ—Ä–≤–∏—Å |

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- 125 –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
- X_raw: —Å–ø–∏—Å–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ (–ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã)
- y_raw: –Ω–∞–∑–≤–∞–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤

**–ó–∞—á–µ–º:**
- –ó–∞–¥–∞—á–∞: –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Å–µ—Ä–≤–∏—Å
- –≠—Ç–æ supervised learning - –Ω—É–∂–Ω—ã –ø–∞—Ä—ã (–≤—Ö–æ–¥, –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç)

---

### –®–∞–≥ 1.3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è PyTorch Geometric

**–§—É–Ω–∫—Ü–∏—è:** `prepare_pytorch_geometric_data(dag, X_raw, y_raw, paths)`

#### 1.3.1: –ú–∞–ø–ø–∏–Ω–≥ —É–∑–ª–æ–≤ ‚Üí –∏–Ω–¥–µ–∫—Å—ã

```python
node_list = list(path_graph.nodes)
# ['table_1002132', 'table_1002133', ..., 'service_308', 'service_309', ...]

node_encoder = LabelEncoder()
node_ids = node_encoder.fit_transform(node_list)
# [0, 1, 2, ..., 48, 49]

node_map = {node: idx for node, idx in zip(node_list, node_ids)}
# {
#   'table_1002132': 0,
#   'table_1002133': 1,
#   ...
#   'service_308': 35,
#   'service_309': 36,
#   ...
# }
```

**–ó–∞—á–µ–º –Ω—É–∂–µ–Ω node_map:**
- –ù–µ–π—Ä–æ—Å–µ—Ç–∏ —Ä–∞–±–æ—Ç–∞—é—Ç —Å —á–∏—Å–ª–∞–º–∏, –Ω–µ —Å–æ —Å—Ç—Ä–æ–∫–∞–º–∏
- –ö–∞–∂–¥–æ–º—É —É–∑–ª—É –Ω–∞–∑–Ω–∞—á–∞–µ—Ç—Å—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å 0-49
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è edge_index –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤

#### 1.3.2: –°–æ–∑–¥–∞–Ω–∏–µ edge_index

```python
edge_index = torch.tensor(
    [[node_map[u], node_map[v]] for u, v in path_graph.edges],
    dtype=torch.long
).t()

# –ü—Ä–∏–º–µ—Ä:
# path_graph.edges: [('table_1002132', 'service_308'), ('service_308', 'service_309'), ...]
# 
# edge_index –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è:
# tensor([[0, 35, 36, ...],    ‚Üê –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (source nodes)
#         [35, 36, 37, ...]])  ‚Üê –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è (target nodes)
```

**–§–æ—Ä–º–∞—Ç edge_index: [2, num_edges]**
```
edge_index[0] = [0,  35, 36, ...]  ‚Üê –∏–Ω–¥–µ–∫—Å—ã —É–∑–ª–æ–≤-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
edge_index[1] = [35, 36, 37, ...]  ‚Üê –∏–Ω–¥–µ–∫—Å—ã —É–∑–ª–æ–≤-–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–π

–û–∑–Ω–∞—á–∞–µ—Ç —Ä–µ–±—Ä–∞:
  0 ‚Üí 35  (table_1002132 ‚Üí service_308)
  35 ‚Üí 36 (service_308 ‚Üí service_309)
  36 ‚Üí 37 (service_309 ‚Üí service_312)
  ...
```

**–ó–∞—á–µ–º –Ω—É–∂–µ–Ω edge_index:**
- **–ì–ª–∞–≤–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è GNN!**
- –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≥—Ä–∞—Ñ–∞
- DAGNN –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–≥–æ –¥–ª—è propagation (—Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)
- –ì–æ–≤–æ—Ä–∏—Ç: "–∫—Ç–æ —Å –∫–µ–º —Å–≤—è–∑–∞–Ω –∏ –≤ –∫–∞–∫–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏"

#### 1.3.3: –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —É–∑–ª–æ–≤ (x)

```python
features = [
    [1, 0] if path_graph.nodes[n]['type'] == 'service' 
    else [0, 1] 
    for n in node_list
]

x = torch.tensor(features, dtype=torch.float)
# –†–µ–∑—É–ª—å—Ç–∞—Ç: tensor([
#   [0, 1],  ‚Üê table_1002132
#   [0, 1],  ‚Üê table_1002133
#   ...
#   [1, 0],  ‚Üê service_308
#   [1, 0],  ‚Üê service_309
#   ...
# ])
# –†–∞–∑–º–µ—Ä: [50, 2]
```

**–ü—Ä–∏–∑–Ω–∞–∫–∏ —É–∑–ª–æ–≤:**
- `[1, 0]` = —Å–µ—Ä–≤–∏—Å (is_service=1, is_table=0)
- `[0, 1]` = —Ç–∞–±–ª–∏—Ü–∞ (is_service=0, is_table=1)

**–ó–∞—á–µ–º —Ç–æ–ª—å–∫–æ 2 –ø—Ä–∏–∑–Ω–∞–∫–∞:**
- –ù–∞ –º–∞–ª–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –ø—Ä–æ—Å—Ç—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –ª—É—á—à–µ
- –ë–æ–ª—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ‚Üí –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
- DAGNN —Å–∞–º –∏–∑–≤–ª–µ—á–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —á–µ—Ä–µ–∑ propagation

#### 1.3.4: –°–æ–∑–¥–∞–Ω–∏–µ PyTorch Geometric Data

```python
data_pyg = Data(x=x, edge_index=edge_index)

# data_pyg —Å–æ–¥–µ—Ä–∂–∏—Ç:
#   - x: [50, 2] - –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤—Å–µ—Ö 50 —É–∑–ª–æ–≤
#   - edge_index: [2, 97] - 97 –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ä–µ–±–µ—Ä
```

**–ó–∞—á–µ–º Data object:**
- –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç PyTorch Geometric
- –£–¥–æ–±–Ω–æ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –≤ GNN –º–æ–¥–µ–ª–∏
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π

#### 1.3.5: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –∏ —Ü–µ–ª–µ–π

```python
# –ö–æ–Ω—Ç–µ–∫—Å—Ç—ã - –∏–Ω–¥–µ–∫—Å—ã –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —É–∑–ª–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö
contexts = torch.tensor([node_map[context[-1]] for context in X_raw], dtype=torch.long)
# contexts = [0, 35, 36, ...]  - –∏–Ω–¥–µ–∫—Å—ã —É–∑–ª–æ–≤ –≤ node_map
# –†–∞–∑–º–µ—Ä: [125]

# –¶–µ–ª–µ–≤—ã–µ –∫–ª–∞—Å—Å—ã - —Å–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ –¢–û–õ–¨–ö–û –¥–ª—è —Å–µ—Ä–≤–∏—Å–æ–≤
unique_services = sorted(set(y_raw))
# ['service_306', 'service_307', ..., 'service_397'] - 15 —Å–µ—Ä–≤–∏—Å–æ–≤

service_map = {service: idx for idx, service in enumerate(unique_services)}
# {
#   'service_306': 0,
#   'service_307': 1,
#   'service_308': 2,
#   ...
#   'service_397': 14
# }

targets = torch.tensor([service_map[y] for y in y_raw], dtype=torch.long)
# targets = [2, 3, 2, ...]  - –∏–Ω–¥–µ–∫—Å—ã –∫–ª–∞—Å—Å–æ–≤ –≤ service_map
# –†–∞–∑–º–µ—Ä: [125]
```

**–í–∞–∂–Ω–æ:**
- `contexts` –∏—Å–ø–æ–ª—å–∑—É–µ—Ç `node_map` (–∏–Ω–¥–µ–∫—Å—ã 0-49 –¥–ª—è –≤—Å–µ—Ö —É–∑–ª–æ–≤)
- `targets` –∏—Å–ø–æ–ª—å–∑—É–µ—Ç `service_map` (–∏–Ω–¥–µ–∫—Å—ã 0-14 –¥–ª—è —Å–µ—Ä–≤–∏—Å–æ–≤)
- –≠—Ç–æ –†–ê–ó–ù–´–ï –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∏–Ω–¥–µ–∫—Å–æ–≤!

**–ó–∞—á–µ–º –¥–≤–∞ –º–∞–ø–ø–∏–Ω–≥–∞:**
- `node_map`: –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≥—Ä–∞—Ñ–æ–º (–≤—Å–µ 50 —É–∑–ª–æ–≤)
- `service_map`: –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (—Ç–æ–ª—å–∫–æ 15 —Ü–µ–ª–µ–≤—ã—Ö –∫–ª–∞—Å—Å–æ–≤)

---

### –ò—Ç–æ–≥ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö

**–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:**

1. **data_pyg** - –≥—Ä–∞—Ñ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏:
   - `x`: [50, 2] - –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤—Å–µ—Ö —É–∑–ª–æ–≤
   - `edge_index`: [2, 97] - —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≥—Ä–∞—Ñ–∞

2. **contexts** - [125] - –∏–Ω–¥–µ–∫—Å—ã –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —É–∑–ª–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –≤ node_map

3. **targets** - [125] - –∏–Ω–¥–µ–∫—Å—ã —Ü–µ–ª–µ–≤—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ –≤ service_map

4. **node_map** - 50 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–≤—Å–µ —É–∑–ª—ã ‚Üí –∏–Ω–¥–µ–∫—Å—ã 0-49)

5. **service_map** - 15 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (—Å–µ—Ä–≤–∏—Å—ã ‚Üí –∏–Ω–¥–µ–∫—Å—ã 0-14)

**Split –Ω–∞ train/test:**
```python
# 87 train / 38 test
contexts_train, contexts_test, targets_train, targets_test = train_test_split(
    contexts, targets, test_size=0.3, random_state=42
)
```

---

## 2. –í—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ DAGNNRecommender

```python
class DAGNNRecommender(nn.Module):
    def __init__(self, 
                 in_channels: int,      # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —É–∑–ª–æ–≤
                 hidden_channels: int,  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤
                 out_channels: int,     # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                 K: int = 10,           # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ propagation hops
                 dropout: float = 0.4   # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å dropout
    ):
```

### –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –Ω–∞—à–µ–π –∑–∞–¥–∞—á–∏:

```python
model = DAGNNRecommender(
    in_channels=2,        # 2 –ø—Ä–∏–∑–Ω–∞–∫–∞ —É–∑–ª–∞: [is_service, is_table]
    hidden_channels=64,   # –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —É–∑–ª–æ–≤
    out_channels=15,      # 15 –∫–ª–∞—Å—Å–æ–≤ (—Å–µ—Ä–≤–∏—Å–æ–≤) –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    K=10,                 # 10 hops —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    dropout=0.4           # 40% dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
)
```

### –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:

**in_channels = 2:**
- –ö–∞–∂–¥—ã–π —É–∑–µ–ª –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è 2 –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏: `[is_service, is_table]`
- –ü—Ä–æ—Å—Ç—ã–µ –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –ª—É—á—à–µ –Ω–∞ –º–∞–ª–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
- –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π –¥–æ–ª–∂–µ–Ω –ø—Ä–∏–Ω—è—Ç—å —ç—Ç–∏ 2 –ø—Ä–∏–∑–Ω–∞–∫–∞

**hidden_channels = 64:**
- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π (—ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)
- –í—Å–µ —É–∑–ª—ã –±—É–¥—É—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤–µ–∫—Ç–æ—Ä–∞–º–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 64
- –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤, –Ω–æ –Ω–µ –∏–∑–±—ã—Ç–æ—á–Ω–æ

**out_channels = 15:**
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤ = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–µ–ª–µ–≤—ã—Ö –∫–ª–∞—Å—Å–æ–≤
- 15 —Å–µ—Ä–≤–∏—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å
- –ö–∞–∂–¥—ã–π –Ω–µ–π—Ä–æ–Ω = –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞

**K = 10:**
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è (propagation hops)
- –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –Ω–∞ —Å–æ—Å–µ–¥–µ–π
- 10 hops –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–æ–π—Ç–∏ –¥–∞–ª–µ–∫–æ –ø–æ –≥—Ä–∞—Ñ—É

**dropout = 0.4:**
- –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å–ª—É—á–∞–π–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º 40% –Ω–µ–π—Ä–æ–Ω–æ–≤
- –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
- –£–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—é

---

### –î–∞–Ω–Ω—ã–µ –Ω–∞ –≤—Ö–æ–¥–µ forward pass

```python
def forward(self, x, edge_index, training=True):
    # x: [50, 2] - –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤—Å–µ—Ö —É–∑–ª–æ–≤ –≥—Ä–∞—Ñ–∞
    # edge_index: [2, 97] - —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≥—Ä–∞—Ñ–∞
    # training: True/False - —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è/inference
```

**x: Node features [50, 2]**
```python
x = tensor([
    [0, 1],  # —É–∑–µ–ª 0 (table_1002132): is_table
    [0, 1],  # —É–∑–µ–ª 1 (table_1002133): is_table
    ...
    [1, 0],  # —É–∑–µ–ª 35 (service_308): is_service
    [1, 0],  # —É–∑–µ–ª 36 (service_309): is_service
    ...
])
```

**edge_index: Graph structure [2, 97]**
```python
edge_index = tensor([
    [0,  35, 36, 1,  35, ...],  # –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    [35, 36, 37, 35, 40, ...]   # –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è
])

# –û–∑–Ω–∞—á–∞–µ—Ç —Ä–µ–±—Ä–∞:
# 0 ‚Üí 35:  table_1002132 ‚Üí service_308
# 35 ‚Üí 36: service_308 ‚Üí service_309
# 36 ‚Üí 37: service_309 ‚Üí service_312
# ...
```

**training: —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã**
- `True`: –æ–±—É—á–µ–Ω–∏–µ (dropout –∞–∫—Ç–∏–≤–µ–Ω, BatchNorm –≤ train mode)
- `False`: inference (dropout –≤—ã–∫–ª—é—á–µ–Ω, BatchNorm –≤ eval mode)

---

## 3. Encoder Block

### –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ
–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —É–∑–ª–æ–≤ (2 —Ñ–∏—á–∏) –≤ –±–æ–≥–∞—Ç—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (64 dims)

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```python
# –°–ª–æ–π 1: –ü–µ—Ä–≤–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
self.lin1 = nn.Linear(in_channels, hidden_channels)     # 2 ‚Üí 64
self.bn1 = nn.BatchNorm1d(hidden_channels)              # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
```

### Forward pass –≤ Encoder

```python
# –®–ê–ì 1: –õ–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
x = self.lin1(x)
# –í—Ö–æ–¥:  [50, 2]
# –í—ã—Ö–æ–¥: [50, 64]
# –§–æ—Ä–º—É–ª–∞: x_out = x_in @ W + b
# W: [2, 64] - –≤–µ—Å–∞, b: [64] - bias

# –®–ê–ì 2: Batch Normalization
x = self.bn1(x)
# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –±–∞—Ç—á—É –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
# –§–æ—Ä–º—É–ª–∞: x_norm = (x - mean) / sqrt(var + eps)

# –®–ê–ì 3: –ê–∫—Ç–∏–≤–∞—Ü–∏—è ReLU
x = F.relu(x)
# –§–æ—Ä–º—É–ª–∞: relu(x) = max(0, x)
# –î–æ–±–∞–≤–ª—è–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å

# –®–ê–ì 4: Dropout (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ training=True)
x = F.dropout(x, p=self.dropout, training=training)
# –°–ª—É—á–∞–π–Ω–æ –æ–±–Ω—É–ª—è–µ—Ç 40% —ç–ª–µ–º–µ–Ω—Ç–æ–≤
# –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å–ª–µ Encoder:**
```python
x: [50, 64]
# –ö–∞–∂–¥—ã–π —É–∑–µ–ª —Ç–µ–ø–µ—Ä—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≤–µ–∫—Ç–æ—Ä–æ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 64
# –í–º–µ—Å—Ç–æ [is_service, is_table] –∏–º–µ–µ–º –±–æ–≥–∞—Ç–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
```

**–ó–∞—á–µ–º Encoder:**
- –£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (2 ‚Üí 64)
- –°–æ–∑–¥–∞–µ—Ç "–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤" –¥–ª—è —É–∑–ª–æ–≤
- –ü–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—å—Å—è —Å–ª–æ–∂–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º

---

## 4. Residual Connection

### –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ
–î–æ–±–∞–≤–∏—Ç—å "shortcut" –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∏ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```python
self.lin2 = nn.Linear(hidden_channels, hidden_channels)  # 64 ‚Üí 64
self.bn2 = nn.BatchNorm1d(hidden_channels)
```

### Forward pass —Å Residual

```python
# –®–ê–ì 1: –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ö–æ–¥ (identity)
identity = x  # [50, 64] - –∑–∞–ø–æ–º–∏–Ω–∞–µ–º

# –®–ê–ì 2: –í—Ç–æ—Ä–æ–µ –ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
x = self.lin2(x)
# [50, 64] ‚Üí [50, 64]

# –®–ê–ì 3: Batch Normalization
x = self.bn2(x)

# –®–ê–ì 4: –ê–∫—Ç–∏–≤–∞—Ü–∏—è
x = F.relu(x)

# –®–ê–ì 5: Residual connection (–ö–õ–Æ–ß–ï–í–û–ô –ú–û–ú–ï–ù–¢!)
x = x + identity
# –§–æ—Ä–º—É–ª–∞: output = F(x) + x
# –≥–¥–µ F(x) = relu(BN(Linear(x)))
```

**–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Residual:**
```
      x [50, 64]
       ‚Üì         \
    Linear        \
       ‚Üì           \  (skip connection)
   BatchNorm       \
       ‚Üì            \
     ReLU            \
       ‚Üì              ‚Üì
       ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥ (—Å–ª–æ–∂–µ–Ω–∏–µ)
             ‚Üì
    x + identity [50, 64]
```

**–ó–∞—á–µ–º –Ω—É–∂–Ω–∞ residual connection:**

1. **–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ç–µ—á—É—Ç –ª—É—á—à–µ:**
   ```
   –ü—Ä–∏ backpropagation –≥—Ä–∞–¥–∏–µ–Ω—Ç –º–æ–∂–µ—Ç "–æ–±–æ–π—Ç–∏" —Å–ª–æ–π
   ‚àÇLoss/‚àÇx_in = ‚àÇLoss/‚àÇx_out * (‚àÇF/‚àÇx + 1)
                                        ‚Üë
                                 –æ—Ç skip connection
   ```

2. **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è:**
   - –ï—Å–ª–∏ F(x) ‚âà 0, —Ç–æ x ‚âà identity (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤—Ö–æ–¥)
   - –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã–º –∏–∑–º–µ–Ω–µ–Ω–∏—è–º, –Ω–µ —Ä–µ–∑–∫–∏–º

3. **–õ—É—á—à–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è:**
   - –ü–æ–º–æ–≥–∞–µ—Ç –æ–±—É—á–∞—Ç—å –≥–ª—É–±–æ–∫–∏–µ —Å–µ—Ç–∏
   - –ò–∑–±–µ–≥–∞–µ—Ç "vanishing gradients"

**–®–ê–ì 6: Dropout**
```python
x = F.dropout(x, p=self.dropout, training=training)
# –í—ã—Ö–æ–¥: [50, 64]
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å–ª–µ Residual Block:**
```python
x: [50, 64]
# –£–ª—É—á—à–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å —É—á–µ—Ç–æ–º residual –æ–±—É—á–µ–Ω–∏—è
```

---

## 5. DAGNN Propagation

### –ß—Ç–æ —ç—Ç–æ —Ç–∞–∫–æ–µ

**DAGNN = Directed Acyclic Graph Neural Network**

–û—Å–Ω–æ–≤–∞–Ω –Ω–∞ **APPNP** (Approximate Personalized Propagation of Neural Predictions)

### –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ
–†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –º–µ–∂–¥—É —É–∑–ª–∞–º–∏ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –≥—Ä–∞—Ñ–∞

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ DAGNN

```python
class DAGNN(nn.Module):
    def __init__(self, in_channels: int, K: int, dropout: float = 0.5):
        super().__init__()
        self.propagation = APPNP(K=K, alpha=0.1)
        self.att = nn.Parameter(torch.Tensor(K + 1))  # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö hops
        self.dropout = dropout
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `K = 10`: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ propagation hops
- `alpha = 0.1`: teleport probability (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å "–≤–µ—Ä–Ω—É—Ç—å—Å—è" –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é)
- `att`: [11] - –æ–±—É—á–∞–µ–º—ã–µ –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ hop

### –§–æ—Ä–º—É–ª–∞ APPNP Propagation

–ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ k:

```
H^(k+1) = (1 - Œ±) ¬∑ A ¬∑ H^(k) + Œ± ¬∑ H^(0)
```

**–ì–¥–µ:**
- `H^(k)` - –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —É–∑–ª–æ–≤ –Ω–∞ —à–∞–≥–µ k
- `H^(0)` - –∏—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è (–æ—Ç encoder)
- `A` - –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Å–º–µ–∂–Ω–æ—Å—Ç–∏
- `Œ± = 0.1` - teleport probability

**–ò–Ω—Ç—É–∏—Ü–∏—è:**
- **(1 - Œ±) ¬∑ A ¬∑ H^(k)**: –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º –æ—Ç —Å–æ—Å–µ–¥–µ–π (90%)
- **Œ± ¬∑ H^(0)**: "–ø–æ–º–Ω–∏–º" –∏—Å—Ö–æ–¥–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ (10%)
- –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ—Ç —Å–æ—Å–µ–¥–µ–π –∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏

### Forward pass –≤ DAGNN

```python
def forward(self, x, edge_index, training=True):
    # –í—Ö–æ–¥:
    #   x: [50, 64] - —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ—Ç encoder
    #   edge_index: [2, 97] - —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≥—Ä–∞—Ñ–∞
    
    xs = [x]  # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –Ω–∞ –∫–∞–∂–¥–æ–º hop
    
    # –í–µ—Å–∞ —Ä–µ–±–µ—Ä (–≤—Å–µ —Ä–∞–≤–Ω—ã 1.0)
    edge_weight = torch.ones(edge_index.size(1), dtype=torch.float32)
    # [97] - –≤–µ—Å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ 97 —Ä–µ–±–µ—Ä
    
    # –û–°–ù–û–í–ù–û–ô –¶–ò–ö–õ: K = 10 hops
    for hop in range(self.propagation.K):  # hop = 0, 1, 2, ..., 9
        
        # PROPAGATION STEP
        x = self.propagation.propagate(edge_index, x=x, edge_weight=edge_weight)
        # –§–æ—Ä–º—É–ª–∞: x_new[i] = Œ£(x[j] * edge_weight[j‚Üíi]) –¥–ª—è –≤—Å–µ—Ö j —Å–æ—Å–µ–¥–µ–π i
        
        # Dropout (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)
        if training:
            x = F.dropout(x, p=self.dropout, training=training)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —ç—Ç–æ–≥–æ hop
        xs.append(x)
    
    # –¢–µ–ø–µ—Ä—å xs —Å–æ–¥–µ—Ä–∂–∏—Ç K+1 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π:
    # xs[0] = H^(0) - –∏—Å—Ö–æ–¥–Ω–æ–µ
    # xs[1] = H^(1) - –ø–æ—Å–ª–µ 1 hop
    # xs[2] = H^(2) - –ø–æ—Å–ª–µ 2 hops
    # ...
    # xs[10] = H^(10) - –ø–æ—Å–ª–µ 10 hops
```

### –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä –æ–¥–Ω–æ–≥–æ hop

**–ü—Ä–∏–º–µ—Ä –¥–ª—è —É–∑–ª–∞ service_308 (–∏–Ω–¥–µ–∫—Å 35):**

```python
# –î–æ propagation:
x[35] = [0.5, 0.2, 0.8, ..., 0.3]  # 64-–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä

# –ù–∞—Ö–æ–¥–∏–º —Å–æ—Å–µ–¥–µ–π –ø–æ edge_index:
# edge_index[:, edge_index[1] == 35] ‚Üí –≤—Ö–æ–¥—è—â–∏–µ —Ä–µ–±—Ä–∞ –≤ —É–∑–µ–ª 35
# –ü—É—Å—Ç—å —Å–æ—Å–µ–¥–∏: [0, 34] (table_1002132 –∏ service_307)

# –ê–≥—Ä–µ–≥–∞—Ü–∏—è:
x_new[35] = (1 - 0.1) * (x[0] + x[34]) / 2 + 0.1 * x_original[35]
          = 0.9 * mean(neighbors) + 0.1 * original
          
# –ü–æ—Å–ª–µ propagation:
x[35] = [0.52, 0.25, 0.75, ..., 0.28]  # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
```

**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:**
1. –£–∑–µ–ª –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ—Ç –≤—Ö–æ–¥—è—â–∏—Ö —Å–æ—Å–µ–¥–µ–π
2. 90% - –æ—Ç —Å–æ—Å–µ–¥–µ–π, 10% - —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–≤–æ–µ –∏—Å—Ö–æ–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
3. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è "—Ç–µ—á–µ—Ç" –ø–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—é —Ä–µ–±–µ—Ä

### Attention –¥–ª—è —Ä–∞–∑–Ω—ã—Ö hops

–ü–æ—Å–ª–µ K hops –∏–º–µ–µ–º K+1 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π. –ù—É–∂–Ω–æ –∏—Ö –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å:

```python
# –°–∫–ª–∞–¥—ã–≤–∞–µ–º –≤—Å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤ –æ–¥–∏–Ω —Ç–µ–Ω–∑–æ—Ä
out = torch.stack(xs, dim=-1)
# –†–∞–∑–º–µ—Ä: [50, 64, 11]
#         —É–∑–ª—ã √ó dims √ó hops

# –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ hop
att_weights = F.softmax(self.att, dim=0)
# self.att: [11] - –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
# att_weights: [11] - –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞ (—Å—É–º–º–∞ = 1)
# –ü—Ä–∏–º–µ—Ä: [0.05, 0.08, 0.12, 0.15, 0.20, 0.15, 0.10, 0.08, 0.04, 0.02, 0.01]

# –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ –ø–æ hops
out = (out * att_weights.view(1, 1, -1)).sum(dim=-1)
# –§–æ—Ä–º—É–ª–∞: out[node, dim] = Œ£(xs[k][node, dim] * att_weights[k]) for k in 0..K
# –†–∞–∑–º–µ—Ä: [50, 64]
```

**–ó–∞—á–µ–º attention –¥–ª—è hops:**
- –†–∞–∑–Ω—ã–µ hops —Å–æ–¥–µ—Ä–∂–∞—Ç —Ä–∞–∑–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
- Hop 0: –ª–æ–∫–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —É–∑–ª–∞
- Hop 1-2: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç –ø—Ä—è–º—ã—Ö —Å–æ—Å–µ–¥–µ–π  
- Hop 3-5: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç –¥–∞–ª—å–Ω–∏—Ö —É–∑–ª–æ–≤
- Hop 6-10: –≥–ª–æ–±–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞
- **–ú–æ–¥–µ–ª—å –°–ê–ú–ê —É—á–∏—Ç—Å—è**, –∫–∞–∫–∏–µ hops –≤–∞–∂–Ω–µ–µ!

**–ü—Ä–∏–º–µ—Ä –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è:**
```
att_weights = [0.05, 0.08, 0.12, 0.15, 0.20, 0.15, 0.10, 0.08, 0.04, 0.02, 0.01]
               ‚Üë     ‚Üë     ‚Üë     ‚Üë     ‚Üë     ‚Üë     ‚Üë     ‚Üë     ‚Üë     ‚Üë     ‚Üë
              hop0  hop1  hop2  hop3  hop4  hop5  hop6  hop7  hop8  hop9  hop10

–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å –Ω–∞ hop 4 (0.20) ‚Üí –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è 4 —à–∞–≥–∞ –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω–∞
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç DAGNN Propagation:**
```python
x: [50, 64]
# –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —É–∑–ª–æ–≤ —Å —É—á–µ—Ç–æ–º –í–°–ï–ì–û –≥—Ä–∞—Ñ–∞
# –ö–∞–∂–¥—ã–π —É–∑–µ–ª "–∑–Ω–∞–µ—Ç" –æ —Å–≤–æ–∏—Ö —Å–æ—Å–µ–¥—è—Ö (—á–µ—Ä–µ–∑ propagation)
```

---

### –ü–æ–¥—Ä–æ–±–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ APPNP

**–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è:**
```
H^(0) = x  # –ò—Å—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ—Ç encoder
```

**–ò—Ç–µ—Ä–∞—Ü–∏—è (–¥–ª—è –∫–∞–∂–¥–æ–≥–æ hop k = 1, 2, ..., K):**
```
H^(k) = (1 - Œ±) ¬∑ D^(-1/2) ¬∑ A ¬∑ D^(-1/2) ¬∑ H^(k-1) + Œ± ¬∑ H^(0)
```

**–ì–¥–µ:**
- `A`: –º–∞—Ç—Ä–∏—Ü–∞ —Å–º–µ–∂–Ω–æ—Å—Ç–∏ (–∏–∑ edge_index)
- `D`: –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Å—Ç–µ–ø–µ–Ω–µ–π —É–∑–ª–æ–≤
- `Œ± = 0.1`: teleport probability
- `D^(-1/2) ¬∑ A ¬∑ D^(-1/2)`: –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ (—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)

**–í –∫–æ–¥–µ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ):**
```python
H_0 = x  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ
H = x

for k in range(K):
    # Propagate: –∞–≥—Ä–µ–≥–∞—Ü–∏—è –æ—Ç —Å–æ—Å–µ–¥–µ–π
    H = propagate(edge_index, H)  # –ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
    
    # Teleport: –¥–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
    H = (1 - alpha) * H + alpha * H_0
    # H = 0.9 * (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç —Å–æ—Å–µ–¥–µ–π) + 0.1 * (–∏—Å—Ö–æ–¥–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)
```

**–ü—Ä–∏–º–µ—Ä —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è (service_308):**

```
Hop 0: [0.5, 0.2, 0.8, ...]  ‚Üê –∏—Å—Ö–æ–¥–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥

Hop 1: –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –æ—Ç table_1002132
       [0.52, 0.25, 0.75, ...] = 0.9 * (—ç–º–±–µ–¥–¥–∏–Ω–≥ table) + 0.1 * hop0

Hop 2: –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –æ—Ç —Å–æ—Å–µ–¥–µ–π hop 1
       [0.48, 0.30, 0.70, ...] = 0.9 * (—Å–æ—Å–µ–¥–∏) + 0.1 * hop0

...

Hop 10: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç –≤—Å–µ–≥–æ –≥—Ä–∞—Ñ–∞
        [0.55, 0.28, 0.72, ...]
```

---

## 6. Classifier

### –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ
–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —É–∑–ª–æ–≤ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```python
# –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
self.lin3 = nn.Linear(hidden_channels, hidden_channels // 2)  # 64 ‚Üí 32
self.bn3 = nn.BatchNorm1d(hidden_channels // 2)

# –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
self.lin_out = nn.Linear(hidden_channels // 2, out_channels)  # 32 ‚Üí 15
```

### Forward pass –≤ Classifier

```python
# –ü–û–°–õ–ï DAGNN –∏–º–µ–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—Å–µ—Ö 50 —É–∑–ª–æ–≤: [50, 64]
x = self.dagnn(x, edge_index, training=training)

# –®–ê–ì 1: –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Å–ª–æ–π
x = self.lin3(x)
# [50, 64] ‚Üí [50, 32]
# –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è

# –®–ê–ì 2: Batch Normalization
x = self.bn3(x)

# –®–ê–ì 3: –ê–∫—Ç–∏–≤–∞—Ü–∏—è
x = F.relu(x)

# –®–ê–ì 4: Dropout
x = F.dropout(x, p=self.dropout, training=training)

# –®–ê–ì 5: –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (–§–ò–ù–ê–õ–¨–ù–´–ï –õ–û–ì–ò–¢–´)
x = self.lin_out(x)
# [50, 32] ‚Üí [50, 15]
# –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ 50 —É–∑–ª–æ–≤ - 15 –ª–æ–≥–∏—Ç–æ–≤ (–æ—Ü–µ–Ω–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞)
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç Classifier:**
```python
x: [50, 15]
# –î–ª—è –∫–∞–∂–¥–æ–≥–æ —É–∑–ª–∞: 15 –ª–æ–≥–∏—Ç–æ–≤ (–æ—Ü–µ–Ω–æ–∫) –¥–ª—è 15 –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤

# –ü—Ä–∏–º–µ—Ä –¥–ª—è —É–∑–ª–∞ 35 (service_308):
x[35] = [-2.1, -1.8, 3.5, 2.1, -0.5, ..., -1.0]
         ‚Üë     ‚Üë     ‚Üë    ‚Üë
       s_306 s_307 s_308 s_309
```

**–ó–∞—á–µ–º –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–π classifier:**
- `64 ‚Üí 32`: –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–∂–∞—Ç–∏–µ
- `32 ‚Üí 15`: —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è
- BatchNorm + ReLU –º–µ–∂–¥—É —Å–ª–æ—è–º–∏: –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
- Dropout: —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è

---

## 7. –ü–æ–ª–Ω—ã–π Forward Pass

### –®–∞–≥ –∑–∞ —à–∞–≥–æ–º —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –ø—Ä–∏–º–µ—Ä–æ–º

**–ó–∞–¥–∞—á–∞:**
```
–ö–æ–Ω—Ç–µ–∫—Å—Ç: ('table_1002132', 'service_308')
–ù—É–∂–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å: —Å–ª–µ–¥—É—é—â–∏–π —Å–µ—Ä–≤–∏—Å
```

### –®–ê–ì 0: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

```python
# –ì—Ä–∞—Ñ (–≤—Å–µ 50 —É–∑–ª–æ–≤)
x = tensor([
    [0, 1],  # —É–∑–µ–ª 0: table_1002132
    ...
    [1, 0],  # —É–∑–µ–ª 35: service_308
    ...
])  # [50, 2]

edge_index = tensor([
    [0, 35, ...],  # –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    [35, 36, ...]  # –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è
])  # [2, 97]

# –ö–æ–Ω—Ç–µ–∫—Å—Ç (–∏–Ω–¥–µ–∫—Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —É–∑–ª–∞)
context_id = 35  # service_308 –≤ node_map
```

---

### –®–ê–ì 1: ENCODER

```python
# 1.1: Linear projection
x = self.lin1(x)  # [50, 2] ‚Üí [50, 64]

# –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–ø—Ä–∏–º–µ—Ä):
x[0] = [0.3, 0.5, 0.2, ..., 0.8]    # table_1002132
x[35] = [0.6, 0.4, 0.7, ..., 0.5]   # service_308

# 1.2: BatchNorm + ReLU + Dropout
x = F.dropout(F.relu(self.bn1(x)), p=0.4, training=True)
# [50, 64]
```

**–ß—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ:**
- –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ [0,1] –∏–ª–∏ [1,0] –ø—Ä–µ–≤—Ä–∞—Ç–∏–ª–∏—Å—å –≤ –±–æ–≥–∞—Ç—ã–µ –≤–µ–∫—Ç–æ—Ä—ã —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 64
- –ö–∞–∂–¥—ã–π —É–∑–µ–ª —Ç–µ–ø–µ—Ä—å –∏–º–µ–µ—Ç "–Ω–∞—á–∞–ª—å–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥"

---

### –®–ê–ì 2: RESIDUAL BLOCK

```python
# 2.1: –°–æ—Ö—Ä–∞–Ω—è–µ–º identity
identity = x  # [50, 64]

# 2.2: –í—Ç–æ—Ä–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
x = self.lin2(x)  # [50, 64] ‚Üí [50, 64]
x = self.bn2(x)
x = F.relu(x)

# 2.3: RESIDUAL CONNECTION
x = x + identity  # –ö–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç!

# 2.4: Dropout
x = F.dropout(x, p=0.4, training=True)
# [50, 64]
```

**–ß—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ:**
- –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ "—É–ª—É—á—à–µ–Ω—ã" —á–µ—Ä–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–ª–æ–π
- Skip connection —Å–æ—Ö—Ä–∞–Ω–∏–ª –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é

---

### –®–ê–ì 3: DAGNN PROPAGATION (10 hops)

```python
# –ò—Å—Ö–æ–¥–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
H_0 = x  # [50, 64]
xs = [H_0]  # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π

# HOP 1
H_1 = self.propagation.propagate(edge_index, x=H_0, edge_weight=edge_weight)
# –§–æ—Ä–º—É–ª–∞: H_1 = (1-0.1) * A*H_0 + 0.1*H_0 = 0.9*A*H_0 + 0.1*H_0

# –ü—Ä–∏–º–µ—Ä –¥–ª—è service_308 (—É–∑–µ–ª 35):
# –í—Ö–æ–¥—è—â–∏–µ —Å–æ—Å–µ–¥–∏: —É–∑–µ–ª 0 (table_1002132)
H_1[35] = 0.9 * H_0[0] + 0.1 * H_0[35]
        = 0.9 * [0.3, 0.5, ...] + 0.1 * [0.6, 0.4, ...]
        = [0.33, 0.49, ...]

xs.append(H_1)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º

# HOP 2
H_2 = self.propagation.propagate(edge_index, x=H_1, edge_weight=edge_weight)
# –¢–µ–ø–µ—Ä—å service_308 –ø–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ—Ç —Å–æ—Å–µ–¥–µ–π –µ–≥–æ —Å–æ—Å–µ–¥–µ–π

xs.append(H_2)

# ... –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –¥–æ HOP 10 ...

# HOP 10
H_10 = self.propagation.propagate(edge_index, x=H_9, edge_weight=edge_weight)
xs.append(H_10)
```

**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –Ω–∞ –∫–∞–∂–¥–æ–º hop:**

```
Hop 0: [50, 64] - –∏—Å—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏

Hop 1: [50, 64] - —É–∑–ª—ã –∑–Ω–∞—é—Ç –æ –ø—Ä—è–º—ã—Ö —Å–æ—Å–µ–¥—è—Ö
  service_308 ‚Üê table_1002132
  service_309 ‚Üê service_308

Hop 2: [50, 64] - —É–∑–ª—ã –∑–Ω–∞—é—Ç –æ —Å–æ—Å–µ–¥—è—Ö —Å–æ—Å–µ–¥–µ–π
  service_309 ‚Üê service_308 ‚Üê table_1002132
  
Hop 3-10: [50, 64] - —É–∑–ª—ã –∑–Ω–∞—é—Ç –æ –≤—Å–µ –±–æ–ª–µ–µ –¥–∞–ª—å–Ω–∏—Ö —É–∑–ª–∞—Ö
  –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –ø–æ –≤—Å–µ–º—É –≥—Ä–∞—Ñ—É
```

**–§–æ—Ä–º—É–ª–∞ propagate (–¥–µ—Ç–∞–ª—å–Ω–æ):**

–î–ª—è —É–∑–ª–∞ i:
```
h_new[i] = (1/sqrt(deg[i])) * Œ£ (h[j] / sqrt(deg[j])) 
                               j‚ààN(i)

–≥–¥–µ:
  N(i) - –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤—Ö–æ–¥—è—â–∏—Ö —Å–æ—Å–µ–¥–µ–π —É–∑–ª–∞ i
  deg[i] - —Å—Ç–µ–ø–µ–Ω—å —É–∑–ª–∞ i
  h[j] - —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å–æ—Å–µ–¥–∞ j
```

–° teleport:
```
h_final[i] = (1 - Œ±) * h_new[i] + Œ± * h_0[i]
           = 0.9 * h_new[i] + 0.1 * h_0[i]
```

### Attention aggregation

```python
# –ü–æ—Å–ª–µ 10 hops –∏–º–µ–µ–º 11 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π
out = torch.stack(xs, dim=-1)  # [50, 64, 11]

# –í–µ—Å–∞ attention (–æ–±—É—á–∞–µ–º—ã–µ)
att_weights = F.softmax(self.att, dim=0)
# [11] - –≤–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ hop
# –ü—Ä–∏–º–µ—Ä: [0.08, 0.09, 0.11, 0.13, 0.15, 0.14, 0.12, 0.09, 0.05, 0.03, 0.01]

# –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
out = (out * att_weights.view(1, 1, -1)).sum(dim=-1)
# [50, 64]

# –î–ª—è —É–∑–ª–∞ i –∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ d:
out[i, d] = Œ£(xs[k][i, d] * att_weights[k]) for k=0..10
```

**–ü—Ä–∏–º–µ—Ä –¥–ª—è service_308:**
```python
# –≠–º–±–µ–¥–¥–∏–Ω–≥ –ø–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 0:
xs[0][35, 0] = 0.6   (hop 0, –∏—Å—Ö–æ–¥–Ω—ã–π)
xs[1][35, 0] = 0.55  (hop 1, —Å —Å–æ—Å–µ–¥—è–º–∏)
xs[2][35, 0] = 0.52  (hop 2)
...
xs[10][35, 0] = 0.48 (hop 10)

# Attention weights:
att = [0.08, 0.09, 0.11, ..., 0.01]

# –ò—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
out[35, 0] = 0.6*0.08 + 0.55*0.09 + 0.52*0.11 + ... + 0.48*0.01
           = 0.534  # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –≤—Å–µ—Ö hops
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç DAGNN Propagation:**
```python
x: [50, 64]
# –ö–∞–∂–¥—ã–π —É–∑–µ–ª —Ç–µ–ø–µ—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç:
# - –°–≤–æ—é —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
# - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ—Ç —Å–æ—Å–µ–¥–µ–π (1-2 hops)
# - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ—Ç –¥–∞–ª—å–Ω–∏—Ö —É–∑–ª–æ–≤ (3-10 hops)
# - –ì–ª–æ–±–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≥—Ä–∞—Ñ–∞
```

**–ó–∞—á–µ–º DAGNN Propagation:**
- **–ì–ª–∞–≤–Ω–∞—è —Ñ–∏—à–∫–∞ –º–æ–¥–µ–ª–∏!**
- –£–∑–ª—ã –æ–±–º–µ–Ω–∏–≤–∞—é—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ
- service_308 "—É–∑–Ω–∞–µ—Ç" –æ table_1002132 –∏ service_309
- –ú–æ–¥–µ–ª—å –ø–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

---

### –®–ê–ì 4: CLASSIFIER

```python
# –í—Ö–æ–¥: —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—Å–µ—Ö 50 —É–∑–ª–æ–≤ –ø–æ—Å–ª–µ DAGNN
x: [50, 64]

# 4.1: –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Å–ª–æ–π
x = self.lin3(x)  # [50, 64] ‚Üí [50, 32]
x = self.bn3(x)
x = F.relu(x)
x = F.dropout(x, p=0.4, training=True)

# 4.2: –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
x = self.lin_out(x)  # [50, 32] ‚Üí [50, 15]
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
```python
x: [50, 15]
# –î–ª—è –∫–∞–∂–¥–æ–≥–æ —É–∑–ª–∞ - 15 –ª–æ–≥–∏—Ç–æ–≤ (–æ—Ü–µ–Ω–æ–∫) –¥–ª—è 15 —Å–µ—Ä–≤–∏—Å–æ–≤

# –ü—Ä–∏–º–µ—Ä –¥–ª—è service_308 (—É–∑–µ–ª 35):
x[35] = [-2.1, -1.8, 3.5, 2.1, -0.5, -1.2, -0.8, ...]
         ‚Üë     ‚Üë     ‚Üë    ‚Üë
       s_306 s_307 s_308 s_309
       
# –í—ã—Å–æ–∫–∏–π –ª–æ–≥–∏—Ç (3.5) –¥–ª—è service_308
# –°—Ä–µ–¥–Ω–∏–π –ª–æ–≥–∏—Ç (2.1) –¥–ª—è service_309
# –ù–∏–∑–∫–∏–µ –ª–æ–≥–∏—Ç—ã –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
```

---

### –®–ê–ì 5: –í—ã–±–æ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

```python
# –í –æ–±—É—á–µ–Ω–∏–∏/inference –º—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –í–°–ï 50 —É–∑–ª–æ–≤
# –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —É–∑–ª—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤!

# –ö–æ–Ω—Ç–µ–∫—Å—Ç—ã train (–∏–Ω–¥–µ–∫—Å—ã –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —É–∑–ª–æ–≤)
contexts_train = [0, 35, 36, ...]  # [87]

# –ë–µ—Ä–µ–º –ª–æ–≥–∏—Ç—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —É–∑–ª–æ–≤
out = x[contexts_train]  # [87, 15]

# –ü—Ä–∏–º–µ—Ä:
# –ö–æ–Ω—Ç–µ–∫—Å—Ç 0: ('table_1002132', 'service_308')
# context_id = 35 (service_308)
# out[0] = x[35] = [-2.1, -1.8, 3.5, 2.1, ...]
```

**–ó–∞—á–µ–º –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —É–∑–ª—ã:**
- –£ –Ω–∞—Å 87 –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
- –ö–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º —É–∑–ª–æ–º (–∫–æ–Ω—Ç–µ–∫—Å—Ç)
- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–µ–ª–∞–µ–º "–æ—Ç –∏–º–µ–Ω–∏" —ç—Ç–æ–≥–æ —É–∑–ª–∞
- –ù–µ –Ω—É–∂–Ω—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö 50 —É–∑–ª–æ–≤, —Ç–æ–ª—å–∫–æ –¥–ª—è 87 –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö

---

### –®–ê–ì 6: –û–±—É—á–µ–Ω–∏–µ (Training)

```python
# –õ–æ–≥–∏—Ç—ã –¥–ª—è train –ø—Ä–∏–º–µ—Ä–æ–≤
out = model(data_pyg.x, data_pyg.edge_index, training=True)[contexts_train]
# [87, 15]

# –¶–µ–ª–µ–≤—ã–µ –∫–ª–∞—Å—Å—ã
targets_train = [2, 3, 2, 8, ...]  # [87] - –∏–Ω–¥–µ–∫—Å—ã –≤ service_map

# Loss function
loss = F.cross_entropy(out, targets_train)
```

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç Cross Entropy:**

```python
# –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞:
# 1. Softmax –∫ –ª–æ–≥–∏—Ç–∞–º
probs = softmax(out[i])
# [-2.1, -1.8, 3.5, 2.1, ...] ‚Üí [0.01, 0.02, 0.68, 0.19, ...]

# 2. –ë–µ—Ä–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
true_class = targets_train[i]  # –ù–∞–ø—Ä–∏–º–µ—Ä, 3 (service_309)
p_true = probs[true_class]      # 0.19

# 3. –í—ã—á–∏—Å–ª—è–µ–º loss
loss_i = -log(p_true) = -log(0.19) = 1.66

# 4. –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –≤—Å–µ–º 87 –ø—Ä–∏–º–µ—Ä–∞–º
loss = mean(loss_i for all i)
```

**Backpropagation:**
```python
loss.backward()  # –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
optimizer.step()  # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞
```

---

### –®–ê–ì 7: Inference (–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ)

```python
model.eval()  # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏

with torch.no_grad():  # –ë–µ–∑ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    # Forward pass
    test_output = model(data_pyg.x, data_pyg.edge_index, training=False)
    # [50, 15] - –ª–æ–≥–∏—Ç—ã –¥–ª—è –≤—Å–µ—Ö —É–∑–ª–æ–≤
    
    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ test –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
    test_output = test_output[contexts_test]
    # [38, 15] - –ª–æ–≥–∏—Ç—ã –¥–ª—è 38 —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
    
    # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    probs = F.softmax(test_output, dim=1)
    # [38, 15] - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    preds = test_output.argmax(dim=1)
    # [38] - –∏–Ω–¥–µ–∫—Å—ã –∫–ª–∞—Å—Å–æ–≤ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
```

**–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–∏–º–µ—Ä:**

```python
# –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä 0:
# –ö–æ–Ω—Ç–µ–∫—Å—Ç: ('table_1002132', 'service_308')
# context_id = 35

# –õ–æ–≥–∏—Ç—ã
logits = test_output[0] = [-2.1, -1.8, 3.5, 2.1, -0.5, ...]

# –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ softmax
probs = [0.01, 0.02, 0.68, 0.19, 0.01, ...]
         ‚Üë     ‚Üë     ‚Üë     ‚Üë
       s_306 s_307 s_308 s_309
       (0)   (1)   (2)   (3)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
pred = argmax(probs) = 2  # –∏–Ω–¥–µ–∫—Å –≤ service_map

# –†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞
predicted_service = service_map_inverse[2] = 'service_308'

# –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
true_service = 'service_309' (–∫–ª–∞—Å—Å 3)

# –†–µ–∑—É–ª—å—Ç–∞—Ç: ‚ùå –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ (mode collapse!)
```

---

## 8. –ü–æ–ª–Ω–∞—è —Å—Ö–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –∏—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ

### 8.1: –î–∞–Ω–Ω—ã–µ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–±–æ—Ç—ã

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•                                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  –ö–æ–º–ø–æ–∑–∏—Ü–∏–∏ (943) ‚Üí –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—É—Ç–∏ (106)              ‚îÇ
‚îÇ         ‚Üì                                                ‚îÇ
‚îÇ  –û–±—É—á–∞—é—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã (125):                               ‚îÇ
‚îÇ    X_raw: [('table_X',), ('table_X', 'service_308'), ...]‚îÇ
‚îÇ    y_raw: ['service_308', 'service_309', ...]           ‚îÇ
‚îÇ         ‚Üì                                                ‚îÇ
‚îÇ  –ì—Ä–∞—Ñ (NetworkX):                                        ‚îÇ
‚îÇ    - 50 —É–∑–ª–æ–≤ —Å –∞—Ç—Ä–∏–±—É—Ç–∞–º–∏ (type: service/table)       ‚îÇ
‚îÇ    - 97 –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ä–µ–±–µ—Ä                              ‚îÇ
‚îÇ         ‚Üì                                                ‚îÇ
‚îÇ  PyTorch Geometric Data:                                 ‚îÇ
‚îÇ    - x: [50, 2] - –ø—Ä–∏–∑–Ω–∞–∫–∏ —É–∑–ª–æ–≤                       ‚îÇ
‚îÇ    - edge_index: [2, 97] - —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≥—Ä–∞—Ñ–∞             ‚îÇ
‚îÇ         ‚Üì                                                ‚îÇ
‚îÇ  –ú–∞–ø–ø–∏–Ω–≥–∏:                                               ‚îÇ
‚îÇ    - node_map: {—É–∑–µ–ª ‚Üí –∏–Ω–¥–µ–∫—Å 0-49}                    ‚îÇ
‚îÇ    - service_map: {—Å–µ—Ä–≤–∏—Å ‚Üí –∏–Ω–¥–µ–∫—Å 0-14}               ‚îÇ
‚îÇ         ‚Üì                                                ‚îÇ
‚îÇ  –ò–Ω–¥–µ–∫—Å—ã:                                                ‚îÇ
‚îÇ    - contexts: [125] - –∏–Ω–¥–µ–∫—Å—ã —É–∑–ª–æ–≤ –≤ node_map        ‚îÇ
‚îÇ    - targets: [125] - –∏–Ω–¥–µ–∫—Å—ã —Å–µ—Ä–≤–∏—Å–æ–≤ –≤ service_map   ‚îÇ
‚îÇ         ‚Üì                                                ‚îÇ
‚îÇ  Train/Test Split:                                       ‚îÇ
‚îÇ    - 87 train / 38 test                                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FORWARD PASS –í –ú–û–î–ï–õ–ò                                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  INPUT:                                                  ‚îÇ
‚îÇ    x: [50, 2] - –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤—Å–µ—Ö —É–∑–ª–æ–≤                    ‚îÇ
‚îÇ    edge_index: [2, 97] - —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≥—Ä–∞—Ñ–∞               ‚îÇ
‚îÇ         ‚Üì                                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
‚îÇ  ‚îÇ  ENCODER BLOCK             ‚îÇ                         ‚îÇ
‚îÇ  ‚îÇ  Linear(2 ‚Üí 64)            ‚îÇ ‚Üê in_channels          ‚îÇ
‚îÇ  ‚îÇ  + BatchNorm + ReLU        ‚îÇ                         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
‚îÇ         ‚Üì                                                ‚îÇ
‚îÇ    [50, 64]                                             ‚îÇ
‚îÇ         ‚Üì                                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
‚îÇ  ‚îÇ  RESIDUAL BLOCK            ‚îÇ                         ‚îÇ
‚îÇ  ‚îÇ  Linear(64 ‚Üí 64)           ‚îÇ ‚Üê hidden_channels      ‚îÇ
‚îÇ  ‚îÇ  + BatchNorm + ReLU        ‚îÇ                         ‚îÇ
‚îÇ  ‚îÇ  + Skip Connection         ‚îÇ ‚Üê RESIDUAL!            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
‚îÇ         ‚Üì                                                ‚îÇ
‚îÇ    [50, 64]                                             ‚îÇ
‚îÇ         ‚Üì                                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
‚îÇ  ‚îÇ  DAGNN PROPAGATION         ‚îÇ                         ‚îÇ
‚îÇ  ‚îÇ  K=10 hops                 ‚îÇ ‚Üê K                    ‚îÇ
‚îÇ  ‚îÇ  Œ±=0.1 teleport            ‚îÇ                         ‚îÇ
‚îÇ  ‚îÇ  + Attention weights       ‚îÇ ‚Üê –æ–±—É—á–∞–µ–º—ã–µ –≤–µ—Å–∞       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
‚îÇ         ‚Üì                                                ‚îÇ
‚îÇ    [50, 64] —Å –≥—Ä–∞—Ñ–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π                     ‚îÇ
‚îÇ         ‚Üì                                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
‚îÇ  ‚îÇ  CLASSIFIER                ‚îÇ                         ‚îÇ
‚îÇ  ‚îÇ  Linear(64 ‚Üí 32)           ‚îÇ                         ‚îÇ
‚îÇ  ‚îÇ  + BatchNorm + ReLU        ‚îÇ                         ‚îÇ
‚îÇ  ‚îÇ  Linear(32 ‚Üí 15)           ‚îÇ ‚Üê out_channels         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
‚îÇ         ‚Üì                                                ‚îÇ
‚îÇ  OUTPUT:                                                 ‚îÇ
‚îÇ    [50, 15] - –ª–æ–≥–∏—Ç—ã –¥–ª—è –≤—Å–µ—Ö —É–∑–ª–æ–≤                    ‚îÇ
‚îÇ         ‚Üì                                                ‚îÇ
‚îÇ  –í—ã–±–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —É–∑–ª–æ–≤:                               ‚îÇ
‚îÇ    out[contexts_train] ‚Üí [87, 15]                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  –û–ë–£–ß–ï–ù–ò–ï / –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  Training:                                               ‚îÇ
‚îÇ    loss = CrossEntropy(out, targets_train)              ‚îÇ
‚îÇ    loss.backward() ‚Üí –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Inference:                                              ‚îÇ
‚îÇ    probs = softmax(out) ‚Üí [87, 15]                     ‚îÇ
‚îÇ    preds = argmax(probs) ‚Üí [87]                        ‚îÇ
‚îÇ    predicted_services = [service_map_inverse[p] for p]  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 9. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ —ç—Ç–∞–ø–∞–º

### –¢–∞–±–ª–∏—Ü–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö

| –î–∞–Ω–Ω—ã–µ | –ì–¥–µ —Å–æ–∑–¥–∞—é—Ç—Å—è | –ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è | –ó–∞—á–µ–º |
|--------|---------------|------------------|-------|
| **x** [50, 2] | prepare_pytorch_geometric_data | –í—Ö–æ–¥ –º–æ–¥–µ–ª–∏ | –ò—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —É–∑–ª–æ–≤ |
| **edge_index** [2, 97] | prepare_pytorch_geometric_data | DAGNN propagation | –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≥—Ä–∞—Ñ–∞ |
| **node_map** | prepare_pytorch_geometric_data | –°–æ–∑–¥–∞–Ω–∏–µ contexts | –ú–∞–ø–ø–∏–Ω–≥ —É–∑–ª–æ–≤ ‚Üí –∏–Ω–¥–µ–∫—Å—ã |
| **service_map** | prepare_pytorch_geometric_data | –°–æ–∑–¥–∞–Ω–∏–µ targets, —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ | –ú–∞–ø–ø–∏–Ω–≥ —Å–µ—Ä–≤–∏—Å–æ–≤ ‚Üí –∫–ª–∞—Å—Å—ã |
| **contexts** [125] | prepare_pytorch_geometric_data | –í—ã–±–æ—Ä —É–∑–ª–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è | –ò–Ω–¥–µ–∫—Å—ã –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —É–∑–ª–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ |
| **targets** [125] | prepare_pytorch_geometric_data | Loss function | –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã |
| **H^(0)** [50, 64] | Encoder | DAGNN (–∏—Å—Ö–æ–¥–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ) | –ù–∞—á–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ |
| **H^(k)** [50, 64] | DAGNN (–∫–∞–∂–¥—ã–π hop) | Attention aggregation | –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ—Å–ª–µ k hops |
| **att_weights** [11] | DAGNN (–æ–±—É—á–∞–µ–º—ã–µ) | –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ hops | –í–µ—Å–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ hop |
| **out** [87, 15] | Classifier ‚Üí contexts_train | CrossEntropyLoss | –õ–æ–≥–∏—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è |

---

## 10. –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª—ã (–ø–æ–ª–Ω—ã–µ)

### 10.1: Encoder

```
H^(0) = ReLU(BN(x ¬∑ W_1 + b_1))

–≥–¥–µ:
  x: [50, 2] - –≤—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
  W_1: [2, 64] - –≤–µ—Å–∞ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è
  b_1: [64] - bias
  BN: Batch Normalization
  H^(0): [50, 64] - –Ω–∞—á–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
```

### 10.2: Residual Block

```
H^(res) = ReLU(BN(H^(0) ¬∑ W_2 + b_2)) + H^(0)

–≥–¥–µ:
  W_2: [64, 64] - –≤–µ—Å–∞
  + H^(0) - skip connection (residual)
```

### 10.3: DAGNN Propagation (APPNP)

**–î–ª—è –∫–∞–∂–¥–æ–≥–æ hop k = 1, 2, ..., K:**

```
H^(k) = (1 - Œ±) ¬∑ D^(-1/2) ¬∑ A ¬∑ D^(-1/2) ¬∑ H^(k-1) + Œ± ¬∑ H^(0)

–≥–¥–µ:
  A[i,j] = 1, –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–±—Ä–æ i ‚Üí j, –∏–Ω–∞—á–µ 0
  D[i,i] = —Å—Ç–µ–ø–µ–Ω—å —É–∑–ª–∞ i (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–¥—è—â–∏—Ö + –∏—Å—Ö–æ–¥—è—â–∏—Ö —Ä–µ–±–µ—Ä)
  Œ± = 0.1 - teleport probability
  H^(0) - –∏—Å—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–æ—Ç encoder + residual)
```

**–î–ª—è —É–∑–ª–∞ i:**
```
h_i^(k) = (1 - Œ±) ¬∑ Œ£(h_j^(k-1) / sqrt(deg_i * deg_j)) + Œ± ¬∑ h_i^(0)
                    j‚ààN_in(i)

–≥–¥–µ:
  N_in(i) - –≤—Ö–æ–¥—è—â–∏–µ —Å–æ—Å–µ–¥–∏ —É–∑–ª–∞ i
  deg_i - —Å—Ç–µ–ø–µ–Ω—å —É–∑–ª–∞ i
```

**Attention aggregation:**
```
H^(final) = Œ£(w_k ¬∑ H^(k)) –¥–ª—è k = 0, 1, ..., K

–≥–¥–µ:
  w_k = softmax(att)[k] - –æ–±—É—á–∞–µ–º—ã–µ –≤–µ—Å–∞ attention
  att: [K+1] - –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
```

### 10.4: Classifier

```
logits = Linear_out(ReLU(BN(Linear_3(H^(final)))))

–î–µ—Ç–∞–ª—å–Ω–æ:
  z_1 = H^(final) ¬∑ W_3 + b_3      # [50, 64] ‚Üí [50, 32]
  z_2 = ReLU(BN(z_1))              # –ê–∫—Ç–∏–≤–∞—Ü–∏—è + –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
  logits = z_2 ¬∑ W_out + b_out     # [50, 32] ‚Üí [50, 15]

–≥–¥–µ:
  W_3: [64, 32]
  W_out: [32, 15]
```

### 10.5: Loss Function

```
L = CrossEntropy(logits[contexts], targets)
  = -(1/N) Œ£ log(softmax(logits[i])[targets[i]])
  
–≥–¥–µ:
  N = 87 - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ train –ø—Ä–∏–º–µ—Ä–æ–≤
  logits[i]: [15] - –ª–æ–≥–∏—Ç—ã –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ i
  targets[i] - –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∫–ª–∞—Å—Å (0-14)
```

---

## 11. –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–∏–º–µ—Ä: –ø–æ–ª–Ω—ã–π –ø—Ä–æ—Ö–æ–¥

### –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

```python
# –û–±—É—á–∞—é—â–∏–π –ø—Ä–∏–º–µ—Ä:
context = ('table_1002132', 'service_308')
target = 'service_309'

# –ü–æ—Å–ª–µ –º–∞–ø–ø–∏–Ω–≥–∞:
context_node_id = 35  # service_308 –≤ node_map
target_class_id = 3   # service_309 –≤ service_map
```

### –ü—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å

```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
ENCODER
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

–í—Ö–æ–¥: x[35] = [1, 0]  (service_308 - —ç—Ç–æ —Å–µ—Ä–≤–∏—Å)
      ‚Üì Linear(2 ‚Üí 64)
      [0.6, 0.4, 0.7, 0.3, ..., 0.5]  (64 dims)
      ‚Üì BatchNorm + ReLU + Dropout
H^(0)[35] = [0.62, 0.38, 0.72, ..., 0.48]

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
RESIDUAL BLOCK
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

      ‚Üì Linear(64 ‚Üí 64) + BN + ReLU
F(x) = [0.65, 0.42, 0.68, ..., 0.52]
      ‚Üì + identity
H^(res)[35] = [0.63, 0.40, 0.70, ..., 0.50]

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
DAGNN PROPAGATION
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Hop 0: [0.63, 0.40, 0.70, ..., 0.50]

Hop 1: –ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –æ—Ç table_1002132 (—É–∑–µ–ª 0)
       [0.58, 0.45, 0.65, ..., 0.48]
       = 0.9*(—ç–º–± table) + 0.1*(hop 0)

Hop 2: –ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –æ—Ç —Å–æ—Å–µ–¥–µ–π hop 1
       [0.60, 0.42, 0.68, ..., 0.52]

Hop 3: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –¥–∞–ª—å—à–µ
       [0.59, 0.44, 0.66, ..., 0.51]

...

Hop 10: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç –≤—Å–µ–≥–æ –≥—Ä–∞—Ñ–∞
        [0.61, 0.43, 0.67, ..., 0.49]

Attention aggregation:
  att_weights = [0.08, 0.09, 0.11, 0.13, 0.15, ...]
  H^(final)[35] = weighted_sum(all hops)
                = [0.605, 0.425, 0.675, ..., 0.495]

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
CLASSIFIER
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

      ‚Üì Linear(64 ‚Üí 32) + BN + ReLU
      [0.55, 0.38, 0.62, ..., 0.44]  (32 dims)
      ‚Üì Linear(32 ‚Üí 15)
logits[35] = [-2.1, -1.8, 3.5, 2.1, -0.5, -1.2, ...]
              ‚Üë     ‚Üë     ‚Üë    ‚Üë
            s_306 s_307 s_308 s_309
            (0)   (1)   (2)   (3)

      ‚Üì Softmax
probs[35] = [0.01, 0.02, 0.68, 0.19, 0.01, ...]
            
      ‚Üì Argmax
prediction = 2 ‚Üí service_308

–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: 3 (service_309)
–†–µ–∑—É–ª—å—Ç–∞—Ç: ‚ùå –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ
```

---

## 12. –ü–æ—á–µ–º—É –∫–∞–∂–¥—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –≤–∞–∂–µ–Ω

### Encoder (Linear 2‚Üí64)
**–ó–∞—á–µ–º:** –£–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
**–ë–µ–∑ –Ω–µ–≥–æ:** –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–ª–∞ –±—ã —Å 2-–º–µ—Ä–Ω—ã–º–∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ capacity)

### Residual Connection
**–ó–∞—á–µ–º:** –°—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –∏ –ø–æ–º–æ—á—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º
**–ë–µ–∑ –Ω–µ–≥–æ:** –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç "–∑–∞—Ç—É—Ö–∞—Ç—å", —Å–ª–æ–∂–Ω–µ–µ –æ–±—É—á–∞—Ç—å –≥–ª—É–±–æ–∫—É—é —Å–µ—Ç—å

### DAGNN Propagation
**–ó–∞—á–µ–º:** **–ö–õ–Æ–ß–ï–í–û–ô –ö–û–ú–ü–û–ù–ï–ù–¢!** –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≥—Ä–∞—Ñ—É
**–ë–µ–∑ –Ω–µ–≥–æ:** –£–∑–ª—ã "–Ω–µ –∑–Ω–∞—é—Ç" –æ —Å–æ—Å–µ–¥—è—Ö, —Ç–æ–ª—å–∫–æ –æ —Å–≤–æ–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö

### Attention –¥–ª—è hops
**–ó–∞—á–µ–º:** –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å —Ä–∞–∑–Ω—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
**–ë–µ–∑ –Ω–µ–≥–æ:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –±—ã —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π hop (—Ç–µ—Ä—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é)

### Classifier
**–ó–∞—á–µ–º:** –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
**–ë–µ–∑ –Ω–µ–≥–æ:** –ò–º–µ–ª–∏ –±—ã —Ç–æ–ª—å–∫–æ –≤–µ–∫—Ç–æ—Ä—ã, –∞ –Ω–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤

---

## 13. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏

### –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ DAGNN-Improved:

```python
# Encoder
lin1: 2 √ó 64 + 64 = 192
bn1: 64 √ó 2 = 128

# Residual
lin2: 64 √ó 64 + 64 = 4160
bn2: 64 √ó 2 = 128

# DAGNN
att: 11 (–≤–µ—Å–∞ –¥–ª—è hops)
APPNP: –Ω–µ—Ç –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (—Ç–æ–ª—å–∫–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)

# Classifier
lin3: 64 √ó 32 + 32 = 2080
bn3: 32 √ó 2 = 64
lin_out: 32 √ó 15 + 15 = 495

–ò–¢–û–ì–û ‚âà 7,312 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
```

**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ:**
- GRU4Rec (50 –∫–ª–∞—Å—Å–æ–≤): ‚âà 50,000 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- DAGNN (15 –∫–ª–∞—Å—Å–æ–≤): ‚âà 7,300 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **DAGNN –≤ 7 —Ä–∞–∑ –º–µ–Ω—å—à–µ!** ‚Üí –º–µ–Ω—å—à–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ

---

## 14. –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä DAGNN propagation

### üîÑ –ú–µ—Ö–∞–Ω–∏–∑–º propagation –≤ DAGNN

DAGNN –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **APPNP (Approximate Personalized Propagation of Neural Predictions)** - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≥—Ä–∞—Ñ—É.

#### –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∞:

```
H^(k) = (1 - Œ±) * A_norm * H^(k-1) + Œ± * H^(0)
```

–ì–¥–µ:
- `H^(k)` - –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —É–∑–ª–æ–≤ –ø–æ—Å–ª–µ k —à–∞–≥–æ–≤ propagation
- `H^(0)` - –∏—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —É–∑–ª–æ–≤ (–æ—Ç encoder)
- `A_norm` - –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Å–º–µ–∂–Ω–æ—Å—Ç–∏
- `Œ±` - teleport probability (–æ–±—ã—á–Ω–æ 0.1)
- `k` - —Ç–µ–∫—É—â–∏–π hop (–æ—Ç 1 –¥–æ K)

#### –ü–æ—á–µ–º—É —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:

**1. –ú–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ (K hops):**
```python
# –ü—Ä–∏–º–µ—Ä —Å K=3:
xs = [x]  # –ò—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏

# Hop 1: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç –ø—Ä—è–º—ã—Ö —Å–æ—Å–µ–¥–µ–π
x = (1-Œ±) * A @ x + Œ± * x_0
xs.append(x)

# Hop 2: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç —Å–æ—Å–µ–¥–µ–π —Å–æ—Å–µ–¥–µ–π
x = (1-Œ±) * A @ x + Œ± * x_0
xs.append(x)

# Hop 3: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è 3
x = (1-Œ±) * A @ x + Œ± * x_0
xs.append(x)

# –¢–µ–ø–µ—Ä—å xs —Å–æ–¥–µ—Ä–∂–∏—Ç [x_0, x_1, x_2, x_3]
```

**2. Teleportation (Œ± * H^(0)):**
- –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å –∏—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç oversmoothing (–≤—Å–µ —É–∑–ª—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –ø–æ—Ö–æ–∂–∏–º–∏)
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —É–∑–ª–∞

**3. Attention-based aggregation:**
```python
# –í–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–≥–æ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º learnable –≤–µ—Å–∞:
att_weights = softmax([w_0, w_1, w_2, ..., w_K])

# –§–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ:
output = sum(att_weights[i] * xs[i] for i in range(K+1))
```

---

### üìê –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã —Å–º–µ–∂–Ω–æ—Å—Ç–∏

**–¶–µ–ª—å:** –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å "–≤–∑—Ä—ã–≤–∞—é—â–∏–µ—Å—è" –∏–ª–∏ "–∑–∞—Ç—É—Ö–∞—é—â–∏–µ" –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–∏ propagation.

**–§–æ—Ä–º—É–ª–∞:**
```
A_norm = D^(-1/2) * A * D^(-1/2)
```

–ì–¥–µ:
- `A` - –º–∞—Ç—Ä–∏—Ü–∞ —Å–º–µ–∂–Ω–æ—Å—Ç–∏ (–∫—Ç–æ —Å –∫–µ–º —Å–≤—è–∑–∞–Ω)
- `D` - –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Å—Ç–µ–ø–µ–Ω–µ–π —É–∑–ª–æ–≤
- `D[i,i]` = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤—è–∑–µ–π —É–∑–ª–∞ i

**–ü—Ä–∏–º–µ—Ä:**
```python
# –ò—Å—Ö–æ–¥–Ω—ã–π –≥—Ä–∞—Ñ:
# –£–∑–µ–ª 0 ‚Üí –£–∑–µ–ª 1 (—Å—Ç–µ–ø–µ–Ω—å 0 = 1)
# –£–∑–µ–ª 1 ‚Üí –£–∑–µ–ª 2 (—Å—Ç–µ–ø–µ–Ω—å 1 = 2)
# –£–∑–µ–ª 1 ‚Üí –£–∑–µ–ª 3

# –ú–∞—Ç—Ä–∏—Ü–∞ —Å–º–µ–∂–Ω–æ—Å—Ç–∏ A:
A = [[0, 1, 0, 0],
     [0, 0, 1, 1],
     [0, 0, 0, 0],
     [0, 0, 0, 0]]

# –°—Ç–µ–ø–µ–Ω–∏ —É–∑–ª–æ–≤:
D = diag([1, 2, 0, 0])

# –ü–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–∞–∂–¥–æ–µ —Ä–µ–±—Ä–æ –ø–æ–ª—É—á–∞–µ—Ç –≤–µ—Å:
edge_weight = 1 / sqrt(deg(source) * deg(target))
```

**–ü–æ—á–µ–º—É –≤–∞–∂–Ω–æ:**
- –ë–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: —É–∑–ª—ã —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–≤—è–∑–µ–π –¥–æ–º–∏–Ω–∏—Ä—É—é—Ç
- –° –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π: –≤—Å–µ —É–∑–ª—ã –≤–Ω–æ—Å—è—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–∫–ª–∞–¥

---

### üéØ Attention mechanism –≤ DAGNN

**Learnable attention weights** –ø–æ–∑–≤–æ–ª—è—é—Ç –º–æ–¥–µ–ª–∏ —Ä–µ—à–∞—Ç—å, –∫–∞–∫–∏–µ hops –≤–∞–∂–Ω–µ–µ.

#### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ attention:

```python
class DAGNN(nn.Module):
    def __init__(self, in_channels, K, dropout=0.5):
        super().__init__()
        self.propagation = APPNP(K=K, alpha=0.1)
        
        # Learnable –≤–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ hop
        self.att = nn.Parameter(torch.Tensor(K + 1))
        nn.init.uniform_(self.att, 0.0, 1.0)
    
    def forward(self, x, edge_index):
        # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å –∫–∞–∂–¥–æ–≥–æ hop
        xs = [x]
        for k in range(self.K):
            x = self.propagation.propagate(edge_index, x=x)
            xs.append(x)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º attention
        att_weights = F.softmax(self.att, dim=0)
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
        out = torch.stack(xs, dim=-1)  # [nodes, features, K+1]
        out = (out * att_weights).sum(dim=-1)  # [nodes, features]
        
        return out
```

#### –ß—Ç–æ –º–æ–¥–µ–ª—å —É—á–∏—Ç:

**–ü—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤:**
```python
att_weights –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:
  Hop 0 (–∏—Å—Ö–æ–¥–Ω—ã–µ):        0.45  # –õ–æ–∫–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤–∞–∂–Ω–∞
  Hop 1 (1 —Ä–µ–±—Ä–æ):         0.25  # –ü—Ä—è–º—ã–µ —Å–æ—Å–µ–¥–∏ –≤–∞–∂–Ω—ã
  Hop 2 (2 —Ä–µ–±—Ä–∞):         0.15  # –°–æ—Å–µ–¥–∏ —Å–æ—Å–µ–¥–µ–π
  Hop 3-10:                0.15  # –î–∞–ª—å–Ω–∏–µ —É–∑–ª—ã –º–µ–Ω–µ–µ –≤–∞–∂–Ω—ã
```

**–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:**
- –ú–æ–¥–µ–ª—å —É–∑–Ω–∞–ª–∞, —á—Ç–æ –ª–æ–∫–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (hop 0) –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω–∞
- –ü—Ä—è–º—ã–µ —Å–æ—Å–µ–¥–∏ (hop 1) –¥–∞—é—Ç –ø–æ–ª–µ–∑–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
- –î–∞–ª—å–Ω–∏–µ —É–∑–ª—ã –≤–Ω–æ—Å—è—Ç –º–µ–Ω—å—à–∏–π –≤–∫–ª–∞–¥

---

### üß© –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Encoder –∏ Classifier

**–ü–æ–ª–Ω—ã–π pipeline DAGNN:**

```python
# 1. ENCODER: –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ‚Üí Rich embeddings
x = self.lin1(x)        # [50, 2] ‚Üí [50, 64]
x = self.bn1(x)
x = F.relu(x)
x = F.dropout(x, p=0.4)

# 2. RESIDUAL BLOCK
identity = x
x = self.lin2(x)        # [50, 64] ‚Üí [50, 64]
x = self.bn2(x)
x = F.relu(x)
x = x + identity        # Residual connection
x = F.dropout(x, p=0.4)

# 3. DAGNN PROPAGATION
x = self.dagnn(x, edge_index)  # [50, 64] ‚Üí [50, 64]
# –í–Ω—É—Ç—Ä–∏: K hops + attention

# 4. CLASSIFIER
x = self.lin3(x)        # [50, 64] ‚Üí [50, 32]
x = self.bn3(x)
x = F.relu(x)
x = F.dropout(x, p=0.4)

x = self.lin_out(x)     # [50, 32] ‚Üí [50, 15]
# Output: –ª–æ–≥–∏—Ç—ã –¥–ª—è 15 –∫–ª–∞—Å—Å–æ–≤
```

**–ü–æ—Ç–æ–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:**
```
–ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ [50, 2]
    ‚Üì Encoder
Rich embeddings [50, 64]
    ‚Üì Residual
Enhanced embeddings [50, 64]
    ‚Üì DAGNN (10 hops + attention)
Graph-aware embeddings [50, 64]
    ‚Üì Classifier
Class predictions [50, 15]
```

---

### üéõÔ∏è –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã DAGNN

#### –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–∏–µ | –í–ª–∏—è–Ω–∏–µ | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ |
|----------|----------|---------|--------------|
| **K** | 10 | –°–∫–æ–ª—å–∫–æ hops –¥–µ–ª–∞—Ç—å | 5-15 –¥–ª—è DAG, –±–æ–ª—å—à–µ = –¥–∞–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è |
| **Œ±** | 0.1 | Teleport probability | 0.1-0.2, –º–µ–Ω—å—à–µ = –±–æ–ª—å—à–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ |
| **hidden_channels** | 64 | –†–∞–∑–º–µ—Ä embeddings | 32-128, –±–æ–ª—å—à–µ = –±–æ–ª—å—à–µ capacity |
| **dropout** | 0.4 | Regularization | 0.3-0.5, –±–æ–ª—å—à–µ = –º–µ–Ω—å—à–µ overfitting |

#### –í–ª–∏—è–Ω–∏–µ K (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ hops):

**K = 1:**
- –¢–æ–ª—å–∫–æ –ø—Ä—è–º—ã–µ —Å–æ—Å–µ–¥–∏
- –ë—ã—Å—Ç—Ä–æ, –Ω–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
- –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –ø–ª–æ—Ç–Ω—ã—Ö –≥—Ä–∞—Ñ–æ–≤

**K = 5:**
- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è 5 —Ä–µ–±–µ—Ä
- –ë–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞
- –•–æ—Ä–æ—à–æ –¥–ª—è —Å—Ä–µ–¥–Ω–∏—Ö –≥—Ä–∞—Ñ–æ–≤

**K = 10:**
- –î–∞–ª—å–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
- –ú–æ–∂–µ—Ç –æ—Ö–≤–∞—Ç–∏—Ç—å –≤–µ—Å—å –≥—Ä–∞—Ñ (–¥–ª—è –º–∞–ª—ã—Ö –≥—Ä–∞—Ñ–æ–≤)
- –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –Ω–∞—à–µ–≥–æ —Å–ª—É—á–∞—è (97 —Ä–µ–±–µ—Ä, 50 —É–∑–ª–æ–≤)

**K > 15:**
- Oversmoothing —Ä–∏—Å–∫
- –í—Å–µ —É–∑–ª—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –ø–æ—Ö–æ–∂–∏–º–∏
- –ò–∑–±—ã—Ç–æ—á–Ω–æ –¥–ª—è –º–∞–ª—ã—Ö –≥—Ä–∞—Ñ–æ–≤

#### –í–ª–∏—è–Ω–∏–µ Œ± (teleport):

**Œ± = 0.0:**
- –ù–µ—Ç –≤–æ–∑–≤—Ä–∞—Ç–∞ –∫ –∏—Å—Ö–æ–¥–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
- –°–∏–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
- –†–∏—Å–∫ –ø–æ—Ç–µ—Ä–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

**Œ± = 0.1 (–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ):**
- 10% –∏—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ
- –ë–∞–ª–∞–Ω—Å –ª–æ–∫–∞–ª—å–Ω–æ–π –∏ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
- –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç oversmoothing

**Œ± = 0.5:**
- –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ª–æ–∫–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
- –ú–∞–ª–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- Propagation —Å–ª–∞–±–æ —Ä–∞–±–æ—Ç–∞–µ—Ç

---

### üîç –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è propagation

**–ü—Ä–∏–º–µ—Ä: –∫–∞–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è**

```
–ò—Å—Ö–æ–¥–Ω—ã–π –≥—Ä–∞—Ñ:
    Table1 ‚Üí Service1 ‚Üí Service2
    Table2 ‚Üó

Hop 0 (–∏—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏):
    Table1:   [0, 1]
    Table2:   [0, 1]
    Service1: [1, 0]
    Service2: [1, 0]

Hop 1 (–ø—Ä—è–º—ã–µ —Å–æ—Å–µ–¥–∏):
    Service1 –ø–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ—Ç Table1 –∏ Table2
    Service2 –ø–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ—Ç Service1

Hop 2 (—Å–æ—Å–µ–¥–∏ —Å–æ—Å–µ–¥–µ–π):
    Service2 —Ç–µ–ø–µ—Ä—å –∑–Ω–∞–µ—Ç –æ Table1 –∏ Table2
    –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —á–µ—Ä–µ–∑ 2 —Ä–µ–±—Ä–∞

–° teleportation (Œ±=0.1):
    –ö–∞–∂–¥—ã–π —É–∑–µ–ª —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç 10% —Å–≤–æ–∏—Ö –∏—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    –ù–µ —Ç–µ—Ä—è–µ—Ç —Å–≤–æ—é "–∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å"
```

---

## 15. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏ best practices

### üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã DAGNN

#### 1. **Residual Connections**

**–ó–∞—á–µ–º –Ω—É–∂–Ω—ã:**
- –ü–æ–º–æ–≥–∞—é—Ç –æ–±—É—á–µ–Ω–∏—é –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π
- –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—é—Ç vanishing gradients
- –ü–æ–∑–≤–æ–ª—è—é—Ç –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Å–ª–æ–π, –µ—Å–ª–∏ –æ–Ω –Ω–µ –Ω—É–∂–µ–Ω

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
# –ü–µ—Ä–µ–¥ propagation:
identity = x
x = self.lin2(x)
x = self.bn2(x)
x = F.relu(x)
x = x + identity  # Residual connection
```

**–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –ª–µ–≥—á–µ –ø—Ä–æ—Ö–æ–¥—è—Ç:**
```
Without residual:
  loss ‚Üí lin_out ‚Üí dagnn ‚Üí lin2 ‚Üí lin1
  (–≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –∑–∞—Ç—É—Ö–Ω—É—Ç—å)

With residual:
  loss ‚Üí lin_out ‚Üí dagnn ‚Üí (skip) ‚Üí lin1
  (–ø—Ä—è–º–æ–π –ø—É—Ç—å –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤)
```

#### 2. **Normalization**

**BatchNorm vs LayerNorm:**

```python
# BatchNorm (—Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è DAGNN):
self.bn1 = nn.BatchNorm1d(64)
# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø–æ batch, –¥–ª—è –∫–∞–∂–¥–æ–π features –æ—Ç–¥–µ–ª—å–Ω–æ

# LayerNorm (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞):
self.ln1 = nn.LayerNorm(64)
# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø–æ features, –¥–ª—è –∫–∞–∂–¥–æ–≥–æ sample –æ—Ç–¥–µ–ª—å–Ω–æ
```

**–ö–æ–≥–¥–∞ —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
- **BatchNorm**: batch_size >= 16, —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
- **LayerNorm**: –º–∞–ª—ã–µ batch (< 16), –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ

**–î–ª—è –Ω–∞—à–µ–≥–æ —Å–ª—É—á–∞—è (batch = –≤–µ—Å—å –≥—Ä–∞—Ñ = 50):**
‚Üí BatchNorm —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ

#### 3. **Dropout placement**

**–ì–¥–µ –ø—Ä–∏–º–µ–Ω—è—Ç—å dropout:**
```python
# ‚úÖ –ü–æ—Å–ª–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏:
x = F.relu(x)
x = F.dropout(x, p=0.4, training=training)

# ‚úÖ –í DAGNN propagation:
if training:
    x = F.dropout(x, p=dropout, training=training)

# ‚ùå –ù–ï –ø—Ä–∏–º–µ–Ω—è—Ç—å –ø–µ—Ä–µ–¥ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º –≤—ã—Ö–æ–¥–æ–º:
x = self.lin_out(x)  # –ë–µ–∑ dropout
```

**–ü–æ—á–µ–º—É:**
- Dropout = —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è, –Ω—É–∂–µ–Ω –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
- –ü–æ—Å–ª–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏: –∑–∞–Ω—É–ª—è–µ–º —É–∂–µ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω—ã
- –í propagation: –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –≥—Ä–∞—Ñ–∞

#### 4. **Activation functions**

**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–π:**

| Activation | Formula | Pros | Cons |
|------------|---------|------|------|
| **ReLU** | max(0, x) | –ë—ã—Å—Ç—Ä–∞—è, –ø—Ä–æ—Å—Ç–∞—è | Dead neurons |
| **ELU** | x if x>0 else Œ±(e^x-1) | –ü–ª–∞–≤–Ω–∞—è, –±–µ–∑ dead neurons | –ú–µ–¥–ª–µ–Ω–Ω–µ–µ |
| **LeakyReLU** | max(0.01x, x) | –ë–µ–∑ dead neurons | –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π slope |

**–î–ª—è DAGNN —Ä–µ–∫–æ–º–µ–Ω–¥—É—é ReLU:**
- –°—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è GNN
- –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞
- –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å BatchNorm

---

### üéØ –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

#### –ë–∞–∑–æ–≤–∞—è DAGNN (–¥–ª—è –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö):

```python
model = DAGNNRecommender(
    in_channels=2,          # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ [service/table]
    hidden_channels=64,     # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    out_channels=15,        # –¢–æ–ª—å–∫–æ —Ü–µ–ª–µ–≤—ã–µ –∫–ª–∞—Å—Å—ã (—Å–µ—Ä–≤–∏—Å—ã)
    K=10,                   # 10 hops –¥–ª—è –Ω–µ–±–æ–ª—å—à–æ–≥–æ –≥—Ä–∞—Ñ–∞
    dropout=0.4             # –£–º–µ—Ä–µ–Ω–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
)

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=0.001,               # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π learning rate
    weight_decay=1e-4       # L2 regularization
)

# Learning rate scheduling
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.5,             # –£–º–µ–Ω—å—à–∞–µ–º lr –≤ 2 —Ä–∞–∑–∞
    patience=20,            # –ü–æ—Å–ª–µ 20 —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è
    verbose=True
)

# Training
epochs = 200
early_stopping_patience = 50
```

#### –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è DAGNN (–¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö):

```python
model = DAGNNRecommender(
    in_channels=64,         # Rich node features
    hidden_channels=128,    # –ë–æ–ª—å—à–µ capacity
    out_channels=100,       # –ë–æ–ª—å—à–µ –∫–ª–∞—Å—Å–æ–≤
    K=15,                   # –ë–æ–ª—å—à–µ hops
    dropout=0.5             # –°–∏–ª—å–Ω–µ–µ regularization
)
```

---

### üîë –ö–ª—é—á–µ–≤—ã–µ takeaways

#### –ß—Ç–æ –¥–µ–ª–∞–µ—Ç DAGNN —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º:

1. **APPNP propagation**
   - –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ K hops
   - Teleportation –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç oversmoothing
   - –õ–∏–Ω–µ–π–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å O(K √ó E)

2. **Attention mechanism**
   - –ú–æ–¥–µ–ª—å —Å–∞–º–∞ —É—á–∏—Ç—Å—è –≤–∞–∂–Ω–æ—Å—Ç—å —Ä–∞–∑–Ω—ã—Ö hops
   - Adaptive weighting > —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞
   - Learnable parameters: –≤—Å–µ–≥–æ K+1 –≤–µ—Å–æ–≤

3. **Residual connections**
   - –°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
   - –ü—Ä—è–º–æ–π –ø—É—Ç—å –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
   - –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å skip –Ω–µ–Ω—É–∂–Ω—ã—Ö —Å–ª–æ–µ–≤

4. **Normalization + Dropout**
   - BatchNorm —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ
   - Dropout –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç overfitting
   - –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –¥–∞—ë—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

5. **DAG-aware**
   - –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ (directed edges)
   - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏
   - –ù–µ –¥–æ–±–∞–≤–ª—è–µ—Ç –æ–±—Ä–∞—Ç–Ω—ã–µ —Ä–µ–±—Ä–∞

#### –ü–æ—á–µ–º—É DAGNN —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö:

- ‚úÖ **–ú–∞–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤** (~7K –¥–ª—è 15 –∫–ª–∞—Å—Å–æ–≤)
- ‚úÖ **Graph structure** –∫–∞–∫ inductive bias
- ‚úÖ **Propagation** —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
- ‚úÖ **Regularization** (dropout + weight decay)

---

### üìè –°–ª–æ–∂–Ω–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å

**Computational complexity:**

```python
# Forward pass:
Encoder:     O(N √ó D √ó H)        # N nodes, D input, H hidden
Residual:    O(N √ó H¬≤)           # Linear layer
DAGNN:       O(K √ó E √ó H)        # K hops, E edges, H features
Classifier:  O(N √ó H √ó C)        # C classes

# Total: O(N√óH¬≤ + K√óE√óH + N√óH√óC)
# –î–ª—è –Ω–∞—à–µ–≥–æ —Å–ª—É—á–∞—è: O(50√ó64¬≤ + 10√ó97√ó64 + 50√ó64√ó15) ‚âà 300K ops
```

**Memory complexity:**
```
Node features: N √ó H √ó 4 bytes = 50 √ó 64 √ó 4 = 12.8 KB
Parameters:    ~7K √ó 4 bytes = 28 KB
Total memory:  < 100 KB

‚Üí –û—á–µ–Ω—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å!
```

**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–≤—ã–º GCN:**
- **DAGNN**: K –æ—Ç–¥–µ–ª—å–Ω—ã—Ö propagation steps + attention
- **GCN**: 3 convolution layers —Å–æ —Å–≤–æ–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
- **DAGNN**: –ë–æ–ª—å—à–µ hops, –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **GCN**: –ú–µ–Ω—å—à–µ hops, –±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –æ—Ç–ª–∏—á–∏—è:**
```
GCN:
  Conv1 ‚Üí Conv2 ‚Üí Conv3 ‚Üí Output
  (–∫–∞–∂–¥—ã–π layer –∏–º–µ–µ—Ç —Å–≤–æ–∏ trainable –≤–µ—Å–∞)

DAGNN:
  Encoder ‚Üí DAGNN_Propagation ‚Üí Classifier
  (propagation –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≥—Ä–∞—Ñ–∞)
```

---

## 16. –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

### üéØ –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã DAGNN

**DAGNN (Directed Acyclic Graph Neural Network)** - —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∞—Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–º–∏ –≥—Ä–∞—Ñ–∞–º–∏, –∫–æ—Ç–æ—Ä–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –≥—Ä–∞—Ñ–∞.

#### –ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:

```
Input (node features) [N √ó 2]
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. ENCODER                 ‚îÇ
‚îÇ  Linear + BatchNorm + ReLU  ‚îÇ
‚îÇ  [N √ó 2] ‚Üí [N √ó 64]         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. RESIDUAL BLOCK          ‚îÇ
‚îÇ  Linear + BN + Skip         ‚îÇ
‚îÇ  [N √ó 64] ‚Üí [N √ó 64]        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. DAGNN PROPAGATION       ‚îÇ
‚îÇ  K-hop + Attention          ‚îÇ
‚îÇ  [N √ó 64] ‚Üí [N √ó 64]        ‚îÇ
‚îÇ  ‚Ä¢ APPNP (K=10, Œ±=0.1)      ‚îÇ
‚îÇ  ‚Ä¢ Learnable attention      ‚îÇ
‚îÇ  ‚Ä¢ Teleportation            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  4. CLASSIFIER              ‚îÇ
‚îÇ  Linear ‚Üí Linear ‚Üí Output   ‚îÇ
‚îÇ  [N √ó 64] ‚Üí [N √ó 15]        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
Output (class logits) [N √ó 15]
```

#### –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å—É—Ç—å:

**Propagation formula:**
```
H^(k) = (1 - Œ±) √ó A_norm √ó H^(k-1) + Œ± √ó H^(0)
```

**Attention aggregation:**
```
Output = Œ£(w_i √ó H^(i)) –¥–ª—è i = 0 to K
–≥–¥–µ w_i = softmax(learnable_weights)
```

#### –ü–æ—á–µ–º—É DAGNN —Ä–∞–±–æ—Ç–∞–µ—Ç:

1. **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏**
   - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç –¥–∞–ª—å–Ω–∏—Ö —É–∑–ª–æ–≤ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–µ–∫—É—â–µ–≥–æ —É–∑–ª–∞
   - K=10 –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ö–≤–∞—Ç–∏—Ç—å –≤–µ—Å—å –Ω–µ–±–æ–ª—å—à–æ–π –≥—Ä–∞—Ñ

2. **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏**
   - Teleportation (Œ±=0.1) –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç oversmoothing
   - Residual connections —Å–æ—Ö—Ä–∞–Ω—è—é—Ç –∏—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏

3. **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ**
   - Learnable attention weights –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥—è—Ç –≤–∞–∂–Ω—ã–µ hops
   - –ú–æ–¥–µ–ª—å —Å–∞–º–∞ —Ä–µ—à–∞–µ—Ç, —á—Ç–æ –≤–∞–∂–Ω–µ–µ: –ª–æ–∫–∞–ª—å–Ω–∞—è –∏–ª–∏ –≥–ª–æ–±–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è

4. **–ú–∞–ª–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤**
   - ~7,300 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è 15 –∫–ª–∞—Å—Å–æ–≤
   - –ú–µ–Ω—å—à–µ —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö

5. **DAG-aware –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**
   - –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ —Ä–µ–±—Ä–∞ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏
   - –ù–µ —Ç—Ä–µ–±—É–µ—Ç —Å–∏–º–º–µ—Ç—Ä–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∞

---

### üöÄ –î–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã

**–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**

```python
import torch
from torch_geometric.data import Data

# 1. –ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –≥—Ä–∞—Ñ
edge_index = torch.tensor([[0, 1, 2], [1, 2, 3]], dtype=torch.long)
x = torch.tensor([[1, 0], [1, 0], [0, 1], [1, 0]], dtype=torch.float)
data = Data(x=x, edge_index=edge_index)

# 2. –°–æ–∑–¥–∞–π—Ç–µ –º–æ–¥–µ–ª—å
model = DAGNNRecommender(
    in_channels=2,
    hidden_channels=64,
    out_channels=15,
    K=10,
    dropout=0.4
)

# 3. Forward pass
output = model(x, edge_index, training=False)
predictions = output.argmax(dim=1)

print(f"Predictions: {predictions}")
```

**–î–ª—è –æ–±—É—á–µ–Ω–∏—è:**

```python
# Optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)

# Training loop
for epoch in range(200):
    model.train()
    optimizer.zero_grad()
    
    # Forward
    out = model(data.x, data.edge_index, training=True)
    loss = F.cross_entropy(out[train_mask], labels[train_mask])
    
    # Backward
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
```

---

### üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

**–°–≤—è–∑–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ –ø—Ä–æ–µ–∫—Ç–µ:**
- `sequence_dag_recommender_final.py` - –ø–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
- `README.md` - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
- `CROSS_VALIDATION_GUIDE.md` - —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏

**–ö–ª—é—á–µ–≤—ã–µ papers:**
- DAGNN: "Towards Deeper Graph Neural Networks" (2020)
- APPNP: "Predict then Propagate" (2019)
- GCN: "Semi-Supervised Classification with Graph Convolutional Networks" (2017)

---

### üéì –ò—Ç–æ–≥–∏

–ü–æ—Å–ª–µ –ø—Ä–æ—á—Ç–µ–Ω–∏—è —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤—ã –¥–æ–ª–∂–Ω—ã –ø–æ–Ω–∏–º–∞—Ç—å:

- ‚úÖ –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∂–¥—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç DAGNN
- ‚úÖ –ü–æ—á–µ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–º–µ–Ω–Ω–æ —Ç–∞–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- ‚úÖ –ö–∞–∫ –¥–∞–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ
- ‚úÖ –ú–∞—Ç–µ–º–∞—Ç–∏–∫—É APPNP propagation
- ‚úÖ –†–æ–ª—å attention mechanism
- ‚úÖ Best practices –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≥—Ä–∞—Ñ–∞–º–∏
- ‚úÖ –ö–∞–∫ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã

**DAGNN - —ç—Ç–æ –º–æ—â–Ω–∞—è, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∏ —ç–ª–µ–≥–∞–Ω—Ç–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –≥—Ä–∞—Ñ–∞–º–∏!** üöÄ

---

*–§–∞–π–ª –æ–±–Ω–æ–≤–ª–µ–Ω: 2025-10-25*  
*–í–µ—Ä—Å–∏—è: DAGNN Architecture Deep Dive v5.0*  
*–ê–≤—Ç–æ—Ä: –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ DAGNN*

