#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ì–∏–±—Ä–∏–¥–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π - Ensemble –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π

–û–±—ä–µ–¥–∏–Ω—è–µ—Ç:
1. Sequence Recommendation (DAGNN) - –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ DAG –≥—Ä–∞—Ñ–∞
2. Collaborative Filtering (Hybrid-BPR) - –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π

–ü–æ–¥—Ö–æ–¥:
- DAGNN –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–π —Å–µ—Ä–≤–∏—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≥—Ä–∞—Ñ–∞
- Hybrid-BPR –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–µ—Ä–≤–∏—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: alpha * DAGNN + (1-alpha) * Hybrid-BPR

–†–µ–∑—É–ª—å—Ç–∞—Ç: –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã DAG
"""

import json
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import networkx as nx
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from torch_geometric.data import Data
from torch_geometric.nn import APPNP
from lightfm import LightFM
from lightfm.data import Dataset
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import ndcg_score, precision_score, recall_score
from tqdm import tqdm
import warnings

warnings.filterwarnings("ignore")

print("="*70)
print("üîÄ –ì–ò–ë–†–ò–î–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô (Ensemble)")
print("   DAGNN (Sequence) + Hybrid-BPR (Collaborative Filtering)")
print("="*70)

# ============================ –ú–û–î–ï–õ–ò ============================

class DAGNN(nn.Module):
    """DAGNN –¥–ª—è sequence recommendation"""
    def __init__(self, in_channels: int, K: int, dropout: float = 0.5):
        super().__init__()
        self.propagation = APPNP(K=K, alpha=0.1)
        self.att = nn.Parameter(torch.Tensor(K + 1))
        self.dropout = dropout
        self.reset_parameters()

    def reset_parameters(self):
        self.propagation.reset_parameters()
        nn.init.uniform_(self.att, 0.0, 1.0)

    def forward(self, x, edge_index, training=True):
        xs = [x]
        edge_weight = torch.ones(edge_index.size(1), dtype=torch.float32, device=edge_index.device)
        for _ in range(self.propagation.K):
            x = self.propagation.propagate(edge_index, x=x, edge_weight=edge_weight)
            if training:
                x = F.dropout(x, p=self.dropout, training=training)
            xs.append(x)
        out = torch.stack(xs, dim=-1)
        att_weights = F.softmax(self.att, dim=0)
        out = (out * att_weights.view(1, 1, -1)).sum(dim=-1)
        return out


class DAGNNRecommender(nn.Module):
    """DAGNN Recommender –¥–ª—è –≥—Ä–∞—Ñ–∞"""
    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int, 
                 K: int = 10, dropout: float = 0.4):
        super().__init__()
        self.dropout = dropout
        
        self.lin1 = nn.Linear(in_channels, hidden_channels)
        self.bn1 = nn.BatchNorm1d(hidden_channels)
        
        self.lin2 = nn.Linear(hidden_channels, hidden_channels)
        self.bn2 = nn.BatchNorm1d(hidden_channels)
        
        self.dagnn = DAGNN(hidden_channels, K, dropout=dropout)
        
        self.lin3 = nn.Linear(hidden_channels, hidden_channels // 2)
        self.bn3 = nn.BatchNorm1d(hidden_channels // 2)
        
        self.lin_out = nn.Linear(hidden_channels // 2, out_channels)

    def forward(self, x, edge_index, training=True):
        x = self.lin1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=training)
        
        identity = x
        x = self.lin2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = x + identity
        x = F.dropout(x, p=self.dropout, training=training)
        
        x = self.dagnn(x, edge_index, training=training)
        
        x = self.lin3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=training)
        
        x = self.lin_out(x)
        return x


# ============================ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ============================

print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Sequence Recommendation...")

# –ó–∞–≥—Ä—É–∑–∫–∞ DAG
with open("compositionsDAG.json", "r", encoding="utf-8") as f:
    dag_data = json.load(f)

dag = nx.DiGraph()
id_to_mid = {}

for composition in dag_data:
    for node in composition["nodes"]:
        if "mid" in node:
            id_to_mid[str(node["id"])] = f"service_{node['mid']}"
        else:
            id_to_mid[str(node["id"])] = f"table_{node['id']}"

    for link in composition["links"]:
        source = str(link["source"])
        target = str(link["target"])
        src_node = id_to_mid[source]
        tgt_node = id_to_mid[target]
        dag.add_node(src_node, type='service' if src_node.startswith("service") else 'table')
        dag.add_node(tgt_node, type='service' if tgt_node.startswith("service") else 'table')
        dag.add_edge(src_node, tgt_node)

print(f"   DAG: {dag.number_of_nodes()} —É–∑–ª–æ–≤, {dag.number_of_edges()} —Ä—ë–±–µ—Ä")

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è DAGNN
node_list = list(dag.nodes)
node_encoder = LabelEncoder()
node_ids = node_encoder.fit_transform(node_list)
node_map = {node: idx for node, idx in zip(node_list, node_ids)}

edge_index = torch.tensor([[node_map[u], node_map[v]] for u, v in dag.edges], dtype=torch.long).t()
features = [[1, 0] if dag.nodes[n]['type'] == 'service' else [0, 1] for n in node_list]
x = torch.tensor(features, dtype=torch.float)
data_pyg = Data(x=x, edge_index=edge_index)

print(f"   PyG Data: {data_pyg.num_nodes} —É–∑–ª–æ–≤, {data_pyg.num_edges} —Ä—ë–±–µ—Ä")

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Collaborative Filtering
print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Collaborative Filtering...")
df = pd.read_csv("calls.csv", sep=";")
df = df[~df['owner'].str.contains('cookies', na=False)]
df = df.sort_values('start_time')

owners = df['owner'].unique()
mids = df['mid'].unique()

print(f"   –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(owners)}")
print(f"   –°–µ—Ä–≤–∏—Å–æ–≤: {len(mids)}")
print(f"   –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {len(df)}")

train_size = int(len(df) * 0.7)
df_train = df[:train_size]
df_test = df[train_size:]

def build_matrix(df, owners, mids, normalize=False):
    pivot = df.pivot_table(index='owner', columns='mid', values='id', aggfunc='count').fillna(0)
    pivot = pivot.reindex(index=owners, columns=mids, fill_value=0)
    mat = pivot.values
    if normalize:
        row_sums = mat.sum(axis=1, keepdims=True)
        row_sums[row_sums == 0] = 1
        mat = mat / row_sums
    return mat

X_train_cf = build_matrix(df_train, owners, mids, normalize=True)
X_test_cf = build_matrix(df_test, owners, mids, normalize=True)

# ============================ –û–±—É—á–µ–Ω–∏–µ DAGNN ============================

print("\nüß† –û–±—É—á–µ–Ω–∏–µ DAGNN (Sequence Recommendation)...")

# –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å—å –≥—Ä–∞—Ñ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
torch.manual_seed(456)  # –õ—É—á—à–∏–π seed –¥–ª—è DAGNN
np.random.seed(456)

dagnn_model = DAGNNRecommender(
    in_channels=2,
    hidden_channels=64,
    out_channels=len(node_map),
    K=10,
    dropout=0.4
)

# –°–æ–∑–¥–∞–µ–º dummy training data –¥–ª—è –æ–±—É—á–µ–Ω–∏—è DAGNN
# –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω—ã paths, –∑–¥–µ—Å—å —É–ø—Ä–æ—â–∞–µ–º
optimizer = torch.optim.Adam(dagnn_model.parameters(), lr=0.0008, weight_decay=1e-4)

# –ü—Ä–æ—Å—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –≥—Ä–∞—Ñ–∞
dagnn_model.train()
for epoch in tqdm(range(50), desc="Training DAGNN"):
    optimizer.zero_grad()
    embeddings = dagnn_model(data_pyg.x, data_pyg.edge_index, training=True)
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º reconstruction loss
    edge_index = data_pyg.edge_index
    src, dst = edge_index[0], edge_index[1]
    pos_scores = (embeddings[src] * embeddings[dst]).sum(dim=1)
    
    # Negative sampling
    neg_dst = torch.randint(0, len(node_map), (len(src),))
    neg_scores = (embeddings[src] * embeddings[neg_dst]).sum(dim=1)
    
    # BPR-like loss
    loss = -F.logsigmoid(pos_scores - neg_scores).mean()
    loss.backward()
    optimizer.step()

dagnn_model.eval()
print("‚úÖ DAGNN –æ–±—É—á–µ–Ω–∞")

# ============================ –û–±—É—á–µ–Ω–∏–µ Hybrid-BPR ============================

print("\nüéØ –û–±—É—á–µ–Ω–∏–µ Hybrid-BPR (Collaborative Filtering)...")

# LightFM-BPR –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
np.random.seed(1000)  # –õ—É—á—à–∏–π seed –¥–ª—è CF

dataset_cf = Dataset()
dataset_cf.fit(owners, mids)
interactions, _ = dataset_cf.build_interactions(
    [(row['owner'], row['mid']) for _, row in df_train.iterrows()]
)

lightfm_model = LightFM(
    loss='bpr',
    no_components=60,  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
    learning_rate=0.07,
    item_alpha=1e-07,
    user_alpha=1e-07,
    max_sampled=20,
    random_state=1000
)

print("   Training PHCF-BPR...")
for epoch in tqdm(range(20), desc="PHCF-BPR"):
    lightfm_model.fit_partial(interactions, epochs=1, num_threads=4)

# KNN –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
print("   Training Weighted KNN...")
knn_model = NearestNeighbors(n_neighbors=6, metric='cosine')
knn_model.fit(X_train_cf)
distances, indices = knn_model.kneighbors(X_train_cf)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è PHCF-BPR
users_cf = np.arange(len(owners))
items_cf = np.arange(len(mids))
users_grid, items_grid = np.meshgrid(users_cf, items_cf, indexing='ij')
phcf_preds = lightfm_model.predict(users_grid.flatten(), items_grid.flatten(), num_threads=4)
phcf_preds = phcf_preds.reshape(len(owners), len(mids))

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è KNN (weighted)
knn_preds = np.zeros_like(X_train_cf)
for i in range(len(X_train_cf)):
    neighbors = indices[i, 1:]
    neighbor_dists = distances[i, 1:]
    weights = 1.0 / (neighbor_dists + 1e-10)
    weights = weights / weights.sum()
    knn_preds[i] = (X_train_cf[neighbors].T @ weights)

# –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –¥–ª—è Hybrid-BPR
hybrid_bpr_preds = 0.7 * phcf_preds + 0.3 * knn_preds

print("‚úÖ Hybrid-BPR –æ–±—É—á–µ–Ω–∞")

# ============================ –ú–∞–ø–ø–∏–Ω–≥ –º–µ–∂–¥—É —Å–∏—Å—Ç–µ–º–∞–º–∏ ============================

print("\nüîó –°–æ–∑–¥–∞–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ –º–µ–∂–¥—É —Å–∏—Å—Ç–µ–º–∞–º–∏...")

# –ò–∑–≤–ª–µ–∫–∞–µ–º service IDs –∏–∑ –æ–±–µ–∏—Ö —Å–∏—Å—Ç–µ–º
dag_services = [node for node in node_list if node.startswith("service_")]
dag_service_ids = [int(s.split('_')[1]) for s in dag_services]

# –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å CF mids
common_service_ids = set(dag_service_ids) & set(mids)
print(f"   –û–±—â–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤: {len(common_service_ids)}")

# –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥
service_map_dag_to_cf = {}
service_map_cf_to_dag = {}

for service_id in common_service_ids:
    dag_node = f"service_{service_id}"
    if dag_node in node_map and service_id in mids:
        dag_idx = node_map[dag_node]
        cf_idx = list(mids).index(service_id)
        service_map_dag_to_cf[dag_idx] = cf_idx
        service_map_cf_to_dag[cf_idx] = dag_idx

print(f"   –ú–∞–ø–ø–∏–Ω–≥ —Å–æ–∑–¥–∞–Ω: {len(service_map_dag_to_cf)} —Å–µ—Ä–≤–∏—Å–æ–≤")

# ============================ –ì–∏–±—Ä–∏–¥–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ============================

print("\nüé® –°–æ–∑–¥–∞–Ω–∏–µ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...")

class HybridEnsembleRecommender:
    """
    –ì–∏–±—Ä–∏–¥–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è DAGNN –∏ Hybrid-BPR
    
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ DAGNN (Sequence)     ‚îÇ Hybrid-BPR (CF)      ‚îÇ
    ‚îÇ –í–µ—Å: beta            ‚îÇ –í–µ—Å: (1-beta)        ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ –ì—Ä–∞—Ñ DAG –ø–∞—Ç—Ç–µ—Ä–Ω—ã    ‚îÇ –õ–∏—á–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è  ‚îÇ
    ‚îÇ –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —Å–≤—è–∑–∏    ‚îÇ –ò—Å—Ç–æ—Ä–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ‚îÇ
    ‚îÇ –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã    ‚îÇ –ö–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–∞—è      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
        Combined = beta * DAGNN + (1-beta) * CF
                    ‚Üì
            Top-k recommendations
    """
    
    def __init__(self, dagnn_model, hybrid_bpr_cf_preds, data_pyg, 
                 service_map_dag_to_cf, service_map_cf_to_dag,
                 node_map, owners, mids, beta=0.5):
        """
        Args:
            dagnn_model: –û–±—É—á–µ–Ω–Ω–∞—è DAGNN –º–æ–¥–µ–ª—å
            hybrid_bpr_cf_preds: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è Hybrid-BPR (users √ó services)
            data_pyg: PyG Data –¥–ª—è DAGNN
            service_map_*: –ú–∞–ø–ø–∏–Ω–≥–∏ –º–µ–∂–¥—É —Å–∏—Å—Ç–µ–º–∞–º–∏
            beta: –í–µ—Å DAGNN (0.5 = —Ä–∞–≤–Ω—ã–µ –≤–µ—Å–∞)
        """
        self.dagnn_model = dagnn_model
        self.hybrid_bpr_cf_preds = hybrid_bpr_cf_preds
        self.data_pyg = data_pyg
        self.service_map_dag_to_cf = service_map_dag_to_cf
        self.service_map_cf_to_dag = service_map_cf_to_dag
        self.node_map = node_map
        self.owners = owners
        self.mids = mids
        self.beta = beta
        
        # –ü–æ–ª—É—á–∞–µ–º embeddings –∏–∑ DAGNN –¥–ª—è –≤—Å–µ—Ö —É–∑–ª–æ–≤
        self.dagnn_model.eval()
        with torch.no_grad():
            self.dagnn_embeddings = self.dagnn_model(
                self.data_pyg.x, 
                self.data_pyg.edge_index, 
                training=False
            )
    
    def recommend_for_user(self, user_id, context_nodes=None, k=10):
        """
        –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–∏–Ω–¥–µ–∫—Å –≤ owners)
            context_nodes: –°–ø–∏—Å–æ–∫ —É–∑–ª–æ–≤ DAG (–∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        
        Returns:
            recommended_services: –°–ø–∏—Å–æ–∫ ID —Å–µ—Ä–≤–∏—Å–æ–≤
            scores: Scores –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞
        """
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏–∑ Collaborative Filtering
        cf_scores_user = self.hybrid_bpr_cf_preds[user_id]  # –î–ª—è —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏–∑ DAGNN (–Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        if context_nodes is not None and len(context_nodes) > 0:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —É–∑–µ–ª –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            last_node = context_nodes[-1]
            if last_node in self.node_map:
                context_idx = self.node_map[last_node]
                # Scores = similarity —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                context_emb = self.dagnn_embeddings[context_idx]
                dag_scores = (self.dagnn_embeddings @ context_emb).numpy()
            else:
                dag_scores = np.zeros(len(self.node_map))
        else:
            # –ë–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–∏–µ scores
            dag_scores = self.dagnn_embeddings.mean(dim=1).numpy()
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º scores –¥–ª—è –æ–±—â–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
        combined_scores = np.zeros(len(self.mids))
        
        for cf_idx, mid in enumerate(self.mids):
            # CF score
            cf_score = cf_scores_user[cf_idx]
            
            # DAGNN score (–µ—Å–ª–∏ —Å–µ—Ä–≤–∏—Å –µ—Å—Ç—å –≤ DAG)
            dag_score = 0.0
            if cf_idx in self.service_map_cf_to_dag:
                dag_idx = self.service_map_cf_to_dag[cf_idx]
                dag_score = dag_scores[dag_idx]
            
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º
            combined_scores[cf_idx] = self.beta * dag_score + (1 - self.beta) * cf_score
        
        # –¢–æ–ø-k
        top_k_indices = np.argsort(combined_scores)[::-1][:k]
        top_k_mids = [self.mids[idx] for idx in top_k_indices]
        top_k_scores = combined_scores[top_k_indices]
        
        return top_k_mids, top_k_scores
    
    def evaluate(self, test_df, k=10):
        """–û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ç–µ—Å—Ç –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
        user_actual = {}
        for _, row in test_df.iterrows():
            owner = row['owner']
            mid = row['mid']
            if owner not in user_actual:
                user_actual[owner] = set()
            user_actual[owner].add(mid)
        
        # –û—Ü–µ–Ω–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        ndcg_scores = []
        precision_scores = []
        recall_scores = []
        
        for user_idx, owner in enumerate(self.owners):
            if owner not in user_actual:
                continue
            
            actual_mids = user_actual[owner]
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            recommended_mids, scores = self.recommend_for_user(user_idx, context_nodes=None, k=k)
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            relevant = [1 if mid in actual_mids else 0 for mid in recommended_mids]
            
            if sum(relevant) > 0:
                # nDCG
                try:
                    ndcg = ndcg_score([relevant], [scores[:k]])
                    ndcg_scores.append(ndcg)
                except:
                    pass
                
                # Precision
                precision = sum(relevant) / k
                precision_scores.append(precision)
                
                # Recall
                recall = sum(relevant) / len(actual_mids) if len(actual_mids) > 0 else 0
                recall_scores.append(recall)
        
        return {
            'ndcg': np.mean(ndcg_scores) if ndcg_scores else 0,
            'precision': np.mean(precision_scores) if precision_scores else 0,
            'recall': np.mean(recall_scores) if recall_scores else 0
        }


# ============================ –°–æ–∑–¥–∞–Ω–∏–µ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–∏–±—Ä–∏–¥–∞ ============================

print("\nüîÄ –°–æ–∑–¥–∞–Ω–∏–µ Hybrid Ensemble Recommender...")

# –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è beta (–≤–µ—Å DAGNN)
betas = [0.3, 0.5, 0.7]
beta_results = []

for beta in betas:
    hybrid_recommender = HybridEnsembleRecommender(
        dagnn_model=dagnn_model,
        hybrid_bpr_cf_preds=hybrid_bpr_preds,
        data_pyg=data_pyg,
        service_map_dag_to_cf=service_map_dag_to_cf,
        service_map_cf_to_dag=service_map_cf_to_dag,
        node_map=node_map,
        owners=owners,
        mids=mids,
        beta=beta
    )
    
    print(f"\nüìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ beta={beta} (–≤–µ—Å DAGNN={beta}, –≤–µ—Å CF={1-beta})...")
    metrics = hybrid_recommender.evaluate(df_test, k=10)
    
    beta_results.append({
        'beta': beta,
        'ndcg': metrics['ndcg'],
        'precision': metrics['precision'],
        'recall': metrics['recall']
    })
    
    print(f"   nDCG:      {metrics['ndcg']:.4f}")
    print(f"   Precision: {metrics['precision']:.4f}")
    print(f"   Recall:    {metrics['recall']:.4f}")

# ============================ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ ============================

print("\n" + "="*70)
print("üìä –°–†–ê–í–ù–ï–ù–ò–ï –° –ë–ê–ó–û–í–´–ú–ò –ú–û–î–ï–õ–Ø–ú–ò")
print("="*70)

# –¢–æ–ª—å–∫–æ CF (beta=0)
print("\n1Ô∏è‚É£ Hybrid-BPR (CF only, beta=0):")
hybrid_cf_only = HybridEnsembleRecommender(
    dagnn_model, hybrid_bpr_preds, data_pyg,
    service_map_dag_to_cf, service_map_cf_to_dag,
    node_map, owners, mids, beta=0.0
)
metrics_cf = hybrid_cf_only.evaluate(df_test, k=10)
print(f"   nDCG:      {metrics_cf['ndcg']:.4f}")
print(f"   Precision: {metrics_cf['precision']:.4f}")
print(f"   Recall:    {metrics_cf['recall']:.4f}")

# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
print("\n" + "="*70)
print("üèÜ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´")
print("="*70)

all_results = [
    ("Hybrid-BPR (CF only)", 0.0, metrics_cf),
] + [(f"Hybrid Ensemble (Œ≤={r['beta']})", r['beta'], r) for r in beta_results]

print(f"\n{'–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è':<30} {'Beta':<8} {'nDCG':<12} {'Precision':<12} {'Recall':<12}")
print("-"*70)

best_result = max(all_results, key=lambda x: x[2]['ndcg'])

for config, beta, metrics in sorted(all_results, key=lambda x: x[2]['ndcg'], reverse=True):
    marker = "ü•á" if config == best_result[0] else "  "
    print(f"{marker} {config:<28} {beta:<8.1f} {metrics['ndcg']:>10.4f}  {metrics['precision']:>10.4f}  {metrics['recall']:>10.4f}")

print("\n" + "="*70)
print("üí° –í–´–í–û–î–´")
print("="*70)

print(f"\nüèÜ –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {best_result[0]}")
print(f"   Beta (–≤–µ—Å DAGNN): {best_result[1]}")
print(f"   nDCG: {best_result[2]['ndcg']:.4f}")
print(f"   Precision: {best_result[2]['precision']:.4f}")
print(f"   Recall: {best_result[2]['recall']:.4f}")

improvement_vs_cf = ((best_result[2]['ndcg'] - metrics_cf['ndcg']) / metrics_cf['ndcg']) * 100 if metrics_cf['ndcg'] > 0 else 0
print(f"\nüìà –£–ª—É—á—à–µ–Ω–∏–µ vs —Ç–æ–ª—å–∫–æ CF: {improvement_vs_cf:+.1f}%")

print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
if best_result[1] > 0:
    print(f"   ‚úÖ –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –†–ê–ë–û–¢–ê–ï–¢!")
    print(f"   ‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π beta: {best_result[1]}")
    print(f"   ‚úÖ –ö–æ–º–±–∏–Ω–∞—Ü–∏—è DAGNN + CF –¥–∞–µ—Ç —Å–∏–Ω–µ—Ä–≥–∏—é")
else:
    print(f"   ‚ö†Ô∏è  CF alone –ª—É—á—à–µ (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–±—â–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤?)")
    print(f"   ‚ÑπÔ∏è  –û–±—â–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤: {len(common_service_ids)} –∏–∑ {len(mids)}")

# ============================ –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ============================

print("\n" + "="*70)
print("üìù –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø")
print("="*70)

# –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
best_beta = best_result[1]
final_recommender = HybridEnsembleRecommender(
    dagnn_model, hybrid_bpr_preds, data_pyg,
    service_map_dag_to_cf, service_map_cf_to_dag,
    node_map, owners, mids, beta=best_beta
)

# –ü—Ä–∏–º–µ—Ä —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
example_user_idx = 0
example_user_id = owners[example_user_idx]

print(f"\nüë§ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {example_user_id}")

# –ü–æ–ª—É—á–∞–µ–º –µ–≥–æ –∏—Å—Ç–æ—Ä–∏—é
user_history = df_train[df_train['owner'] == example_user_id]['mid'].tolist()
print(f"   –ò—Å—Ç–æ—Ä–∏—è –≤—ã–∑–æ–≤–æ–≤: {user_history[:5]}... (–≤—Å–µ–≥–æ {len(user_history)})")

# –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è DAGNN (–ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤—ã–∑–æ–≤—ã)
context_nodes = [f"service_{mid}" for mid in user_history[-3:] if f"service_{mid}" in node_map]
print(f"   –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è DAGNN: {context_nodes}")

# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
recommended, scores = final_recommender.recommend_for_user(
    example_user_idx, 
    context_nodes=context_nodes, 
    k=10
)

print(f"\nüìã –¢–æ–ø-10 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:")
for i, (mid, score) in enumerate(zip(recommended, scores), 1):
    in_history = "‚úì" if mid in user_history else " "
    print(f"   {i:2d}. Service {mid:6d}  Score: {score:8.4f}  {in_history}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –±—ã–ª–æ –≤ —Ç–µ—Å—Ç–µ
actual_test_user = df_test[df_test['owner'] == example_user_id]['mid'].tolist()
if actual_test_user:
    print(f"\n‚úÖ –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã–∑–æ–≤—ã –≤ —Ç–µ—Å—Ç–µ: {actual_test_user}")
    hits = [mid for mid in recommended if mid in actual_test_user]
    print(f"   –ü–æ–ø–∞–¥–∞–Ω–∏–π: {len(hits)} –∏–∑ {len(actual_test_user)}")
    if hits:
        print(f"   –ù–∞–π–¥–µ–Ω–Ω—ã–µ: {hits}")

print("\n" + "="*70)
print("‚úÖ –ì–ò–ë–†–ò–î–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê!")
print("="*70)

print(f"\nüéØ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ:")
print(f"   final_recommender.recommend_for_user(user_id, context_nodes, k=10)")
print(f"\nüèÜ –õ—É—á—à–∏–π beta: {best_beta}")
print(f"   nDCG: {best_result[2]['ndcg']:.4f}")

print("\n" + "="*70)

